{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, TimeDistributed\n",
    "from keras.layers import Concatenate, Flatten\n",
    "from keras.layers import GRU, Conv2D, MaxPooling2D, Embedding\n",
    "from keras.layers import Input, Reshape, Dot, Add\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from data_handle import *\n",
    "from gensim_wrapper import *\n",
    "from utils import *\n",
    "import gensim\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "seed_val = 1000\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "tf.set_random_seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_tokens, seq_length, embed_size, n_units):\n",
    "    int_words_input = Input(shape=(seq_length, ), dtype='int32')\n",
    "    embedding = Embedding(input_dim=num_tokens, output_dim=embed_size, input_length=seq_length)(int_words_input)\n",
    "\n",
    "    x = GRU(128, name=\"first_gru\")(embedding)\n",
    "    x = Dense(n_units, activation='linear')(x)\n",
    "    main_model = Model(int_words_input, x)\n",
    "    return main_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coocur(embed_size, window_size, n_units):\n",
    "    word_encoding = Input(shape=(embed_size, ), dtype='float32')\n",
    "    word_pos = Input(shape=(window_size, ), dtype='float32')\n",
    "    pos = Dense(2, activation=\"tanh\")(word_pos)\n",
    "    x = Concatenate(axis=1)([word_encoding, pos])\n",
    "    x = Dense(n_units, activation='sigmoid')(x)\n",
    "    x = Dense(embed_size, activation='linear')(x)\n",
    "    main_model = Model([word_encoding, word_pos], x)\n",
    "    return main_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = read_file()\n",
    "vocab, word2int, int2word = build_vocab(words)\n",
    "int_words = words_to_ints(word2int, words)\n",
    "word2freq = get_frequency(words, word2int, int2word)\n",
    "char2int, int2char, char2tup, tup2char, n_consonant, n_vowels = build_charset()\n",
    "ns_unigrams = ns_sample(word2freq, word2int, int2word, .75)\n",
    "n_chars = 11 + 2 \n",
    "n_features = len(char2int)\n",
    "batch_size = 120\n",
    "embed_size = 50\n",
    "skip_window = 5\n",
    "seq_len = 5\n",
    "embeddings = normalize(np.load('results/seq_encoding.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(int_words, int2word, char2tup, embeddings, seq_length, batch_size,n_chars, n_cons, n_vows):\n",
    "    embed_size = embeddings.shape[1]\n",
    "    ci = 0\n",
    "    while True:\n",
    "        batch_inputs = np.ndarray((batch_size, seq_length), dtype=np.int32)\n",
    "        batch_output = np.ndarray((batch_size, embed_size), dtype=np.float32)\n",
    "        for i in range(batch_size):\n",
    "            seq, ci = get_context_words(int_words, ci, ci+seq_length + 1)\n",
    "            ci = ci+1\n",
    "            target = seq[-1]\n",
    "            seq = seq[:seq_length]\n",
    "            batch_inputs[i] = seq\n",
    "            batch_output[i] = embeddings[target]\n",
    "        yield batch_inputs, batch_output\n",
    "        \n",
    "def gen_co(int_words, int2word, embeddings, batch_size, window_size):\n",
    "    assert batch_size % window_size == 0\n",
    "    embed_size = embeddings.shape[1]\n",
    "    ci = 0\n",
    "    indexes = list(range(window_size * 2 + 1))\n",
    "    indexes.pop(window_size)\n",
    "    while True:\n",
    "        batch_inputs = np.ndarray((batch_size, embed_size), dtype=np.float32)\n",
    "        batch_poses = np.ndarray((batch_size, window_size * 2 + 1), dtype=np.float32)\n",
    "        batch_output = np.ndarray((batch_size, embed_size), dtype=np.float32)\n",
    "        for i in range(0, batch_size, window_size):\n",
    "            seq, ci = get_context_words(int_words, ci, window_size * 2 + 1)\n",
    "            target = seq[window_size]\n",
    "            np.random.shuffle(indexes)\n",
    "            rand_indexs = indexes[:window_size]\n",
    "#             seq = seq[rand_indexs]\n",
    "            for j in range(window_size):\n",
    "                batch_inputs[i+j] = embeddings[seq[rand_indexs[j]]]\n",
    "                pos_vec = np.zeros((window_size * 2 + 1,), dtype=np.int32)\n",
    "                pos_vec[rand_indexs[j]] = 1\n",
    "                batch_poses[i+j] = pos_vec\n",
    "                batch_output[i+j] = embeddings[target]\n",
    "            ci = ci+1\n",
    "        yield [batch_inputs, batch_poses], batch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(final_embedding, word2int, embed_size):\n",
    "    gensim = GensimWrapper('data/news.txt', embed_size, 0, log=False)\n",
    "    gensim.set_embeddings(word2int, final_embedding)\n",
    "    result = gensim.evaluate()\n",
    "    for key in result:\n",
    "        print(\"{0}: {1:.2f}%\".format(key, result[key]), end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_loss(yTrue, yPred):\n",
    "    loss = K.sum(1 - K.dot(K.l2_normalize(yTrue) , K.l2_normalize(yPred)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen = generate(int_words, int2word, char2tup, embeddings, seq_len, batch_size, n_chars, n_consonant, n_vowels)\n",
    "gen = gen_co(int_words, int2word, embeddings, batch_size, skip_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            24          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 52)           0           input_1[0][0]                    \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          6784        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 50)           6450        dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 13,258\n",
      "Trainable params: 13,258\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model = create_model(len(vocab), seq_length=seq_len, embed_size=embed_size, n_units=embeddings.shape[1])\n",
    "# adam = keras.optimizers.Nadam(0.01)\n",
    "# model.compile(optimizer=adam, loss='mse', metrics=['mse'])\n",
    "# model.summary()\n",
    "try:\n",
    "    del model\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "except:\n",
    "    pass\n",
    "model = create_coocur(embed_size, skip_window * 2 + 1, 128)\n",
    "adam = keras.optimizers.Nadam(0.001)\n",
    "model.compile(optimizer=adam, loss=\"cosine_proximity\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "34078/34078 [==============================] - 210s 6ms/step - loss: -0.9371\n",
      "Epoch 2/2\n",
      "34078/34078 [==============================] - 212s 6ms/step - loss: -0.9375\n"
     ]
    }
   ],
   "source": [
    "n_batches = len(words) // batch_size\n",
    "history = model.fit_generator(gen, steps_per_epoch=n_batches, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_embeddings = model.layers[1].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(lm_embeddings, word2int, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util = Utils(embedding=normalize(lm_embeddings), int2word=int2word, word2int=word2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.sorted_sim(\"ነበር\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
