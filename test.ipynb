{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, TimeDistributed\n",
    "from keras.layers import Concatenate, Flatten\n",
    "from keras.layers import GRU, Conv2D, MaxPooling2D\n",
    "from keras.layers import Input, Reshape, Dot, Add\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from data_handle import *\n",
    "from gensim_wrapper import *\n",
    "from utils import *\n",
    "import gensim\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "seed_val = 1000\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "tf.set_random_seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58384\n"
     ]
    }
   ],
   "source": [
    "words = read_file(filename='data/news.txt')\n",
    "unkown_word = \"<unk>\"\n",
    "# words = [unkown_word] + words\n",
    "xvocab, xword2int, xint2word = build_vocab(words)\n",
    "\n",
    "words, word2freq = min_count_threshold(words)\n",
    "vocab, word2int, int2word = build_vocab(words)\n",
    "print(len(vocab))\n",
    "# word2freq = get_frequency(words, word2int, int2word)\n",
    "unigrams = [word2freq[int2word[i]] for i in range(len(word2int))]\n",
    "\n",
    "char2int, int2char, char2tup, tup2char, n_consonant, n_vowel = build_charset()\n",
    "ns_unigrams = ns_sample(word2freq, word2int, int2word, .75)\n",
    "n_chars = 11 + 2 \n",
    "n_features = len(char2int)\n",
    "batch_size = 120\n",
    "embed_size = 100\n",
    "skip_window = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseVec(file, delimiter):\n",
    "    lines = open(file, encoding='utf8').readlines()\n",
    "    vocab_size, embed_size = [int(s) for s in lines[0].split()]\n",
    "    vocab_size  = min(len(word2int), vocab_size)\n",
    "    embeddings = np.zeros((vocab_size + 1, embed_size), dtype=np.float64)\n",
    "    for i in range(1, vocab_size):\n",
    "        try:\n",
    "            line = lines[i][:-1].split(delimiter)\n",
    "            word = line[0]\n",
    "            if 'unk' in word:\n",
    "                continue\n",
    "            if word in word2int:\n",
    "                wordvec = np.array([np.float64(j) for j in line[1:] if j != ''])\n",
    "                embeddings[word2int[word]] = wordvec\n",
    "        except Exception as e:\n",
    "            print(lines[i])\n",
    "            print(e)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_emb = parseVec('results/ft/w2v1.txt.vec', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Library\\Projects\\amharic_word_embedding\\data_handle.py:338: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return array / norms\n",
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'syntactic': 7.94392523364486, 'semantic': 2.952029520295203, 'total': 6.008583690987124, 'pick-one-out': 76.14678899082568}\n",
      "{'syntactic': 0.9345794392523364, 'semantic': 4.428044280442805, 'total': 2.2889842632331905, 'pick-one-out': 64.22018348623853}\n"
     ]
    }
   ],
   "source": [
    "emb = seq_emb\n",
    "print(evaluate(word2int, normalize(emb), embed_size=emb.shape[1]))\n",
    "print(evaluate(word2int, emb, embed_size=emb.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Library\\Projects\\amharic_word_embedding\\data_handle.py:338: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return array / norms\n",
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "all_vecs = np.zeros(shape=(58384, 100))\n",
    "mmax = 11\n",
    "result = np.zeros(shape=(mmax, 6))\n",
    "for i in range(mmax):\n",
    "    emb = parseVec('results/ft/w2v{0}.txt.vec'.format(i), ' ')\n",
    "    all_vecs += emb\n",
    "    nr = evaluate(word2int, normalize(emb), embed_size=emb.shape[1])\n",
    "    ur = evaluate(word2int, emb, embed_size=emb.shape[1])\n",
    "    result[i][0] = nr['syntactic']\n",
    "    result[i][1] = nr['semantic']\n",
    "    result[i][2] = nr['pick-one-out']\n",
    "    \n",
    "    result[i][3] = ur['syntactic']\n",
    "    result[i][4] = ur['semantic']\n",
    "    result[i][5] = ur['pick-one-out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Library\\Projects\\amharic_word_embedding\\data_handle.py:338: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return array / norms\n",
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'syntactic': 7.009345794392523, 'semantic': 4.059040590405904, 'total': 5.86552217453505, 'pick-one-out': 76.14678899082568}\n",
      "{'syntactic': 0.7009345794392523, 'semantic': 5.904059040590406, 'total': 2.7181688125894135, 'pick-one-out': 63.30275229357798}\n"
     ]
    }
   ],
   "source": [
    "emb = all_vecs/mmax\n",
    "print(evaluate(word2int, normalize(emb), embed_size=emb.shape[1]))\n",
    "print(evaluate(word2int, emb, embed_size=emb.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.54035684  3.28748742 76.31359466  0.97706032  5.60214693 64.13678065]\n"
     ]
    }
   ],
   "source": [
    "print(result.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb2D = np.zeros((seq_emb.shape[0], seq_emb.shape[1] * sem_emb.shape[1]))\n",
    "for i in range(seq_emb.shape[0]):\n",
    "    seq = seq_emb[i]\n",
    "    sem = sem_emb[i]\n",
    "    vec = np.outer(seq, sem).flatten()\n",
    "    emb2D[i] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9253903990746096\n"
     ]
    }
   ],
   "source": [
    "lines = open('data/newan2.txt', encoding='utf-8').readlines()\n",
    "emb = emb2D\n",
    "correct = 0\n",
    "for line in lines:\n",
    "    if ':' in line:\n",
    "        continue\n",
    "    try:\n",
    "        line = [x.strip() for x in line[:-1].split(' ')]\n",
    "        vs = [emb[word2int[w]] for w in line]\n",
    "        v = vs[1] - vs[0] + vs[2]\n",
    "        sims = emb.dot(v)\n",
    "        sim_with_word = [(word, sims[word2int[word]])\n",
    "                             for word in word2int.keys()]\n",
    "        sorted_sim = sorted(sim_with_word, key=lambda t: t[1], reverse=True)[:4]\n",
    "#         print(type(sorted_sim[0][1]), sorted_sim[0][1])\n",
    "        result = [pred[0] for pred in sorted_sim]\n",
    "        if not np.isnan(sorted_sim[0][1]):\n",
    "#             print(result, line[-1])\n",
    "            if line[-1] in result:\n",
    "#                 print(result, line[-1])\n",
    "                correct += 1\n",
    "    except:\n",
    "        pass\n",
    "print(correct * 100.0 / (len(lines) - 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util = Utils(word2int, int2word, emb2D)\n",
    "# lines = open('data/newan2.txt', encoding='utf-8').readlines()\n",
    "# acc = np.array([0, 0])\n",
    "# total = np.array([0, 0])\n",
    "# ind = 0\n",
    "# for line in lines[1:]:\n",
    "#     if ':' in line[:-1]:\n",
    "#         ind = 1\n",
    "#         continue\n",
    "#     ws = line[:-1].split()\n",
    "#     result = util.solve(ws)\n",
    "#     if result is None:\n",
    "#         continue\n",
    "#     if ws[-1] in result:\n",
    "#         acc[ind] += 1\n",
    "#     total[ind] += 1\n",
    "# print(acc * 100 / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = open('data/analogies.txt', encoding='utf-8').readlines()\n",
    "# e = normalize(cew)\n",
    "# # e = cew\n",
    "# for line in lines:\n",
    "# #     ws = \"ልጅቱ ልጁ ወይዘሮ አቶ\".split(' ')\n",
    "#     if \":\" in line:\n",
    "#         continue\n",
    "#     try:\n",
    "#         ws = line[:-1].split(' ')\n",
    "#         v = normalize(vec(ws[1], e) - vec(ws[0], e) + vec(ws[2], e))\n",
    "#         V = np.dot(e, v)\n",
    "#         sort = list(sorted(enumerate(V[1:]), key=lambda x: x[1], reverse=True))\n",
    "#         so = [int2word[index[0]] for index in sort[:4]]\n",
    "#         if ws[-1] in so:\n",
    "#             print(so, ws[-1])\n",
    "# #         break\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = open('data/analogies.txt', encoding='utf-8').readlines()\n",
    "# words = []\n",
    "# for line in lines:\n",
    "#     ws = line[:-1].split(' ')\n",
    "#     in_vocab = True\n",
    "#     for w in ws:\n",
    "#         if w not in word2int:\n",
    "#             in_vocab = False\n",
    "#         if in_vocab:\n",
    "#             words.append(ws)\n",
    "# ws = []\n",
    "# for wl in words:\n",
    "#     s = ' '.join(wl)\n",
    "#     ws.append(s)\n",
    "# ws = '\\n'.join(ws)\n",
    "# open('data/newan.txt', mode='w', encoding='utf-8').write(ws)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = open('data/analogies.txt', encoding='utf-8').readlines()\n",
    "# words = []\n",
    "# lns = {}\n",
    "# for line in lines:\n",
    "#     ws = line[:-1].split(' ')\n",
    "#     in_vocab = True\n",
    "#     for w in ws:\n",
    "#         if w not in word2int:\n",
    "#             in_vocab = False\n",
    "#         if in_vocab:\n",
    "#             ln = ' '.join(ws)\n",
    "#             if ln not in lns:\n",
    "#                 lns[ln] = len(lns)\n",
    "#             ls = ' '.join(ws[2:4] + ws[0:2])\n",
    "#             if ln not in lns:\n",
    "#                 lns[ln] = len(lns)\n",
    "# ws = '\\n'.join(lns.keys())\n",
    "# open('data/newan2.txt', mode='w', encoding='utf-8').write(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
