{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amany\\anaconda2\\envs\\tf2\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import Dropout\n",
    "# from keras.layers import LSTM, TimeDistributed\n",
    "# from keras.layers import Concatenate, Flatten\n",
    "# from keras.layers import GRU, Conv2D, MaxPooling2D\n",
    "# from keras.layers import Input, Reshape, Dot, Add\n",
    "# from keras.models import Model\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.optimizers import RMSprop\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "# import keras\n",
    "# import keras.backend as K\n",
    "from data_handle import *\n",
    "from gensim_wrapper import *\n",
    "from utils import *\n",
    "import gensim\n",
    "import random\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "seed_val = 1000\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "# tf.set_random_seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58383\n"
     ]
    }
   ],
   "source": [
    "words = read_file(filename='data/news.txt')\n",
    "# unkown_word = \"<unk>\"\n",
    "# words = [unkown_word] + words\n",
    "xvocab, xword2int, xint2word = build_vocab(words)\n",
    "\n",
    "words, word2freq = min_count_threshold(words)\n",
    "vocab, word2int, int2word = build_vocab(words)\n",
    "print(len(vocab))\n",
    "# word2freq = get_frequency(words, word2int, int2word)\n",
    "unigrams = [word2freq[int2word[i]] for i in range(len(word2int))]\n",
    "\n",
    "char2int, int2char, char2tup, tup2char, n_consonant, n_vowel = build_charset()\n",
    "ns_unigrams = ns_sample(word2freq, word2int, int2word, .75)\n",
    "n_chars = 11 + 2 \n",
    "n_features = len(char2int)\n",
    "batch_size = 120\n",
    "embed_size = 100\n",
    "skip_window = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseVec(file, delimiter):\n",
    "    lines = open(file, encoding='utf8').readlines()\n",
    "    vocab_size, embed_size = [int(s) for s in lines[0].split()]\n",
    "    embeddings = np.zeros((vocab_size, embed_size), dtype=np.float64)\n",
    "#     print(embeddings.shape)\n",
    "    wordint = {}\n",
    "    for i in range(1, vocab_size+1):\n",
    "        try:\n",
    "            line = lines[i][:-1].split(delimiter)\n",
    "            word = line[0]\n",
    "#             if word in word2int:\n",
    "            wordvec = np.array([np.float64(j) for j in line[1:] if j != ''])\n",
    "            embeddings[len(wordint)] = wordvec\n",
    "            wordint[word] = len(wordint)\n",
    "        except Exception as e:\n",
    "#             print(lines[i])\n",
    "#             print(e)\n",
    "            pass\n",
    "#     print(len(wordint))\n",
    "    return embeddings, wordint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectArabicEval(result):\n",
    "    nsemantic = 0\n",
    "    nsynatctic = 0\n",
    "    for nrr in result:\n",
    "        if 'total' in nrr:\n",
    "            continue\n",
    "        if \"gram\" in nrr:\n",
    "            nsynatctic += result[nrr]/6\n",
    "        else:\n",
    "            nsemantic += result[nrr]/9\n",
    "    return nsynatctic, nsemantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder = 'results/'\n",
    "file_list = os.listdir(folder)[2:]\n",
    "row = \"{0},{1},{2},{3},{3},{4},{5},{6},{7},{8}\"\n",
    "csv = open('sg4.csv', mode='a')\n",
    "csv.write(row.format('data', 'size', 'model', 'embed_size', 'win_size', 'norm-syn', 'norm-sem', 'unnorm-syn', 'unnorm-sem'))\n",
    "csv.write('\\n')\n",
    "csv.close()\n",
    "for file in file_list:\n",
    "    line = open(folder + file).readline()\n",
    "    vals = file.split('_')\n",
    "    print(file)\n",
    "    data, model, embed_size, win_size, _ = vals\n",
    "\n",
    "    name = 'am'\n",
    "    size = 0\n",
    "    corpus =  'data/corpus/am16.txt'\n",
    "    question = 'data/newan2.txt'\n",
    "    if 'am' in data:\n",
    "        name = data[:2]\n",
    "        size = data[2:]\n",
    "    if 'ar' in data:\n",
    "        corpus =  'data/corpus/20'\n",
    "        name = data[:2]\n",
    "        size = data[2:]\n",
    "        question = 'data/corpus/ar_questions.txt'\n",
    "    if 'abj' in data:\n",
    "        name = 'abj'\n",
    "        size = data[3:]\n",
    "        corpus =  'data/corpus/abj-am-all.txt'\n",
    "        question = 'data/abj-am-analogy.txt'\n",
    "    if 'alpha' in data:\n",
    "        name = 'alpha'\n",
    "        size = data[5:]\n",
    "        corpus =  'data/corpus/alpha-all.txt'\n",
    "        question = 'data/alpha-am-analogy.txt'\n",
    "    seq_emb = parseVec(folder + file, ' ')\n",
    "#         text = open('data/grid/' + file).read()\n",
    "#         text = text.replace(\"'\", '\"')\n",
    "    emb = seq_emb[0]\n",
    "    wordint = seq_emb[1]\n",
    "    norm_res = evaluate(wordint, normalize(emb), embed_size=emb.shape[1], corpus=corpus, analogy=question)\n",
    "    unorm_res = evaluate(wordint, emb, embed_size=emb.shape[1], corpus=corpus, analogy=question)\n",
    "    nsyn, nsem = collectArabicEval(norm_res)\n",
    "    usyn, usem = collectArabicEval(unorm_res)\n",
    "#     nsyn, nsem = norm_res['syntactic'], norm_res['semantic']\n",
    "#     usyn, usem = unorm_res['syntactic'], unorm_res['semantic']\n",
    "    csv = open('sg4.csv', mode='a')\n",
    "    csv.write(row.format(name, size, model, embed_size, win_size, nsyn, nsem, usyn, usem))\n",
    "    csv.write('\\n')\n",
    "    csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_emb = parseVec(\"results/vec.txt\", ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226397\n",
      "{'syntactic': 2.5700934579439254, 'semantic': 0.36900369003690037, 'total': 1.7167381974248928}\n",
      "{'syntactic': 0.0, 'semantic': 0.0, 'total': 0.0}\n"
     ]
    }
   ],
   "source": [
    "emb = seq_emb[0]\n",
    "wordint = seq_emb[1]\n",
    "print(len(wordint))\n",
    "print(evaluate(wordint, normalize(emb), embed_size=emb.shape[1]))\n",
    "print(evaluate(wordint, emb, embed_size=emb.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb2D = np.zeros((seq_emb.shape[0], seq_emb.shape[1] * sem_emb.shape[1]))\n",
    "for i in range(seq_emb.shape[0]):\n",
    "    seq = seq_emb[i]\n",
    "    sem = sem_emb[i]\n",
    "    vec = np.outer(seq, sem).flatten()\n",
    "    emb2D[i] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('data/newan2.txt', encoding='utf-8').readlines()\n",
    "emb = emb2D\n",
    "correct = 0\n",
    "for line in lines:\n",
    "    if ':' in line:\n",
    "        continue\n",
    "    try:\n",
    "        line = [x.strip() for x in line[:-1].split(' ')]\n",
    "        vs = [emb[word2int[w]] for w in line]\n",
    "        v = vs[1] - vs[0] + vs[2]\n",
    "        sims = emb.dot(v)\n",
    "        sim_with_word = [(word, sims[word2int[word]])\n",
    "                             for word in word2int.keys()]\n",
    "        sorted_sim = sorted(sim_with_word, key=lambda t: t[1], reverse=True)[:4]\n",
    "#         print(type(sorted_sim[0][1]), sorted_sim[0][1])\n",
    "        result = [pred[0] for pred in sorted_sim]\n",
    "        if not np.isnan(sorted_sim[0][1]):\n",
    "#             print(result, line[-1])\n",
    "            if line[-1] in result:\n",
    "#                 print(result, line[-1])\n",
    "                correct += 1\n",
    "    except:\n",
    "        pass\n",
    "print(correct * 100.0 / (len(lines) - 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util = Utils(word2int, int2word, emb2D)\n",
    "# lines = open('data/newan2.txt', encoding='utf-8').readlines()\n",
    "# acc = np.array([0, 0])\n",
    "# total = np.array([0, 0])\n",
    "# ind = 0\n",
    "# for line in lines[1:]:\n",
    "#     if ':' in line[:-1]:\n",
    "#         ind = 1\n",
    "#         continue\n",
    "#     ws = line[:-1].split()\n",
    "#     result = util.solve(ws)\n",
    "#     if result is None:\n",
    "#         continue\n",
    "#     if ws[-1] in result:\n",
    "#         acc[ind] += 1\n",
    "#     total[ind] += 1\n",
    "# print(acc * 100 / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = open('data/analogies.txt', encoding='utf-8').readlines()\n",
    "# e = normalize(cew)\n",
    "# # e = cew\n",
    "# for line in lines:\n",
    "# #     ws = \"ልጅቱ ልጁ ወይዘሮ አቶ\".split(' ')\n",
    "#     if \":\" in line:\n",
    "#         continue\n",
    "#     try:\n",
    "#         ws = line[:-1].split(' ')\n",
    "#         v = normalize(vec(ws[1], e) - vec(ws[0], e) + vec(ws[2], e))\n",
    "#         V = np.dot(e, v)\n",
    "#         sort = list(sorted(enumerate(V[1:]), key=lambda x: x[1], reverse=True))\n",
    "#         so = [int2word[index[0]] for index in sort[:4]]\n",
    "#         if ws[-1] in so:\n",
    "#             print(so, ws[-1])\n",
    "# #         break\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = open('data/analogies.txt', encoding='utf-8').readlines()\n",
    "# words = []\n",
    "# for line in lines:\n",
    "#     ws = line[:-1].split(' ')\n",
    "#     in_vocab = True\n",
    "#     for w in ws:\n",
    "#         if w not in word2int:\n",
    "#             in_vocab = False\n",
    "#         if in_vocab:\n",
    "#             words.append(ws)\n",
    "# ws = []\n",
    "# for wl in words:\n",
    "#     s = ' '.join(wl)\n",
    "#     ws.append(s)\n",
    "# ws = '\\n'.join(ws)\n",
    "# open('data/newan.txt', mode='w', encoding='utf-8').write(ws)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = open('data/analogies.txt', encoding='utf-8').readlines()\n",
    "# words = []\n",
    "# lns = {}\n",
    "# for line in lines:\n",
    "#     ws = line[:-1].split(' ')\n",
    "#     in_vocab = True\n",
    "#     for w in ws:\n",
    "#         if w not in word2int:\n",
    "#             in_vocab = False\n",
    "#         if in_vocab:\n",
    "#             ln = ' '.join(ws)\n",
    "#             if ln not in lns:\n",
    "#                 lns[ln] = len(lns)\n",
    "#             ls = ' '.join(ws[2:4] + ws[0:2])\n",
    "#             if ln not in lns:\n",
    "#                 lns[ln] = len(lns)\n",
    "# ws = '\\n'.join(lns.keys())\n",
    "# open('data/newan2.txt', mode='w', encoding='utf-8').write(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
