{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "seed_val = 1000\n",
    "random.seed(seed_val)\n",
    "import numpy as np\n",
    "np.random.seed(seed_val)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(seed_val)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, TimeDistributed\n",
    "from keras.layers import Concatenate, Flatten\n",
    "from keras.layers import GRU, Conv2D, MaxPooling2D\n",
    "from keras.layers import Input, Reshape, Dot, Add\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from data_handle import *\n",
    "from gensim_wrapper import *\n",
    "from utils import *\n",
    "import gensim\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model_multi(n_chars, n_consonant, n_vowels, n_units):\n",
    "    root_word_input = Input(shape=(n_chars, (n_consonant + n_vowels), 1), name=\"root_word_input\")\n",
    "    \n",
    "    x = Conv2D(16, (5, 5), padding='same', activation='relu')(root_word_input)\n",
    "    x = MaxPooling2D(3, 3)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    state_h = Dense(n_units, activation='linear')(x)\n",
    "    outputs = []\n",
    "    for i in range(n_chars):\n",
    "        y = Dense(20, activation='relu')(state_h)\n",
    "        outputs.append(y)\n",
    "    con_softs = []\n",
    "    for i in range(n_chars):\n",
    "        y = Dense(n_consonant, activation='softmax')(outputs[i])\n",
    "        con_softs.append(y)\n",
    "    vow_softs = []\n",
    "    for i in range(n_chars):\n",
    "        y = Dense(n_consonant, activation='softmax')(outputs[i])\n",
    "        vow_softs.append(y)\n",
    "    model = Model(root_word_input, con)\n",
    "    \n",
    "    return main_model, encoder_model, decoder_model\n",
    "\n",
    "def pred_embeddings_multi(vocab, encoder, char2tup):\n",
    "    embeddings = np.ndarray((len(vocab), embed_size))\n",
    "    i = 0\n",
    "    vec_buffer, con_buffer = [], []\n",
    "    buffer_size = 10000\n",
    "    for wi, word in enumerate(vocab):\n",
    "        word = int2word[word2int[word]]\n",
    "        convec, vowvec = word2vec_seperated(char2tup, word, n_chars, n_consonant, n_vowel)\n",
    "#         convec = convec.reshape((-1, n_chars, n_consonant, 1))\n",
    "#         vowvec = convec.reshape((-1, n_chars, n_vowel, 1))\n",
    "#         mat = np.concatenate([convec, vowvec], axis=1)\n",
    "        \n",
    "        con_buffer.append(convec)\n",
    "        vec_buffer.append(vowvec)\n",
    "        if len(vec_buffer) == buffer_size or len(vocab) - wi < buffer_size:\n",
    "            buffer_np_c = np.stack(con_buffer).reshape((-1, n_chars, n_consonant, 1))\n",
    "            buffer_np_v = np.stack(vec_buffer).reshape((-1, n_chars, n_vowel, 1))\n",
    "            result = encoder.predict([buffer_np_c, buffer_np_v])\n",
    "            embeddings[i:i+len(buffer_np_c)] = result\n",
    "            i += len(con_buffer)\n",
    "            vec_buffer, con_buffer = [], []\n",
    "            if i % (4 *buffer_size) == 0:\n",
    "                print(\"Predicting: {0:.2f}%\".format((i * 100.0 / len(vocab))))\n",
    "    print(\"finished\")\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def generateSG(data, word2int,int2word, char2tup, skip_window, batch_size, n_consonant, n_vowels):\n",
    "    win_size = skip_window  \n",
    "    i = win_size\n",
    "    assert batch_size % (win_size) == 0\n",
    "    targets, target_inputs = {}, {}\n",
    "    for word in word2int.keys():\n",
    "        target = word + '|'\n",
    "        target_input = '&' + target \n",
    "        targets[word] = target\n",
    "        target_inputs[word] = target_input\n",
    "        \n",
    "    batch = 0\n",
    "    n_batchs = len(words) // batch_size\n",
    "    n_chars = 13\n",
    "    while True:\n",
    "        batch_input = []\n",
    "        batch_decoder_cons_input = []\n",
    "        batch_decoder_vow_input = []\n",
    "        batch_output_cons = []\n",
    "        batch_output_vow = []\n",
    "        for bi in range(0, batch_size, skip_window * 2 + 1):\n",
    "            context = data[i - win_size: i + win_size + 1]\n",
    "            target = [context.pop(win_size)] * (win_size * 2)\n",
    "#             print(context, target)\n",
    "            for input_word, output_word in zip(context, target):\n",
    "#                 print(target_inputs[output_word])\n",
    "                input_con, input_vow = word2vec_seperated(char2tup, input_word, n_chars, n_consonant, n_vowels)\n",
    "                target_con, target_vow = word2vec_seperated(char2tup,targets[output_word], n_chars, n_consonant, n_vowels)\n",
    "                decoder_con, decoder_vow = word2vec_seperated(char2tup,target_inputs[output_word], n_chars, n_consonant, n_vowels)\n",
    "                input_vec = np.concatenate([input_con, input_vow], axis=1).reshape((n_chars, (n_consonant + n_vowels), 1))\n",
    "                \n",
    "                batch_input.append(input_vec)\n",
    "                batch_decoder_cons_input.append(decoder_con)\n",
    "                batch_decoder_vow_input.append(decoder_vow)\n",
    "                batch_output_cons.append(target_con)\n",
    "                batch_output_vow.append(target_vow)\n",
    "\n",
    "            i += 1\n",
    "            if i + win_size + 1 > len(data):\n",
    "                i = win_size\n",
    "        batch_input = np.array(batch_input)\n",
    "        batch_decoder_cons_input = np.array(batch_decoder_cons_input)\n",
    "        batch_decoder_vow_input = np.array(batch_decoder_vow_input)\n",
    "        batch_output_cons = np.array(batch_output_cons)\n",
    "        batch_output_vow = np.array(batch_output_vow)\n",
    "#         print(batch_input.shape)\n",
    "        yield [batch_input, batch_decoder_cons_input, batch_decoder_vow_input], [batch_output_cons, batch_output_vow]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = read_file()\n",
    "vocab, word2int, int2word = build_vocab(words)\n",
    "int_words = words_to_ints(word2int, words)\n",
    "word2freq = get_frequency(words, word2int, int2word)\n",
    "char2int, int2char, char2tup, tup2char, n_consonant, n_vowel = build_charset()\n",
    "ns_unigrams = ns_sample(word2freq, word2int, int2word, .75)\n",
    "n_chars = 11 + 2 \n",
    "n_features = len(char2int)\n",
    "batch_size = 300\n",
    "embed_size = 100\n",
    "skip_window = 1\n",
    "\n",
    "\n",
    "n_batches = len(vocab)  // batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del multi_train\n",
    "    del multi_enc\n",
    "    del multi_dec\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "except:\n",
    "    pass\n",
    "multi_train, multi_enc, multi_dec = conv_model_multi(n_chars, n_consonant, n_vowel, embed_size)\n",
    "adam = keras.optimizers.Nadam(.001)\n",
    "multi_train.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['acc'])\n",
    "multi_gen = generateSG(words, char2tup, batch_size, n_consonant, n_vowel)\n",
    "# plot_model(multi_train)\n",
    "# multi_train.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
