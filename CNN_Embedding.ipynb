{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "seed_val = 1000\n",
    "random.seed(seed_val)\n",
    "import numpy as np\n",
    "np.random.seed(seed_val)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(seed_val)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, TimeDistributed\n",
    "from keras.layers import Concatenate, Flatten\n",
    "from keras.layers import GRU, Conv2D, MaxPooling2D, AveragePooling2D, AvgPool2D, MaxPool1D\n",
    "from keras.layers import Input, Reshape, Dot, Add\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from data_handle import *\n",
    "from gensim_wrapper import *\n",
    "from utils import *\n",
    "import gensim\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model_multi(n_chars, n_consonant, n_vowels, n_units):\n",
    "    root_word_input = Input(shape=(n_chars, n_consonant + n_vowels, 1), name=\"root_word_input\")\n",
    "    \n",
    "    x = Conv2D(10, (5, 5), padding='same', activation='relu')(root_word_input)\n",
    "    x = MaxPool1D(3, 3)(x)\n",
    "#     x = Conv2D(8, (3, 3), padding='same', activation='relu')(x)\n",
    "#     x = AveragePooling2D(3, 3)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "#     x = MaxPool1D(3)(x)\n",
    "    state_h = Dense(n_units, activation='linear')(x)\n",
    "    \n",
    "    consonant_decoder_inputs = Input(shape=(None, n_consonant), name=\"target_consonant\")\n",
    "    consonant_decoder_gru = GRU(n_units, return_sequences=True, return_state=True,  name=\"consonant_decoder_gru\")\n",
    "    consonant_decoder_outputs, _= consonant_decoder_gru(consonant_decoder_inputs, initial_state=state_h)\n",
    "    \n",
    "    vowel_decoder_inputs = Input(shape=(None, n_vowels), name=\"vowel_input\")\n",
    "    vowel_decoder_gru = GRU(n_units, return_sequences=True, return_state=True, name=\"vowl_decoder_gru\")\n",
    "    vowel_decoder_outputs, _= vowel_decoder_gru(vowel_decoder_inputs, initial_state=state_h)\n",
    "\n",
    "    consonant_decoder_dense = Dense(n_consonant, activation='softmax', name=\"consonant_output\")\n",
    "    consonant_decoder_outputs = consonant_decoder_dense(consonant_decoder_outputs)\n",
    "    \n",
    "    vowel_decoder_dense = Dense(n_vowels, activation='softmax', name=\"vowel_output\")\n",
    "    vowel_decoder_outputs = vowel_decoder_dense(vowel_decoder_outputs)\n",
    "    \n",
    "    main_model = Model([root_word_input, consonant_decoder_inputs, vowel_decoder_inputs], [consonant_decoder_outputs, vowel_decoder_outputs])\n",
    "    \n",
    "    encoder_model = Model(root_word_input, state_h)\n",
    "    \n",
    "    decoder_state_input_h = Input(shape=(n_units,))\n",
    "    \n",
    "    consonant_decoder_outputs, state_h= consonant_decoder_gru(consonant_decoder_inputs, initial_state=decoder_state_input_h)\n",
    "    consonant_decoder_outputs = consonant_decoder_dense(consonant_decoder_outputs)\n",
    "    \n",
    "    vowel_decoder_outputs, state_h= vowel_decoder_gru(vowel_decoder_inputs, initial_state=decoder_state_input_h)\n",
    "    vowel_decoder_outputs = vowel_decoder_dense(vowel_decoder_outputs)\n",
    "    \n",
    "    decoder_model = Model([consonant_decoder_inputs, vowel_decoder_inputs, decoder_state_input_h], [consonant_decoder_outputs, vowel_decoder_outputs, state_h])\n",
    "\n",
    "    return main_model, encoder_model, decoder_model\n",
    "\n",
    "def one_hot_sep(con, con_max, vow, vow_max):\n",
    "    con_vec = np.zeros((con_max, ))\n",
    "    con_vec[con] = 1\n",
    "    vow_vec = np.zeros((vow_max, ))\n",
    "    vow_vec[vow] = 1\n",
    "    return con_vec, vow_vec\n",
    "    \n",
    "def decode_multi_sequence(model, char2tup, tup2char, state, n_consonant, n_vowels):\n",
    "    con, vow = char2tup['&']\n",
    "    con_vec, vow_vec = one_hot_sep(con, n_consonant, vow, n_vowels) \n",
    "    con_vec = con_vec.reshape((1, 1, -1))\n",
    "    vow_vec = vow_vec.reshape((1, 1, -1))\n",
    "#     target_seq = np.concatenate([con_vec, vow_vec])\n",
    "    decoded_chars = []\n",
    "    for i in range(13):\n",
    "        con_vec, vow_vec, state = model.predict([con_vec, vow_vec, state])\n",
    "#         target_seq = np.concatenate([con_vec, vow_vec])\n",
    "        new_con_vec = np.zeros_like(con_vec)\n",
    "        new_con_vec[0, 0, np.argmax(con_vec[0, 0, :])] = 1\n",
    "        new_vow_vec = np.zeros_like(vow_vec)\n",
    "        new_vow_vec[0, 0, np.argmax(vow_vec[0, 0, :])] = 1\n",
    "        con_vec, vow_vec = new_con_vec, new_vow_vec\n",
    "        name = \"{0}-{1}\".format(np.argmax(con_vec[0, 0, :]), np.argmax(vow_vec[0, 0, :]))\n",
    "        try:\n",
    "            char = tup2char[name]\n",
    "        except:\n",
    "            char = ' '\n",
    "        decoded_chars += [char]\n",
    "    \n",
    "    return decoded_chars\n",
    "\n",
    "def pred_embeddings_multi(vocab, encoder, char2tup):\n",
    "    embeddings = np.ndarray((len(vocab), embed_size))\n",
    "    i = 0\n",
    "    buffer = []\n",
    "    buffer_size = 10000\n",
    "    for wi, word in enumerate(vocab):\n",
    "        word = int2word[word2int[word]]\n",
    "        convec, vowvec = word2vec_seperated(char2tup, word, n_chars, n_consonant, n_vowel)\n",
    "        convec = convec.reshape((-1, n_chars, n_consonant, 1))\n",
    "        vowvec = vowvec.reshape((-1, n_chars, n_vowel, 1))\n",
    "        mat = np.concatenate([convec, vowvec], axis=2)\n",
    "        buffer.append(mat)\n",
    "        if len(buffer) == buffer_size or len(vocab) - wi < buffer_size:\n",
    "            buffer_np = np.stack(buffer).reshape((-1, 13, 50, 1))\n",
    "            result = encoder.predict(buffer_np)\n",
    "            embeddings[i:i+len(buffer)] = result\n",
    "            i += len(buffer)\n",
    "            buffer = []\n",
    "            if i % (4 * buffer_size) == 0:\n",
    "                print(\"Predicting: {0:.2f}%\".format((i * 100.0 / len(vocab))))\n",
    "                \n",
    "    print(\"finished\")\n",
    "    return embeddings\n",
    "\n",
    "def generateSG(data, word2int,int2word, char2tup, skip_window, batch_size, n_consonant, n_vowels):\n",
    "    win_size = skip_window  \n",
    "    i = win_size\n",
    "    assert batch_size % (win_size) == 0\n",
    "    targets, target_inputs = {}, {}\n",
    "    for word in word2int.keys():\n",
    "        target = word + '|'\n",
    "        target_input = '&' + target \n",
    "        targets[word] = target\n",
    "        target_inputs[word] = target_input\n",
    "        \n",
    "    batch = 0\n",
    "    n_batchs = len(words) // batch_size\n",
    "    n_chars = 13\n",
    "    while True:\n",
    "        batch_input = []\n",
    "        batch_decoder_cons_input = []\n",
    "        batch_decoder_vow_input = []\n",
    "        batch_output_cons = []\n",
    "        batch_output_vow = []\n",
    "        for bi in range(0, batch_size, skip_window * 2 + 1):\n",
    "            context = data[i - win_size: i + win_size + 1]\n",
    "            target = [context.pop(win_size)] * (win_size * 2)\n",
    "#             print(context, target)\n",
    "            for input_word, output_word in zip(context, target):\n",
    "#                 print(target_inputs[output_word])\n",
    "                input_con, input_vow = word2vec_seperated(char2tup, input_word, n_chars, n_consonant, n_vowels)\n",
    "                target_con, target_vow = word2vec_seperated(char2tup,targets[output_word], n_chars, n_consonant, n_vowels)\n",
    "                decoder_con, decoder_vow = word2vec_seperated(char2tup,target_inputs[output_word], n_chars, n_consonant, n_vowels)\n",
    "                input_vec = np.concatenate([input_con, input_vow], axis=1).reshape((n_chars, (n_consonant + n_vowels), 1))\n",
    "                \n",
    "                batch_input.append(input_vec)\n",
    "                batch_decoder_cons_input.append(decoder_con)\n",
    "                batch_decoder_vow_input.append(decoder_vow)\n",
    "                batch_output_cons.append(target_con)\n",
    "                batch_output_vow.append(target_vow)\n",
    "\n",
    "            i += 1\n",
    "            if i + win_size + 1 > len(data):\n",
    "                i = win_size\n",
    "        batch_input = np.array(batch_input)\n",
    "        batch_decoder_cons_input = np.array(batch_decoder_cons_input)\n",
    "        batch_decoder_vow_input = np.array(batch_decoder_vow_input)\n",
    "        batch_output_cons = np.array(batch_output_cons)\n",
    "        batch_output_vow = np.array(batch_output_vow)\n",
    "#         print(batch_input.shape)\n",
    "        yield [batch_input, batch_decoder_cons_input, batch_decoder_vow_input], [batch_output_cons, batch_output_vow]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = read_file()\n",
    "vocab, word2int, int2word = build_vocab(words)\n",
    "int_words = words_to_ints(word2int, words)\n",
    "word2freq = get_frequency(words, word2int, int2word)\n",
    "char2int, int2char, char2tup, tup2char, n_consonant, n_vowel = build_charset()\n",
    "ns_unigrams = ns_sample(word2freq, word2int, int2word, .75)\n",
    "n_chars = 11 + 2 \n",
    "n_features = len(char2int)\n",
    "batch_size = 300\n",
    "embed_size = 100\n",
    "skip_window = 1\n",
    "\n",
    "# words = read_file()\n",
    "# unkown_word = \"<unk>\"\n",
    "# # words = [unkown_word] + words\n",
    "# xvocab, xword2int, xint2word = build_vocab(words)\n",
    "\n",
    "# words, word2freq = min_count_threshold(words)\n",
    "\n",
    "# vocab, word2int, int2word = build_vocab(words)\n",
    "# print(len(vocab))\n",
    "# # word2freq = get_frequency(words, word2int, int2word)\n",
    "# unigrams = [word2freq[int2word[i]] for i in range(len(word2int))]\n",
    "\n",
    "# char2int, int2char, char2tup, tup2char, n_consonant, n_vowel = build_charset()\n",
    "# ns_unigrams = ns_sample(word2freq, word2int, int2word, .75)\n",
    "# del words[0]\n",
    "# del word2int['<unk>']\n",
    "# n_chars = 11 + 2 \n",
    "# n_features = len(char2int)\n",
    "# batch_size = 500\n",
    "# embed_size = 100\n",
    "# skip_window = 1\n",
    "\n",
    "n_batches = len(vocab)  // batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensg = generateSG(words, word2int, int2word, char2tup, skip_window, batch_size, n_consonant, n_vowel)\n",
    "# [x1, x2, x3], [y1, y2] = next(gensg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer max_pooling1d_1: expected ndim=3, found ndim=4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b6731917577c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmulti_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_enc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_dec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_model_multi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_chars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_consonant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_vowel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0madam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNadam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmulti_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-5a4cdd7c58b2>\u001b[0m in \u001b[0;36mconv_model_multi\u001b[1;34m(n_chars, n_consonant, n_vowels, n_units)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_word_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMaxPool1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m#     x = Conv2D(8, (3, 3), padding='same', activation='relu')(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#     x = AveragePooling2D(3, 3)(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    472\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m                                      str(K.ndim(x)))\n\u001b[0m\u001b[0;32m    475\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 is incompatible with layer max_pooling1d_1: expected ndim=3, found ndim=4"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del multi_train\n",
    "    del multi_enc\n",
    "    del multi_dec\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "except:\n",
    "    pass\n",
    "multi_train, multi_enc, multi_dec = conv_model_multi(n_chars, n_consonant, n_vowel, embed_size)\n",
    "adam = keras.optimizers.Nadam(.001)\n",
    "multi_train.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['acc'])\n",
    "multi_gen = generate_word_images_multi(vocab, char2tup, batch_size, n_consonant, n_vowel)\n",
    "# words, char2int, char2tup, batch_size, n_consonant, n_vowels\n",
    "# plot_model(multi_train)\n",
    "# multi_train.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# SVG(model_to_dot(multi_train, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = multi_train.fit_generator(multi_gen, steps_per_epoch=n_batches, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del int2word[0]\n",
    "# i = vocab.index('<unk>')\n",
    "# del vocab[i]\n",
    "\n",
    "embeddings = pred_embeddings_multi(vocab, multi_enc, char2tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(embed_size=cew.shape[1],final_embedding=normalize(cew), word2int=word2int) \n",
    "# evaluate(embed_size=cew.shape[1],final_embedding=cew, word2int=word2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"results/seq_k\", embeddings)\n",
    "# embeddings = np.vstack([np.zeros((1, embed_size)), embeddings])\n",
    "# print(embeddings.shape, len(vocab))\n",
    "# file = open(\"results/c_v.txt\", encoding='utf8', mode='w')\n",
    "# file.write(\"{0} {1}\\n\".format(len(vocab), embed_size))\n",
    "# for word, index in word2int.items():\n",
    "#     e = embeddings[index]\n",
    "#     e = ' '.join(map(lambda x: str(x), e))\n",
    "#     file.write(\"{0} {1}\\n\".format(word, e))\n",
    "# file.close()\n",
    "# print(evaluate(word2int, normalize(embeddings)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
