{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "from scipy.spatial.distance import cosine\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Data size 10193286  Vocab Size 610151\n"
     ]
    }
   ],
   "source": [
    "filename = \"data/all.txt\"\n",
    "\n",
    "words = open(filename, encoding='utf8').read().split()\n",
    "vocab = list(set(words))\n",
    "print('ከባንኮክ' in words)\n",
    "\n",
    "vocabulary_size = len(vocab)\n",
    "print('Data size', len(words), ' Vocab Size', vocabulary_size)\n",
    "\n",
    "def build_dataset(words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "\n",
    "    dictionary = dict()\n",
    "    for word in words:\n",
    "        if word not in dictionary:\n",
    "            dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    for word in words:\n",
    "        index = dictionary[word]\n",
    "        data.append(index)\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, dictionary, reversed_dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] ['ጋዜጠኛ', 'ተመስገን', 'ደሳለኝ', 'በጠበቃው', 'በአቶ', 'አምሐ', 'መኮንን', 'አማካይነት', 'በፌዴራል', 'ጠቅላይ']\n"
     ]
    }
   ],
   "source": [
    "data, dictionary, reverse_dictionary = build_dataset(words)\n",
    "del words\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "\n",
    "data_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ደሳለኝ -> 4 በአቶ\n",
      "2 ደሳለኝ -> 3 በጠበቃው\n",
      "3 በጠበቃው -> 5 አምሐ\n",
      "3 በጠበቃው -> 1 ተመስገን\n",
      "4 በአቶ -> 2 ደሳለኝ\n",
      "4 በአቶ -> 6 መኮንን\n",
      "5 አምሐ -> 3 በጠበቃው\n",
      "5 አምሐ -> 4 በአቶ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Function to generate a training batch for the skip-gram model.\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "#     batch_size = batch_size // 2\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(\n",
    "        maxlen=span)  # pylint: disable=redefined-builtin\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span\n",
    "    for i in range(batch_size // num_skips):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "        if data_index == len(data):\n",
    "            buffer.extend(data[0:span])\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "            \n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=2)\n",
    "for i in range(len(batch)):\n",
    "    print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0],\n",
    "          reverse_dictionary[labels[i, 0]])\n",
    "\n",
    "# Step 4: Build and train a skip-gram model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1  # How many words to consider left and right.\n",
    "num_skips = 2  # How many times to reuse an input to generate a label.\n",
    "num_sampled = 64  # Number of negative examples to sample.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. These 3 variables are used only for\n",
    "# displaying model accuracy, they don't affect calculation.\n",
    "valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "np.random.seed(1000)\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "n_words = len(data)\n",
    "epoches = 10\n",
    "steps_per_batch = n_words // batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim loaded\n"
     ]
    }
   ],
   "source": [
    "import gensim, logging\n",
    "import collections\n",
    "\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "sentenses = open(filename, encoding='utf-8').read().split('*')\n",
    "sentenses = [s.strip().split() for s in sentenses]\n",
    "gensim_model = gensim.models.Word2Vec(sentenses, size=embedding_size, iter=1, min_count=1)\n",
    "print(\"Gensim loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    with tf.name_scope('inputs'):\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "        \n",
    "\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/GPU:0'):\n",
    "        # Look up embeddings for inputs.\n",
    "        with tf.name_scope('embeddings'):\n",
    "            embeddings = tf.Variable(\n",
    "                tf.random_normal([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "            embed = tf.nn.embedding_lookup(embeddings, train_inputs)    \n",
    "            print(embed.get_shape())\n",
    "\n",
    "        # Construct the variables for the NCE loss\n",
    "        with tf.name_scope('weights'):\n",
    "            nce_weights = tf.Variable(\n",
    "                tf.truncated_normal(\n",
    "                    [vocabulary_size, embedding_size],\n",
    "                    stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        with tf.name_scope('biases'):\n",
    "            nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "    # time we evaluate the loss.\n",
    "    # Explanation of the meaning of NCE loss:\n",
    "    #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(\n",
    "                weights=nce_weights,\n",
    "                biases=nce_biases,\n",
    "                labels=train_labels,\n",
    "                inputs=embed,\n",
    "                num_sampled=num_sampled,\n",
    "                num_classes=vocabulary_size))\n",
    "\n",
    "    # Add the loss value as a scalar to summary.\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norms = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norms\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(\n",
    "        valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Merge all summaries.\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Progress: 0.00% Averlage loss: 439.06\n",
      "Progress: 1.00% Averlage loss: 279.37\n",
      "Progress: 2.00% Averlage loss: 229.89\n",
      "Progress: 3.00% Averlage loss: 213.15\n",
      "Progress: 4.00% Averlage loss: 194.85\n",
      "Progress: 5.00% Averlage loss: 183.26\n",
      "Progress: 6.00% Averlage loss: 172.58\n",
      "Progress: 7.00% Averlage loss: 162.15\n",
      "Progress: 8.00% Averlage loss: 167.28\n",
      "Progress: 9.00% Averlage loss: 158.53\n",
      "Progress: 10.00% Averlage loss: 151.34\n",
      "Progress: 11.00% Averlage loss: 141.99\n",
      "Progress: 11.99% Averlage loss: 137.64\n",
      "Progress: 12.99% Averlage loss: 133.67\n",
      "Progress: 13.99% Averlage loss: 127.64\n",
      "Progress: 14.99% Averlage loss: 124.86\n",
      "Progress: 15.99% Averlage loss: 121.60\n",
      "Progress: 16.99% Averlage loss: 115.53\n",
      "Progress: 17.99% Averlage loss: 112.51\n",
      "Progress: 18.99% Averlage loss: 112.41\n",
      "Progress: 19.99% Averlage loss: 106.94\n",
      "Progress: 20.99% Averlage loss: 105.26\n",
      "Progress: 21.99% Averlage loss: 100.33\n",
      "Progress: 22.99% Averlage loss: 99.36\n",
      "Progress: 23.99% Averlage loss: 97.28\n",
      "Progress: 24.99% Averlage loss: 94.19\n",
      "Progress: 25.99% Averlage loss: 93.03\n",
      "Progress: 26.99% Averlage loss: 91.78\n",
      "Progress: 27.99% Averlage loss: 88.43\n",
      "Progress: 28.99% Averlage loss: 86.83\n",
      "Progress: 29.99% Averlage loss: 84.55\n",
      "Progress: 30.99% Averlage loss: 82.93\n",
      "Progress: 31.99% Averlage loss: 80.72\n",
      "Progress: 32.99% Averlage loss: 81.32\n",
      "Progress: 33.99% Averlage loss: 78.04\n",
      "Progress: 34.98% Averlage loss: 76.43\n",
      "Progress: 35.98% Averlage loss: 74.68\n",
      "Progress: 36.98% Averlage loss: 72.86\n",
      "Progress: 37.98% Averlage loss: 73.48\n",
      "Progress: 38.98% Averlage loss: 70.19\n",
      "Progress: 39.98% Averlage loss: 71.32\n",
      "Progress: 40.98% Averlage loss: 69.13\n",
      "Progress: 41.98% Averlage loss: 69.92\n",
      "Progress: 42.98% Averlage loss: 68.72\n",
      "Progress: 43.98% Averlage loss: 65.44\n",
      "Progress: 44.98% Averlage loss: 66.57\n",
      "Progress: 45.98% Averlage loss: 64.10\n",
      "Progress: 46.98% Averlage loss: 64.29\n",
      "Progress: 47.98% Averlage loss: 63.22\n",
      "Progress: 48.98% Averlage loss: 60.94\n",
      "Progress: 49.98% Averlage loss: 60.94\n",
      "Progress: 50.98% Averlage loss: 59.08\n",
      "Progress: 51.98% Averlage loss: 58.44\n",
      "Progress: 52.98% Averlage loss: 57.54\n",
      "Progress: 53.98% Averlage loss: 57.51\n",
      "Progress: 54.98% Averlage loss: 56.03\n",
      "Progress: 55.98% Averlage loss: 55.58\n",
      "Progress: 56.97% Averlage loss: 54.11\n",
      "Progress: 57.97% Averlage loss: 52.73\n",
      "Progress: 58.97% Averlage loss: 52.43\n",
      "Progress: 59.97% Averlage loss: 52.99\n",
      "Progress: 60.97% Averlage loss: 51.60\n",
      "Progress: 61.97% Averlage loss: 51.66\n",
      "Progress: 62.97% Averlage loss: 51.66\n",
      "Progress: 63.97% Averlage loss: 48.51\n",
      "Progress: 64.97% Averlage loss: 49.36\n",
      "Progress: 65.97% Averlage loss: 49.88\n",
      "Progress: 66.97% Averlage loss: 48.25\n",
      "Progress: 67.97% Averlage loss: 45.15\n",
      "Progress: 68.97% Averlage loss: 47.19\n",
      "Progress: 69.97% Averlage loss: 46.76\n",
      "Progress: 70.97% Averlage loss: 45.84\n",
      "Progress: 71.97% Averlage loss: 45.23\n",
      "Progress: 72.97% Averlage loss: 45.40\n",
      "Progress: 73.97% Averlage loss: 44.43\n",
      "Progress: 74.97% Averlage loss: 43.49\n",
      "Progress: 75.97% Averlage loss: 43.63\n",
      "Progress: 76.97% Averlage loss: 42.25\n",
      "Progress: 77.97% Averlage loss: 42.40\n",
      "Progress: 78.97% Averlage loss: 42.80\n",
      "Progress: 79.96% Averlage loss: 40.73\n",
      "Progress: 80.96% Averlage loss: 40.14\n",
      "Progress: 81.96% Averlage loss: 38.83\n",
      "Progress: 82.96% Averlage loss: 39.96\n",
      "Progress: 83.96% Averlage loss: 39.74\n",
      "Progress: 84.96% Averlage loss: 37.67\n",
      "Progress: 85.96% Averlage loss: 38.63\n",
      "Progress: 86.96% Averlage loss: 38.98\n",
      "Progress: 87.96% Averlage loss: 37.89\n",
      "Progress: 88.96% Averlage loss: 41.29\n",
      "Progress: 89.96% Averlage loss: 40.46\n",
      "Progress: 90.96% Averlage loss: 40.41\n",
      "Progress: 91.96% Averlage loss: 40.11\n",
      "Progress: 92.96% Averlage loss: 39.17\n",
      "Progress: 93.96% Averlage loss: 38.99\n",
      "Progress: 94.96% Averlage loss: 39.34\n",
      "Progress: 95.96% Averlage loss: 36.69\n",
      "Progress: 96.96% Averlage loss: 37.87\n",
      "Progress: 97.96% Averlage loss: 37.55\n",
      "Progress: 98.96% Averlage loss: 35.48\n",
      "Progress: 99.96% Averlage loss: 35.96\n",
      "Progress: 100.96% Averlage loss: 37.03\n",
      "Progress: 101.96% Averlage loss: 35.50\n",
      "Progress: 102.95% Averlage loss: 34.21\n",
      "Progress: 103.95% Averlage loss: 34.77\n",
      "Progress: 104.95% Averlage loss: 33.78\n",
      "Progress: 105.95% Averlage loss: 33.46\n",
      "Progress: 106.95% Averlage loss: 32.87\n",
      "Progress: 107.95% Averlage loss: 33.66\n",
      "Progress: 108.95% Averlage loss: 34.45\n",
      "Progress: 109.95% Averlage loss: 32.99\n",
      "Progress: 110.95% Averlage loss: 32.82\n",
      "Progress: 111.95% Averlage loss: 32.61\n",
      "Progress: 112.95% Averlage loss: 32.35\n",
      "Progress: 113.95% Averlage loss: 31.90\n",
      "Progress: 114.95% Averlage loss: 31.52\n",
      "Progress: 115.95% Averlage loss: 30.65\n",
      "Progress: 116.95% Averlage loss: 30.46\n",
      "Progress: 117.95% Averlage loss: 30.09\n",
      "Progress: 118.95% Averlage loss: 29.99\n",
      "Progress: 119.95% Averlage loss: 30.38\n",
      "Progress: 120.95% Averlage loss: 28.63\n",
      "Progress: 121.95% Averlage loss: 28.47\n",
      "Progress: 122.95% Averlage loss: 29.21\n",
      "Progress: 123.95% Averlage loss: 28.84\n",
      "Progress: 124.95% Averlage loss: 27.38\n",
      "Progress: 125.94% Averlage loss: 28.02\n",
      "Progress: 126.94% Averlage loss: 28.64\n",
      "Progress: 127.94% Averlage loss: 27.89\n",
      "Progress: 128.94% Averlage loss: 27.61\n",
      "Progress: 129.94% Averlage loss: 27.38\n",
      "Progress: 130.94% Averlage loss: 27.71\n",
      "Progress: 131.94% Averlage loss: 26.29\n",
      "Progress: 132.94% Averlage loss: 26.67\n",
      "Progress: 133.94% Averlage loss: 26.81\n",
      "Progress: 134.94% Averlage loss: 26.10\n",
      "Progress: 135.94% Averlage loss: 26.27\n",
      "Progress: 136.94% Averlage loss: 26.92\n",
      "Progress: 137.94% Averlage loss: 26.27\n",
      "Progress: 138.94% Averlage loss: 25.54\n",
      "Progress: 139.94% Averlage loss: 25.36\n",
      "Progress: 140.94% Averlage loss: 24.77\n",
      "Progress: 141.94% Averlage loss: 25.10\n",
      "Progress: 142.94% Averlage loss: 25.41\n",
      "Progress: 143.94% Averlage loss: 25.82\n",
      "Progress: 144.94% Averlage loss: 25.64\n",
      "Progress: 145.94% Averlage loss: 23.95\n",
      "Progress: 146.94% Averlage loss: 23.26\n",
      "Progress: 147.93% Averlage loss: 23.34\n",
      "Progress: 148.93% Averlage loss: 23.47\n",
      "Progress: 149.93% Averlage loss: 23.35\n",
      "Progress: 150.93% Averlage loss: 23.22\n",
      "Progress: 151.93% Averlage loss: 22.27\n",
      "Progress: 152.93% Averlage loss: 22.61\n",
      "Progress: 153.93% Averlage loss: 23.08\n",
      "Progress: 154.93% Averlage loss: 23.91\n",
      "Progress: 155.93% Averlage loss: 23.40\n",
      "Progress: 156.93% Averlage loss: 23.74\n",
      "Progress: 157.93% Averlage loss: 22.02\n",
      "Progress: 158.93% Averlage loss: 22.00\n",
      "Progress: 159.93% Averlage loss: 21.49\n",
      "Progress: 160.93% Averlage loss: 21.76\n",
      "Progress: 161.93% Averlage loss: 22.17\n",
      "Progress: 162.93% Averlage loss: 20.37\n",
      "Progress: 163.93% Averlage loss: 20.74\n",
      "Progress: 164.93% Averlage loss: 20.37\n",
      "Progress: 165.93% Averlage loss: 19.97\n",
      "Progress: 166.93% Averlage loss: 20.04\n",
      "Progress: 167.93% Averlage loss: 20.45\n",
      "Progress: 168.93% Averlage loss: 18.90\n",
      "Progress: 169.93% Averlage loss: 20.09\n",
      "Progress: 170.92% Averlage loss: 20.77\n",
      "Progress: 171.92% Averlage loss: 18.86\n",
      "Progress: 172.92% Averlage loss: 19.88\n",
      "Progress: 173.92% Averlage loss: 19.02\n",
      "Progress: 174.92% Averlage loss: 18.98\n",
      "Progress: 175.92% Averlage loss: 19.35\n",
      "Progress: 176.92% Averlage loss: 19.19\n",
      "Progress: 177.92% Averlage loss: 19.30\n",
      "Progress: 178.92% Averlage loss: 18.64\n",
      "Progress: 179.92% Averlage loss: 19.10\n",
      "Progress: 180.92% Averlage loss: 18.20\n",
      "Progress: 181.92% Averlage loss: 17.97\n",
      "Progress: 182.92% Averlage loss: 18.12\n",
      "Progress: 183.92% Averlage loss: 18.26\n",
      "Progress: 184.92% Averlage loss: 18.07\n",
      "Progress: 185.92% Averlage loss: 17.94\n",
      "Progress: 186.92% Averlage loss: 18.31\n",
      "Progress: 187.92% Averlage loss: 17.57\n",
      "Progress: 188.92% Averlage loss: 18.04\n",
      "Progress: 189.92% Averlage loss: 17.75\n",
      "Progress: 190.92% Averlage loss: 17.33\n",
      "Progress: 191.92% Averlage loss: 16.92\n",
      "Progress: 192.92% Averlage loss: 17.82\n",
      "Progress: 193.91% Averlage loss: 17.91\n",
      "Progress: 194.91% Averlage loss: 17.13\n",
      "Progress: 195.91% Averlage loss: 17.46\n",
      "Progress: 196.91% Averlage loss: 17.56\n",
      "Progress: 197.91% Averlage loss: 16.57\n",
      "Progress: 198.91% Averlage loss: 16.80\n",
      "Progress: 199.91% Averlage loss: 17.31\n"
     ]
    }
   ],
   "source": [
    "epoches = 2\n",
    "num_steps = steps_per_batch * epoches\n",
    "log_on = num_steps // (100 // epoches)\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # Open a writer to write summaries.\n",
    "    writer = tf.summary.FileWriter('./log', session.graph)\n",
    "\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, num_skips,\n",
    "                                                    skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        # Define metadata variable.\n",
    "        run_metadata = tf.RunMetadata()\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        # Also, evaluate the merged op to get all summaries from the returned \"summary\" variable.\n",
    "        # Feed metadata variable to session for visualizing the graph in TensorBoard.\n",
    "        _, summary, loss_val = session.run(\n",
    "            [optimizer, merged, loss],\n",
    "            feed_dict=feed_dict,\n",
    "            run_metadata=run_metadata)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        # Add returned summaries to writer in each step.\n",
    "        writer.add_summary(summary, step)\n",
    "        # Add metadata to visualize the graph for the last run.\n",
    "        if step == (num_steps - 1):\n",
    "            writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "\n",
    "        if step % log_on == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= log_on\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            log_text = \"Progress: {0:.2f}% Averlage loss: {1:.2f}\".format((step * 100 / steps_per_batch), average_loss)\n",
    "            print(log_text)\n",
    "            average_loss = 0\n",
    "\n",
    "#         # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "#         if step % 10000 == 0:\n",
    "#             sim = similarity.eval()\n",
    "#             for i in xrange(valid_size):\n",
    "#                 valid_word = reverse_dictionary[valid_examples[i]]\n",
    "#                 top_k = 8  # number of nearest neighbors\n",
    "#                 nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "#                 log_str = 'Nearest to %s:' % valid_word\n",
    "#                 for k in xrange(top_k):\n",
    "#                     close_word = reverse_dictionary[nearest[k]]\n",
    "#                     log_str = '%s %s,' % (log_str, close_word)\n",
    "#                 print(log_str)\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "    \n",
    "    # Write corresponding labels for the embeddings.\n",
    "    with open('./log/metadata.tsv', 'w', encoding='utf8') as f:\n",
    "        for i in xrange(vocabulary_size):\n",
    "            f.write(reverse_dictionary[i] + '\\n')\n",
    "\n",
    "    # Save the model for checkpoints.\n",
    "    saver.save(session, os.path.join('./log', 'model.ckpt'))\n",
    "\n",
    "    # Create a configuration for visualizing embeddings with the labels in TensorBoard.\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding_conf = config.embeddings.add()\n",
    "    embedding_conf.tensor_name = embeddings.name\n",
    "    embedding_conf.metadata_path = os.path.join('./log', 'metadata.tsv')\n",
    "    projector.visualize_embeddings(writer, config)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in to gensim\n"
     ]
    }
   ],
   "source": [
    "for gindex in range(len(gensim_model.wv.index2word)):\n",
    "    gword = gensim_model.wv.index2word[gindex]\n",
    "    index = dictionary[gword]\n",
    "    embedding = final_embeddings[index]\n",
    "    gensim_model.wv.vectors[gindex] = embedding\n",
    "print(\"Loaded in to gensim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `accuracy` (Method will be removed in 4.0.0, use self.wv.evaluate_word_analogies() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'section': 'syntactic evaluation',\n",
       "  'correct': [],\n",
       "  'incorrect': [('ትሄዳለች', 'እየሄደች', 'ይበላል', 'እየበላ'),\n",
       "   ('ትሄዳለች', 'እየሄደች', 'ታደርጋለች', 'እያደረገች'),\n",
       "   ('ትሄዳለች', 'እየሄደች', 'ይመለከታል', 'እየተመለከተ'),\n",
       "   ('ትሄዳለች', 'እየሄደች', 'ይሄዳል', 'እየሄደ'),\n",
       "   ('ትሄዳለች', 'እየሄደች', 'ትላለች', 'እያለች'),\n",
       "   ('ትሄዳለች', 'እየሄደች', 'ያስባል', 'እያሰበ'),\n",
       "   ('ትሄዳለች', 'እየሄደች', 'ያወጣል', 'እያወጣ'),\n",
       "   ('ትሄዳለች', 'እየሄደች', 'ይመስላል', 'እየመሰለ'),\n",
       "   ('ይበላል', 'እየበላ', 'ትሄዳለች', 'እየሄደች'),\n",
       "   ('ይበላል', 'እየበላ', 'ታደርጋለች', 'እያደረገች'),\n",
       "   ('ይበላል', 'እየበላ', 'ይመለከታል', 'እየተመለከተ'),\n",
       "   ('ይበላል', 'እየበላ', 'ይሄዳል', 'እየሄደ'),\n",
       "   ('ይበላል', 'እየበላ', 'ትላለች', 'እያለች'),\n",
       "   ('ይበላል', 'እየበላ', 'ያስባል', 'እያሰበ'),\n",
       "   ('ይበላል', 'እየበላ', 'ያወጣል', 'እያወጣ'),\n",
       "   ('ይበላል', 'እየበላ', 'ይመስላል', 'እየመሰለ'),\n",
       "   ('ታደርጋለች', 'እያደረገች', 'ትሄዳለች', 'እየሄደች'),\n",
       "   ('ታደርጋለች', 'እያደረገች', 'ይበላል', 'እየበላ'),\n",
       "   ('ታደርጋለች', 'እያደረገች', 'ይመለከታል', 'እየተመለከተ'),\n",
       "   ('አድርጋለች', 'ልታደርግ', 'ሄዷል', 'ሊሄድ'),\n",
       "   ('ታደርጋለች', 'እያደረገች', 'ይሄዳል', 'እየሄደ'),\n",
       "   ('አድርጋለች', 'ልታደርግ', 'ተቀምጧል', 'ሊቀመጥ'),\n",
       "   ('አድርጋለች', 'ልታደርግ', 'ብላለች', 'ልትል'),\n",
       "   ('ታደርጋለች', 'እያደረገች', 'ትላለች', 'እያለች'),\n",
       "   ('ታደርጋለች', 'እያደረገች', 'ያስባል', 'እያሰበ'),\n",
       "   ('አድርጋለች', 'ልታደርግ', 'ጀምሯል', 'ሊጀምር'),\n",
       "   ('አድርጋለች', 'ልታደርግ', 'አውጥቷል', 'ሊያወጣ'),\n",
       "   ('ታደርጋለች', 'እያደረገች', 'ያወጣል', 'እያወጣ'),\n",
       "   ('አድርጋለች', 'ልታደርግ', 'መስሏል', 'ሊመስል'),\n",
       "   ('ታደርጋለች', 'እያደረገች', 'ይመስላል', 'እየመሰለ'),\n",
       "   ('አድርጋለች', 'ልታደርግ', 'አሸነፈ', 'ሊያሸንፍ'),\n",
       "   ('ይመለከታል', 'እየተመለከተ', 'ትሄዳለች', 'እየሄደች'),\n",
       "   ('ይመለከታል', 'እየተመለከተ', 'ይበላል', 'እየበላ'),\n",
       "   ('ይመለከታል', 'እየተመለከተ', 'ታደርጋለች', 'እያደረገች'),\n",
       "   ('ይመለከታል', 'እየተመለከተ', 'ይሄዳል', 'እየሄደ'),\n",
       "   ('ይመለከታል', 'እየተመለከተ', 'ትላለች', 'እያለች'),\n",
       "   ('ይመለከታል', 'እየተመለከተ', 'ያስባል', 'እያሰበ'),\n",
       "   ('ይመለከታል', 'እየተመለከተ', 'ያወጣል', 'እያወጣ'),\n",
       "   ('ይመለከታል', 'እየተመለከተ', 'ይመስላል', 'እየመሰለ'),\n",
       "   ('ይሄዳል', 'እየሄደ', 'ትሄዳለች', 'እየሄደች'),\n",
       "   ('ይሄዳል', 'እየሄደ', 'ይበላል', 'እየበላ'),\n",
       "   ('ሄዷል', 'ሊሄድ', 'አድርጋለች', 'ልታደርግ'),\n",
       "   ('ይሄዳል', 'እየሄደ', 'ታደርጋለች', 'እያደረገች'),\n",
       "   ('ይሄዳል', 'እየሄደ', 'ይመለከታል', 'እየተመለከተ'),\n",
       "   ('ሄዷል', 'ሊሄድ', 'ተቀምጧል', 'ሊቀመጥ'),\n",
       "   ('ሄዷል', 'ሊሄድ', 'ብላለች', 'ልትል'),\n",
       "   ('ይሄዳል', 'እየሄደ', 'ትላለች', 'እያለች'),\n",
       "   ('ይሄዳል', 'እየሄደ', 'ያስባል', 'እያሰበ'),\n",
       "   ('ሄዷል', 'ሊሄድ', 'ጀምሯል', 'ሊጀምር'),\n",
       "   ('ሄዷል', 'ሊሄድ', 'አውጥቷል', 'ሊያወጣ'),\n",
       "   ('ይሄዳል', 'እየሄደ', 'ያወጣል', 'እያወጣ'),\n",
       "   ('ሄዷል', 'ሊሄድ', 'መስሏል', 'ሊመስል'),\n",
       "   ('ይሄዳል', 'እየሄደ', 'ይመስላል', 'እየመሰለ'),\n",
       "   ('ሄዷል', 'ሊሄድ', 'አሸነፈ', 'ሊያሸንፍ'),\n",
       "   ('ተቀምጧል', 'ሊቀመጥ', 'አድርጋለች', 'ልታደርግ'),\n",
       "   ('ተቀምጧል', 'ሊቀመጥ', 'ሄዷል', 'ሊሄድ'),\n",
       "   ('ተቀምጧል', 'ሊቀመጥ', 'ብላለች', 'ልትል'),\n",
       "   ('ተቀምጧል', 'ሊቀመጥ', 'ጀምሯል', 'ሊጀምር'),\n",
       "   ('ተቀምጧል', 'ሊቀመጥ', 'አውጥቷል', 'ሊያወጣ'),\n",
       "   ('ተቀምጧል', 'ሊቀመጥ', 'መስሏል', 'ሊመስል'),\n",
       "   ('ተቀምጧል', 'ሊቀመጥ', 'አሸነፈ', 'ሊያሸንፍ'),\n",
       "   ('ትላለች', 'እያለች', 'ትሄዳለች', 'እየሄደች'),\n",
       "   ('ትላለች', 'እያለች', 'ይበላል', 'እየበላ'),\n",
       "   ('ብላለች', 'ልትል', 'አድርጋለች', 'ልታደርግ'),\n",
       "   ('ትላለች', 'እያለች', 'ታደርጋለች', 'እያደረገች'),\n",
       "   ('ትላለች', 'እያለች', 'ይመለከታል', 'እየተመለከተ'),\n",
       "   ('ብላለች', 'ልትል', 'ሄዷል', 'ሊሄድ'),\n",
       "   ('ትላለች', 'እያለች', 'ይሄዳል', 'እየሄደ'),\n",
       "   ('ብላለች', 'ልትል', 'ተቀምጧል', 'ሊቀመጥ'),\n",
       "   ('ትላለች', 'እያለች', 'ያስባል', 'እያሰበ'),\n",
       "   ('ብላለች', 'ልትል', 'ጀምሯል', 'ሊጀምር'),\n",
       "   ('ብላለች', 'ልትል', 'አውጥቷል', 'ሊያወጣ'),\n",
       "   ('ትላለች', 'እያለች', 'ያወጣል', 'እያወጣ'),\n",
       "   ('ብላለች', 'ልትል', 'መስሏል', 'ሊመስል'),\n",
       "   ('ትላለች', 'እያለች', 'ይመስላል', 'እየመሰለ'),\n",
       "   ('ብላለች', 'ልትል', 'አሸነፈ', 'ሊያሸንፍ'),\n",
       "   ('ያስባል', 'እያሰበ', 'ትሄዳለች', 'እየሄደች'),\n",
       "   ('ያስባል', 'እያሰበ', 'ይበላል', 'እየበላ'),\n",
       "   ('ያስባል', 'እያሰበ', 'ታደርጋለች', 'እያደረገች'),\n",
       "   ('ያስባል', 'እያሰበ', 'ይመለከታል', 'እየተመለከተ'),\n",
       "   ('ያስባል', 'እያሰበ', 'ይሄዳል', 'እየሄደ'),\n",
       "   ('ያስባል', 'እያሰበ', 'ትላለች', 'እያለች'),\n",
       "   ('ያስባል', 'እያሰበ', 'ያወጣል', 'እያወጣ'),\n",
       "   ('ያስባል', 'እያሰበ', 'ይመስላል', 'እየመሰለ'),\n",
       "   ('ጀምሯል', 'ሊጀምር', 'አድርጋለች', 'ልታደርግ'),\n",
       "   ('ጀምሯል', 'ሊጀምር', 'ሄዷል', 'ሊሄድ'),\n",
       "   ('ጀምሯል', 'ሊጀምር', 'ተቀምጧል', 'ሊቀመጥ'),\n",
       "   ('ጀምሯል', 'ሊጀምር', 'ብላለች', 'ልትል'),\n",
       "   ('ጀምሯል', 'ሊጀምር', 'አውጥቷል', 'ሊያወጣ'),\n",
       "   ('ጀምሯል', 'ሊጀምር', 'መስሏል', 'ሊመስል'),\n",
       "   ('ጀምሯል', 'ሊጀምር', 'አሸነፈ', 'ሊያሸንፍ'),\n",
       "   ('ያወጣል', 'እያወጣ', 'ትሄዳለች', 'እየሄደች'),\n",
       "   ('ያወጣል', 'እያወጣ', 'ይበላል', 'እየበላ'),\n",
       "   ('አውጥቷል', 'ሊያወጣ', 'አድርጋለች', 'ልታደርግ'),\n",
       "   ('ያወጣል', 'እያወጣ', 'ታደርጋለች', 'እያደረገች'),\n",
       "   ('ያወጣል', 'እያወጣ', 'ይመለከታል', 'እየተመለከተ'),\n",
       "   ('አውጥቷል', 'ሊያወጣ', 'ሄዷል', 'ሊሄድ'),\n",
       "   ('ያወጣል', 'እያወጣ', 'ይሄዳል', 'እየሄደ'),\n",
       "   ('አውጥቷል', 'ሊያወጣ', 'ተቀምጧል', 'ሊቀመጥ'),\n",
       "   ('አውጥቷል', 'ሊያወጣ', 'ብላለች', 'ልትል'),\n",
       "   ('ያወጣል', 'እያወጣ', 'ትላለች', 'እያለች'),\n",
       "   ('ያወጣል', 'እያወጣ', 'ያስባል', 'እያሰበ'),\n",
       "   ('አውጥቷል', 'ሊያወጣ', 'ጀምሯል', 'ሊጀምር'),\n",
       "   ('አውጥቷል', 'ሊያወጣ', 'መስሏል', 'ሊመስል'),\n",
       "   ('ያወጣል', 'እያወጣ', 'ይመስላል', 'እየመሰለ'),\n",
       "   ('አውጥቷል', 'ሊያወጣ', 'አሸነፈ', 'ሊያሸንፍ'),\n",
       "   ('ይመስላል', 'እየመሰለ', 'ትሄዳለች', 'እየሄደች'),\n",
       "   ('ይመስላል', 'እየመሰለ', 'ይበላል', 'እየበላ'),\n",
       "   ('መስሏል', 'ሊመስል', 'አድርጋለች', 'ልታደርግ'),\n",
       "   ('ይመስላል', 'እየመሰለ', 'ታደርጋለች', 'እያደረገች'),\n",
       "   ('ይመስላል', 'እየመሰለ', 'ይመለከታል', 'እየተመለከተ'),\n",
       "   ('መስሏል', 'ሊመስል', 'ሄዷል', 'ሊሄድ'),\n",
       "   ('ይመስላል', 'እየመሰለ', 'ይሄዳል', 'እየሄደ'),\n",
       "   ('መስሏል', 'ሊመስል', 'ተቀምጧል', 'ሊቀመጥ'),\n",
       "   ('መስሏል', 'ሊመስል', 'ብላለች', 'ልትል'),\n",
       "   ('ይመስላል', 'እየመሰለ', 'ትላለች', 'እያለች'),\n",
       "   ('ይመስላል', 'እየመሰለ', 'ያስባል', 'እያሰበ'),\n",
       "   ('መስሏል', 'ሊመስል', 'ጀምሯል', 'ሊጀምር'),\n",
       "   ('መስሏል', 'ሊመስል', 'አውጥቷል', 'ሊያወጣ'),\n",
       "   ('ይመስላል', 'እየመሰለ', 'ያወጣል', 'እያወጣ'),\n",
       "   ('መስሏል', 'ሊመስል', 'አሸነፈ', 'ሊያሸንፍ'),\n",
       "   ('አሸነፈ', 'ሊያሸንፍ', 'አድርጋለች', 'ልታደርግ'),\n",
       "   ('አሸነፈ', 'ሊያሸንፍ', 'ሄዷል', 'ሊሄድ'),\n",
       "   ('አሸነፈ', 'ሊያሸንፍ', 'ተቀምጧል', 'ሊቀመጥ'),\n",
       "   ('አሸነፈ', 'ሊያሸንፍ', 'ብላለች', 'ልትል'),\n",
       "   ('አሸነፈ', 'ሊያሸንፍ', 'ጀምሯል', 'ሊጀምር'),\n",
       "   ('አሸነፈ', 'ሊያሸንፍ', 'አውጥቷል', 'ሊያወጣ'),\n",
       "   ('አሸነፈ', 'ሊያሸንፍ', 'መስሏል', 'ሊመስል')]},\n",
       " {'section': 'total',\n",
       "  'correct': [],\n",
       "  'incorrect': [('ትሄዳለች', 'እየሄደች', 'ይበላል', 'እየበላ'),\n",
       "   ('ትሄዳለች', 'እየሄደች', 'ታደርጋለች', 'እያደረገች'),\n",
       "   ('ትሄዳለች', 'እየሄደች', 'ይመለከታል', 'እየተመለከተ'),\n",
       "   ('ትሄዳለች', 'እየሄደች', 'ይሄዳል', 'እየሄደ'),\n",
       "   ('ትሄዳለች', 'እየሄደች', 'ትላለች', 'እያለች'),\n",
       "   ('ትሄዳለች', 'እየሄደች', 'ያስባል', 'እያሰበ'),\n",
       "   ('ትሄዳለች', 'እየሄደች', 'ያወጣል', 'እያወጣ'),\n",
       "   ('ትሄዳለች', 'እየሄደች', 'ይመስላል', 'እየመሰለ'),\n",
       "   ('ይበላል', 'እየበላ', 'ትሄዳለች', 'እየሄደች'),\n",
       "   ('ይበላል', 'እየበላ', 'ታደርጋለች', 'እያደረገች'),\n",
       "   ('ይበላል', 'እየበላ', 'ይመለከታል', 'እየተመለከተ'),\n",
       "   ('ይበላል', 'እየበላ', 'ይሄዳል', 'እየሄደ'),\n",
       "   ('ይበላል', 'እየበላ', 'ትላለች', 'እያለች'),\n",
       "   ('ይበላል', 'እየበላ', 'ያስባል', 'እያሰበ'),\n",
       "   ('ይበላል', 'እየበላ', 'ያወጣል', 'እያወጣ'),\n",
       "   ('ይበላል', 'እየበላ', 'ይመስላል', 'እየመሰለ'),\n",
       "   ('ታደርጋለች', 'እያደረገች', 'ትሄዳለች', 'እየሄደች'),\n",
       "   ('ታደርጋለች', 'እያደረገች', 'ይበላል', 'እየበላ'),\n",
       "   ('ታደርጋለች', 'እያደረገች', 'ይመለከታል', 'እየተመለከተ'),\n",
       "   ('አድርጋለች', 'ልታደርግ', 'ሄዷል', 'ሊሄድ'),\n",
       "   ('ታደርጋለች', 'እያደረገች', 'ይሄዳል', 'እየሄደ'),\n",
       "   ('አድርጋለች', 'ልታደርግ', 'ተቀምጧል', 'ሊቀመጥ'),\n",
       "   ('አድርጋለች', 'ልታደርግ', 'ብላለች', 'ልትል'),\n",
       "   ('ታደርጋለች', 'እያደረገች', 'ትላለች', 'እያለች'),\n",
       "   ('ታደርጋለች', 'እያደረገች', 'ያስባል', 'እያሰበ'),\n",
       "   ('አድርጋለች', 'ልታደርግ', 'ጀምሯል', 'ሊጀምር'),\n",
       "   ('አድርጋለች', 'ልታደርግ', 'አውጥቷል', 'ሊያወጣ'),\n",
       "   ('ታደርጋለች', 'እያደረገች', 'ያወጣል', 'እያወጣ'),\n",
       "   ('አድርጋለች', 'ልታደርግ', 'መስሏል', 'ሊመስል'),\n",
       "   ('ታደርጋለች', 'እያደረገች', 'ይመስላል', 'እየመሰለ'),\n",
       "   ('አድርጋለች', 'ልታደርግ', 'አሸነፈ', 'ሊያሸንፍ'),\n",
       "   ('ይመለከታል', 'እየተመለከተ', 'ትሄዳለች', 'እየሄደች'),\n",
       "   ('ይመለከታል', 'እየተመለከተ', 'ይበላል', 'እየበላ'),\n",
       "   ('ይመለከታል', 'እየተመለከተ', 'ታደርጋለች', 'እያደረገች'),\n",
       "   ('ይመለከታል', 'እየተመለከተ', 'ይሄዳል', 'እየሄደ'),\n",
       "   ('ይመለከታል', 'እየተመለከተ', 'ትላለች', 'እያለች'),\n",
       "   ('ይመለከታል', 'እየተመለከተ', 'ያስባል', 'እያሰበ'),\n",
       "   ('ይመለከታል', 'እየተመለከተ', 'ያወጣል', 'እያወጣ'),\n",
       "   ('ይመለከታል', 'እየተመለከተ', 'ይመስላል', 'እየመሰለ'),\n",
       "   ('ይሄዳል', 'እየሄደ', 'ትሄዳለች', 'እየሄደች'),\n",
       "   ('ይሄዳል', 'እየሄደ', 'ይበላል', 'እየበላ'),\n",
       "   ('ሄዷል', 'ሊሄድ', 'አድርጋለች', 'ልታደርግ'),\n",
       "   ('ይሄዳል', 'እየሄደ', 'ታደርጋለች', 'እያደረገች'),\n",
       "   ('ይሄዳል', 'እየሄደ', 'ይመለከታል', 'እየተመለከተ'),\n",
       "   ('ሄዷል', 'ሊሄድ', 'ተቀምጧል', 'ሊቀመጥ'),\n",
       "   ('ሄዷል', 'ሊሄድ', 'ብላለች', 'ልትል'),\n",
       "   ('ይሄዳል', 'እየሄደ', 'ትላለች', 'እያለች'),\n",
       "   ('ይሄዳል', 'እየሄደ', 'ያስባል', 'እያሰበ'),\n",
       "   ('ሄዷል', 'ሊሄድ', 'ጀምሯል', 'ሊጀምር'),\n",
       "   ('ሄዷል', 'ሊሄድ', 'አውጥቷል', 'ሊያወጣ'),\n",
       "   ('ይሄዳል', 'እየሄደ', 'ያወጣል', 'እያወጣ'),\n",
       "   ('ሄዷል', 'ሊሄድ', 'መስሏል', 'ሊመስል'),\n",
       "   ('ይሄዳል', 'እየሄደ', 'ይመስላል', 'እየመሰለ'),\n",
       "   ('ሄዷል', 'ሊሄድ', 'አሸነፈ', 'ሊያሸንፍ'),\n",
       "   ('ተቀምጧል', 'ሊቀመጥ', 'አድርጋለች', 'ልታደርግ'),\n",
       "   ('ተቀምጧል', 'ሊቀመጥ', 'ሄዷል', 'ሊሄድ'),\n",
       "   ('ተቀምጧል', 'ሊቀመጥ', 'ብላለች', 'ልትል'),\n",
       "   ('ተቀምጧል', 'ሊቀመጥ', 'ጀምሯል', 'ሊጀምር'),\n",
       "   ('ተቀምጧል', 'ሊቀመጥ', 'አውጥቷል', 'ሊያወጣ'),\n",
       "   ('ተቀምጧል', 'ሊቀመጥ', 'መስሏል', 'ሊመስል'),\n",
       "   ('ተቀምጧል', 'ሊቀመጥ', 'አሸነፈ', 'ሊያሸንፍ'),\n",
       "   ('ትላለች', 'እያለች', 'ትሄዳለች', 'እየሄደች'),\n",
       "   ('ትላለች', 'እያለች', 'ይበላል', 'እየበላ'),\n",
       "   ('ብላለች', 'ልትል', 'አድርጋለች', 'ልታደርግ'),\n",
       "   ('ትላለች', 'እያለች', 'ታደርጋለች', 'እያደረገች'),\n",
       "   ('ትላለች', 'እያለች', 'ይመለከታል', 'እየተመለከተ'),\n",
       "   ('ብላለች', 'ልትል', 'ሄዷል', 'ሊሄድ'),\n",
       "   ('ትላለች', 'እያለች', 'ይሄዳል', 'እየሄደ'),\n",
       "   ('ብላለች', 'ልትል', 'ተቀምጧል', 'ሊቀመጥ'),\n",
       "   ('ትላለች', 'እያለች', 'ያስባል', 'እያሰበ'),\n",
       "   ('ብላለች', 'ልትል', 'ጀምሯል', 'ሊጀምር'),\n",
       "   ('ብላለች', 'ልትል', 'አውጥቷል', 'ሊያወጣ'),\n",
       "   ('ትላለች', 'እያለች', 'ያወጣል', 'እያወጣ'),\n",
       "   ('ብላለች', 'ልትል', 'መስሏል', 'ሊመስል'),\n",
       "   ('ትላለች', 'እያለች', 'ይመስላል', 'እየመሰለ'),\n",
       "   ('ብላለች', 'ልትል', 'አሸነፈ', 'ሊያሸንፍ'),\n",
       "   ('ያስባል', 'እያሰበ', 'ትሄዳለች', 'እየሄደች'),\n",
       "   ('ያስባል', 'እያሰበ', 'ይበላል', 'እየበላ'),\n",
       "   ('ያስባል', 'እያሰበ', 'ታደርጋለች', 'እያደረገች'),\n",
       "   ('ያስባል', 'እያሰበ', 'ይመለከታል', 'እየተመለከተ'),\n",
       "   ('ያስባል', 'እያሰበ', 'ይሄዳል', 'እየሄደ'),\n",
       "   ('ያስባል', 'እያሰበ', 'ትላለች', 'እያለች'),\n",
       "   ('ያስባል', 'እያሰበ', 'ያወጣል', 'እያወጣ'),\n",
       "   ('ያስባል', 'እያሰበ', 'ይመስላል', 'እየመሰለ'),\n",
       "   ('ጀምሯል', 'ሊጀምር', 'አድርጋለች', 'ልታደርግ'),\n",
       "   ('ጀምሯል', 'ሊጀምር', 'ሄዷል', 'ሊሄድ'),\n",
       "   ('ጀምሯል', 'ሊጀምር', 'ተቀምጧል', 'ሊቀመጥ'),\n",
       "   ('ጀምሯል', 'ሊጀምር', 'ብላለች', 'ልትል'),\n",
       "   ('ጀምሯል', 'ሊጀምር', 'አውጥቷል', 'ሊያወጣ'),\n",
       "   ('ጀምሯል', 'ሊጀምር', 'መስሏል', 'ሊመስል'),\n",
       "   ('ጀምሯል', 'ሊጀምር', 'አሸነፈ', 'ሊያሸንፍ'),\n",
       "   ('ያወጣል', 'እያወጣ', 'ትሄዳለች', 'እየሄደች'),\n",
       "   ('ያወጣል', 'እያወጣ', 'ይበላል', 'እየበላ'),\n",
       "   ('አውጥቷል', 'ሊያወጣ', 'አድርጋለች', 'ልታደርግ'),\n",
       "   ('ያወጣል', 'እያወጣ', 'ታደርጋለች', 'እያደረገች'),\n",
       "   ('ያወጣል', 'እያወጣ', 'ይመለከታል', 'እየተመለከተ'),\n",
       "   ('አውጥቷል', 'ሊያወጣ', 'ሄዷል', 'ሊሄድ'),\n",
       "   ('ያወጣል', 'እያወጣ', 'ይሄዳል', 'እየሄደ'),\n",
       "   ('አውጥቷል', 'ሊያወጣ', 'ተቀምጧል', 'ሊቀመጥ'),\n",
       "   ('አውጥቷል', 'ሊያወጣ', 'ብላለች', 'ልትል'),\n",
       "   ('ያወጣል', 'እያወጣ', 'ትላለች', 'እያለች'),\n",
       "   ('ያወጣል', 'እያወጣ', 'ያስባል', 'እያሰበ'),\n",
       "   ('አውጥቷል', 'ሊያወጣ', 'ጀምሯል', 'ሊጀምር'),\n",
       "   ('አውጥቷል', 'ሊያወጣ', 'መስሏል', 'ሊመስል'),\n",
       "   ('ያወጣል', 'እያወጣ', 'ይመስላል', 'እየመሰለ'),\n",
       "   ('አውጥቷል', 'ሊያወጣ', 'አሸነፈ', 'ሊያሸንፍ'),\n",
       "   ('ይመስላል', 'እየመሰለ', 'ትሄዳለች', 'እየሄደች'),\n",
       "   ('ይመስላል', 'እየመሰለ', 'ይበላል', 'እየበላ'),\n",
       "   ('መስሏል', 'ሊመስል', 'አድርጋለች', 'ልታደርግ'),\n",
       "   ('ይመስላል', 'እየመሰለ', 'ታደርጋለች', 'እያደረገች'),\n",
       "   ('ይመስላል', 'እየመሰለ', 'ይመለከታል', 'እየተመለከተ'),\n",
       "   ('መስሏል', 'ሊመስል', 'ሄዷል', 'ሊሄድ'),\n",
       "   ('ይመስላል', 'እየመሰለ', 'ይሄዳል', 'እየሄደ'),\n",
       "   ('መስሏል', 'ሊመስል', 'ተቀምጧል', 'ሊቀመጥ'),\n",
       "   ('መስሏል', 'ሊመስል', 'ብላለች', 'ልትል'),\n",
       "   ('ይመስላል', 'እየመሰለ', 'ትላለች', 'እያለች'),\n",
       "   ('ይመስላል', 'እየመሰለ', 'ያስባል', 'እያሰበ'),\n",
       "   ('መስሏል', 'ሊመስል', 'ጀምሯል', 'ሊጀምር'),\n",
       "   ('መስሏል', 'ሊመስል', 'አውጥቷል', 'ሊያወጣ'),\n",
       "   ('ይመስላል', 'እየመሰለ', 'ያወጣል', 'እያወጣ'),\n",
       "   ('መስሏል', 'ሊመስል', 'አሸነፈ', 'ሊያሸንፍ'),\n",
       "   ('አሸነፈ', 'ሊያሸንፍ', 'አድርጋለች', 'ልታደርግ'),\n",
       "   ('አሸነፈ', 'ሊያሸንፍ', 'ሄዷል', 'ሊሄድ'),\n",
       "   ('አሸነፈ', 'ሊያሸንፍ', 'ተቀምጧል', 'ሊቀመጥ'),\n",
       "   ('አሸነፈ', 'ሊያሸንፍ', 'ብላለች', 'ልትል'),\n",
       "   ('አሸነፈ', 'ሊያሸንፍ', 'ጀምሯል', 'ሊጀምር'),\n",
       "   ('አሸነፈ', 'ሊያሸንፍ', 'አውጥቷል', 'ሊያወጣ'),\n",
       "   ('አሸነፈ', 'ሊያሸንፍ', 'መስሏል', 'ሊመስል')]}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_model.accuracy('data/syntax.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ቱንጋ', 0.7262619733810425),\n",
       " ('እንደሚያጠጣት', 0.7166581153869629),\n",
       " ('ስላስፈራራው', 0.7128387689590454),\n",
       " ('ያጣናን', 0.7123674750328064),\n",
       " ('ፊታዉራሪን', 0.7114162445068359),\n",
       " ('ቢያሳትምም', 0.7111265659332275),\n",
       " ('መሂድ', 0.7107560634613037),\n",
       " ('ሳናሱሲ', 0.7092135548591614),\n",
       " ('ከጃፓኑ', 0.7065313458442688),\n",
       " ('ቢዞሩ', 0.705833911895752)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_model.similar_by_word('ዝቅተኛ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
