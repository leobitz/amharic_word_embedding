{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed_val = 1000\n",
    "random.seed(seed_val)\n",
    "import numpy as np\n",
    "np.random.seed(seed_val)\n",
    "import tensorflow as tf\n",
    "# tf.set_random_seed(seed_val)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM, TimeDistributed, SimpleRNN\n",
    "from tensorflow.keras.layers import Concatenate, Flatten, Embedding\n",
    "from tensorflow.keras.layers import GRU, Conv2D, MaxPooling2D, AveragePooling2D, AvgPool2D, MaxPool1D\n",
    "from tensorflow.keras.layers import Input, Reshape, Dot, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "from data_handle import *\n",
    "from gensim_wrapper import *\n",
    "from utils import *\n",
    "import gensim\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model_multi(n_chars, n_consonant, n_vowels, n_units, char_embed_size=32):\n",
    "    cons_inputs = Input(shape=(n_chars, ), dtype=\"int32\", name=\"cons_inputs\")\n",
    "    vow_inputs = Input(shape=(n_chars, ), dtype=\"int32\", name=\"vow_inputs\")\n",
    "    \n",
    "    cons_embed = Embedding(n_consonant,char_embed_size, input_length=n_chars)(cons_inputs)\n",
    "    vow_embed = Embedding(n_vowels, char_embed_size, input_length=n_chars)(vow_inputs)\n",
    "    \n",
    "    x = Concatenate(axis=2)([cons_embed, vow_embed])\n",
    "    \n",
    "    x = Reshape([n_chars, char_embed_size*2, 1])(x)\n",
    "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling2D(3)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(embed_size, activation=\"tanh\", name=\"x\")(x)\n",
    "    \n",
    "    H = Dense(embed_size, activation=\"tanh\", name=\"H\")(x)\n",
    "    T = Dense(embed_size, activation=\"sigmoid\", name=\"T\")(x)\n",
    "    \n",
    "    C = 1 - T\n",
    "    x = H * T + C * x\n",
    "    state_h = Dense(embed_size, activation=\"tanh\", name=\"state\")(x)\n",
    "    \n",
    "    consonant_decoder_inputs = Input(shape=(None, n_consonant), name=\"target_consonant\")\n",
    "    consonant_decoder_gru = GRU(n_units, return_sequences=True, return_state=True,  name=\"consonant_decoder_gru\")\n",
    "    consonant_decoder_outputs, _= consonant_decoder_gru(consonant_decoder_inputs, initial_state=state_h)\n",
    "\n",
    "    vowel_decoder_inputs = Input(shape=(None, n_vowels), name=\"vowel_input\")\n",
    "    vowel_decoder_gru = GRU(n_units, return_sequences=True, return_state=True, name=\"vowl_decoder_gru\")\n",
    "    vowel_decoder_outputs, _= vowel_decoder_gru(vowel_decoder_inputs, initial_state=state_h)\n",
    "\n",
    "    consonant_decoder_dense = Dense(n_consonant, activation='softmax', name=\"consonant_output\")\n",
    "    consonant_decoder_outputs = consonant_decoder_dense(consonant_decoder_outputs)\n",
    "    \n",
    "    vowel_decoder_dense = Dense(n_vowels, activation='softmax', name=\"vowel_output\")\n",
    "    vowel_decoder_outputs = vowel_decoder_dense(vowel_decoder_outputs)\n",
    "    \n",
    "    main_model = Model([cons_inputs, vow_inputs, consonant_decoder_inputs, vowel_decoder_inputs], [consonant_decoder_outputs, vowel_decoder_outputs])\n",
    "    encoder_model = Model([cons_inputs, vow_inputs], state_h)\n",
    "    \n",
    "    decoder_state_input_h = Input(shape=(n_units,))\n",
    "    \n",
    "    consonant_decoder_outputs, state_h= consonant_decoder_gru(consonant_decoder_inputs, initial_state=decoder_state_input_h)\n",
    "    consonant_decoder_outputs = consonant_decoder_dense(consonant_decoder_outputs)\n",
    "    \n",
    "    vowel_decoder_outputs, state_h= vowel_decoder_gru(vowel_decoder_inputs, initial_state=decoder_state_input_h)\n",
    "    vowel_decoder_outputs = vowel_decoder_dense(vowel_decoder_outputs)\n",
    "    \n",
    "    decoder_model = Model([consonant_decoder_inputs, vowel_decoder_inputs, decoder_state_input_h], [consonant_decoder_outputs, vowel_decoder_outputs, state_h])\n",
    "\n",
    "    return main_model, encoder_model, decoder_model\n",
    "\n",
    "def generator(word2int, int2word, char2tup,  batch_size, n_consonant, n_vowels):\n",
    "    targets, target_inputs = {}, {}\n",
    "    vocab = []\n",
    "    for word in word2int.keys():\n",
    "        target = word + '|'\n",
    "        target_input = '&' + target \n",
    "        targets[word] = target\n",
    "        target_inputs[word] = target_input\n",
    "        vocab.append(word)\n",
    "    \n",
    "    batch = 0\n",
    "    n_batchs = len(word2int) // batch_size\n",
    "    n_chars = 13\n",
    "    current_word = 0\n",
    "    while True:\n",
    "        batch_cons_input = []\n",
    "        batch_vow_input = []\n",
    "        batch_decoder_cons_input = []\n",
    "        batch_decoder_vow_input = []\n",
    "        batch_output_cons = []\n",
    "        batch_output_vow = []\n",
    "        for bi in range(0, batch_size):\n",
    "            input_word = vocab[current_word]\n",
    "\n",
    "            input_con, input_vow = word2vec_indexed(char2tup, input_word, n_chars, n_consonant, n_vowels)\n",
    "            target_con, target_vow = word2vec_seperated(char2tup, targets[input_word], n_chars, n_consonant, n_vowels)\n",
    "            decoder_con, decoder_vow = word2vec_seperated(char2tup,target_inputs[input_word], n_chars, n_consonant, n_vowels)\n",
    "\n",
    "            batch_cons_input.append(input_con)\n",
    "            batch_vow_input.append(input_vow)\n",
    "            batch_decoder_cons_input.append(decoder_con)\n",
    "            batch_decoder_vow_input.append(decoder_vow)\n",
    "            batch_output_cons.append(target_con)\n",
    "            batch_output_vow.append(target_vow)\n",
    "\n",
    "            current_word += 1\n",
    "            if current_word > batch_size * n_batchs:\n",
    "                current_word = 0\n",
    "                \n",
    "        batch_cons_input = np.array(batch_cons_input)\n",
    "        batch_vow_input = np.array(batch_vow_input)\n",
    "        batch_decoder_cons_input = np.array(batch_decoder_cons_input)\n",
    "        batch_decoder_vow_input = np.array(batch_decoder_vow_input)\n",
    "        batch_output_cons = np.array(batch_output_cons)\n",
    "        batch_output_vow = np.array(batch_output_vow)\n",
    "\n",
    "        yield [batch_cons_input, batch_vow_input, batch_decoder_cons_input, batch_decoder_vow_input], [batch_output_cons, batch_output_vow]\n",
    "\n",
    "def pred_embeddings(vocab, encoder, char2tup):\n",
    "    embeddings = np.ndarray((len(vocab), embed_size))\n",
    "    i = 0\n",
    "    cons_buffer = []\n",
    "    vow_buffer = []\n",
    "    buffer_size = 10000\n",
    "    for wi, word in enumerate(vocab):\n",
    "        word = int2word[word2int[word]]\n",
    "        convec, vowvec = word2vec_indexed(char2tup, word, n_chars, n_consonant, n_vowel)\n",
    "        vow_buffer.append(vowvec)\n",
    "        cons_buffer.append(convec)\n",
    "        if len(vow_buffer) == buffer_size or len(vocab) - wi < buffer_size:\n",
    "            vow_buffer_np = np.stack(vow_buffer)\n",
    "            cons_buffer_np = np.stack(cons_buffer)\n",
    "            result = encoder.predict([vow_buffer_np, cons_buffer_np])\n",
    "            embeddings[i:i+len(vow_buffer)] = result\n",
    "            i += len(vow_buffer)\n",
    "            cons_buffer = []\n",
    "            vow_buffer = []\n",
    "            if i % (4 * buffer_size) == 0:\n",
    "                print(\"Predicting: {0:.2f}%\".format((i * 100.0 / len(vocab))))\n",
    "                \n",
    "    print(\"finished\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = read_file()\n",
    "vocab, word2int, int2word = build_vocab(words)\n",
    "del words\n",
    "char2int, int2char, char2tup, tup2char, n_consonant, n_vowel = build_charset()\n",
    "n_chars = 11 + 2 \n",
    "n_features = len(char2int)\n",
    "\n",
    "batch_size = 100\n",
    "embed_size = 100\n",
    "\n",
    "n_batches = len(vocab)  // batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, encoder, decoder = conv_model_multi(n_chars, n_consonant, n_vowel, embed_size)\n",
    "adam = keras.optimizers.Adam(.001)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['acc'])\n",
    "gen = generator(word2int, int2word, char2tup, batch_size, n_consonant, n_vowel)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(gen, steps_per_epoch=n_batches, epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pred_embeddings(vocab, encoder, char2tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(evaluate(word2int, embeddings) )\n",
    "print(evaluate(word2int, normalize(embeddings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open(\"results/embed_embed10.txt\", encoding='utf8', mode='w')\n",
    "# file.write(\"{0} {1}\\n\".format(len(vocab), embed_size))\n",
    "# for word, index in word2int.items():\n",
    "#     e = embeddings[index]\n",
    "#     e = ' '.join(map(lambda x: str(x), e))\n",
    "#     file.write(\"{0} {1}\\n\".format(word, e))\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
