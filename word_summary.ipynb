{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, TimeDistributed\n",
    "from keras.layers import Concatenate, Flatten\n",
    "from keras.layers import GRU, Conv2D, MaxPooling2D\n",
    "from keras.layers import Input, Reshape, Dot, Add\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from data_handle import *\n",
    "from gensim_wrapper import *\n",
    "from utils import *\n",
    "import gensim\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "seed_val = 1000\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "tf.set_random_seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2sem(embed_size):\n",
    "    syntax_input = Input(shape=(embed_size, ), name=\"syn\")\n",
    "    x = Dense(256, activation='tanh')(syntax_input)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(256, activation='tanh')(x)\n",
    "    x = Dense(embed_size, activation='linear')(x)\n",
    "    model = Model(syntax_input, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder(embed_size, input_size):\n",
    "    word_input = Input(shape=(input_size,), name=\"word_input\")\n",
    "    x = Dense(400, activation='tanh')(word_input)\n",
    "    x = Dense(256, activation='tanh')(x)\n",
    "    embed = Dense(128, activation=\"tanh\")(x)\n",
    "    x = Dense(256, activation='tanh')(embed)\n",
    "    x = Dense(400, activation='tanh')(x)\n",
    "    x = Dense(input_size, activation='tanh')(x)\n",
    "    model = Model(word_input, x)\n",
    "    em_model = Model(word_input, embed)\n",
    "    return model, em_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model(n_input, n_output, n_enc_units, n_dec_units):\n",
    "    root_word_input = Input(shape=(13, 309, 1), name=\"root_word_input\")\n",
    "    \n",
    "    x = Conv2D(16, (3, 3), padding='same', activation='relu')(root_word_input)\n",
    "    x = MaxPooling2D(1, 1)(x)\n",
    "    X = Dropout(.2)(x)\n",
    "    x = Conv2D(8, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    x = Flatten()(x)\n",
    "#     x = Dense(300, activation='relu')(x)\n",
    "    X = Dropout(.2)(x)\n",
    "    state_h = Dense(n_dec_units, activation='linear')(x)\n",
    "    \n",
    "    decoder_inputs = Input(shape=(None, 309), name=\"target_word_input\")\n",
    "    decoder_gru = GRU(n_dec_units, return_sequences=True, return_state=True, name=\"decoder_gru\")\n",
    "    decoder_outputs, _= decoder_gru(decoder_inputs, initial_state=state_h)\n",
    "    \n",
    "    decoder_dense = Dense(309, activation='softmax', name=\"train_output\")\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    model = Model([root_word_input, decoder_inputs], decoder_outputs)\n",
    "    encoder_model = Model(root_word_input, state_h)\n",
    "    \n",
    "    decoder_state_input_h = Input(shape=(n_dec_units,))\n",
    "    decoder_outputs, state_h= decoder_gru(decoder_inputs, initial_state=decoder_state_input_h)\n",
    "\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model([decoder_inputs, decoder_state_input_h], [decoder_outputs, state_h])\n",
    "\n",
    "    return model, encoder_model, decoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model_multi(n_chars, n_consonant, n_vowels, n_units):\n",
    "    root_word_input = Input(shape=(n_chars, (n_consonant + n_vowels), 1), name=\"root_word_input\")\n",
    "    \n",
    "    x = Conv2D(8, (5, 5), padding='same', activation='relu')(root_word_input)\n",
    "    x = MaxPooling2D(3, 3)(x)\n",
    "#     x = Conv2D(8, (3, 3), padding='same', activation='relu')(x)\n",
    "#     x = MaxPooling2D(2, 2)(x)\n",
    "#     x = Conv2D(8, (5, 5), padding='same', activation='relu')(x)\n",
    "#     x = MaxPooling2D(2, 2)(x)\n",
    "    x = Flatten()(x)\n",
    "#     x = Dense(300, activation='relu')(x)\n",
    "#     X = Dropout(.2)(x)\n",
    "    state_h = Dense(n_units, activation='linear')(x)\n",
    "    \n",
    "    consonant_decoder_inputs = Input(shape=(None, n_consonant), name=\"target_consonant\")\n",
    "    consonant_decoder_gru = GRU(n_units, return_sequences=True, return_state=True,  name=\"consonant_decoder_gru\")\n",
    "    consonant_decoder_outputs, _= consonant_decoder_gru(consonant_decoder_inputs, initial_state=state_h)\n",
    "    \n",
    "    vowel_decoder_inputs = Input(shape=(None, n_vowels), name=\"vowel_input\")\n",
    "    vowel_decoder_gru = GRU(n_units, return_sequences=True, return_state=True, name=\"vowl_decoder_gru\")\n",
    "    vowel_decoder_outputs, _= vowel_decoder_gru(vowel_decoder_inputs, initial_state=state_h)\n",
    "#     print(vowel_decoder_outputs.shape, consonant_decoder_outputs.shape)\n",
    "#     x = Concatenate(axis=1)([vowel_decoder_outputs, consonant_decoder_outputs])\n",
    "#     print(x.shape)\n",
    "    consonant_decoder_dense = Dense(n_consonant, activation='softmax', name=\"consonant_output\")\n",
    "    consonant_decoder_outputs = consonant_decoder_dense(consonant_decoder_outputs)\n",
    "    \n",
    "    vowel_decoder_dense = Dense(n_vowels, activation='softmax', name=\"vowel_output\")\n",
    "    vowel_decoder_outputs = vowel_decoder_dense(vowel_decoder_outputs)\n",
    "    \n",
    "    main_model = Model([root_word_input, consonant_decoder_inputs, vowel_decoder_inputs], [consonant_decoder_outputs, vowel_decoder_outputs])\n",
    "    \n",
    "    encoder_model = Model(root_word_input, state_h)\n",
    "    \n",
    "    decoder_state_input_h = Input(shape=(n_units,))\n",
    "    \n",
    "    consonant_decoder_outputs, state_h= consonant_decoder_gru(consonant_decoder_inputs, initial_state=decoder_state_input_h)\n",
    "    consonant_decoder_outputs = consonant_decoder_dense(consonant_decoder_outputs)\n",
    "    \n",
    "    vowel_decoder_outputs, state_h= vowel_decoder_gru(vowel_decoder_inputs, initial_state=decoder_state_input_h)\n",
    "    vowel_decoder_outputs = vowel_decoder_dense(vowel_decoder_outputs)\n",
    "    \n",
    "    decoder_model = Model([consonant_decoder_inputs, vowel_decoder_inputs, decoder_state_input_h], [consonant_decoder_outputs, vowel_decoder_outputs, state_h])\n",
    "\n",
    "    return main_model, encoder_model, decoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_nce(input_size, embed_size):\n",
    "    context_word = Input(shae=(128,), name=\"context_input\")\n",
    "    target_word = Input(shae=(128,), name=\"target_input\")\n",
    "    context_vec = Dense(embed_size)(context_word)\n",
    "    target_vec = Dense(embed_size)(target_word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model_multi_v2(n_chars, n_char_class, n_consonant, n_vowels, n_units):\n",
    "    root_word_input = Input(shape=(n_chars, (n_consonant + n_vowels), 1), name=\"root_word_input\")\n",
    "    \n",
    "    x = Conv2D(16, (3, 3), padding='same', activation='relu')(root_word_input)\n",
    "    x = MaxPooling2D(1, 1)(x)\n",
    "    X = Dropout(.2)(x)\n",
    "    x = Conv2D(8, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling2D(1, 1)(x)\n",
    "    x = Flatten()(x)\n",
    "#     x = Dense(300, activation='relu')(x)\n",
    "#     X = Dropout(.2)(x)\n",
    "    state_h = Dense(n_units, activation='linear')(x)\n",
    "    \n",
    "    consonant_decoder_inputs = Input(shape=(None, n_consonant), name=\"target_consonant\")\n",
    "    consonant_decoder_gru = GRU(n_units, return_sequences=True, return_state=True, name=\"consonant_decoder_gru\")\n",
    "    consonant_decoder_outputs, _= consonant_decoder_gru(consonant_decoder_inputs, initial_state=state_h)\n",
    "    \n",
    "    vowel_decoder_inputs = Input(shape=(None, n_vowels), name=\"vowel_input\")\n",
    "    vowel_decoder_gru = GRU(n_units, return_sequences=True, return_state=True, name=\"vowl_decoder_gru\")\n",
    "    vowel_decoder_outputs, _= vowel_decoder_gru(vowel_decoder_inputs, initial_state=state_h)\n",
    "    \n",
    "    decoders_outputs = Concatenate(axis=1)([consonant_decoder_outputs, vowel_decoder_outputs])\n",
    "  \n",
    "    decoder_dense = Dense(n_char_class, activation='softmax', name=\"decoder_output\")\n",
    "    decoders_outputs = decoder_dense(decoders_outputs)\n",
    "    \n",
    "    main_model = Model([root_word_input, consonant_decoder_inputs, vowel_decoder_inputs], decoders_outputs)\n",
    "    encoder_model = Model(root_word_input, state_h)\n",
    "    \n",
    "    decoder_state_input_h = Input(shape=(n_units,))\n",
    "    \n",
    "    consonant_decoder_outputs, state_h= consonant_decoder_gru(consonant_decoder_inputs, initial_state=decoder_state_input_h)\n",
    "    vowel_decoder_outputs, state_h= vowel_decoder_gru(vowel_decoder_inputs, initial_state=decoder_state_input_h)\n",
    "    \n",
    "    decoders_outputs = Concatenate(axis=1)([consonant_decoder_outputs, vowel_decoder_outputs])\n",
    "    decoders_outputs = decoder_dense(decoders_outputs)\n",
    "    \n",
    "    decoder_model = Model([consonant_decoder_inputs, vowel_decoder_inputs, decoder_state_input_h], [decoders_outputs, state_h])\n",
    "    \n",
    "    return main_model, encoder_model, decoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model2(n_input, n_output, n_enc_units, n_dec_units):\n",
    "    root_word_input = Input(shape=(13, 309, 1), name=\"root_word_input\")\n",
    "    word_feature = Input(shape=(128,), name=\"word_feature\")\n",
    "    \n",
    "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(root_word_input)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    X = Dropout(.2)(x)\n",
    "    x = Conv2D(16, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    x = Conv2D(8, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='linear')(x)\n",
    "#     X = Dropout(.2)(x)\n",
    "    x = Add()([x, word_feature])\n",
    "    state_h = Dense(n_dec_units, activation='linear')(x)\n",
    "    \n",
    "    decoder_inputs = Input(shape=(None, 309), name=\"target_word_input\")\n",
    "    decoder_gru = GRU(n_dec_units, return_sequences=True, return_state=True, name=\"decoder_gru\")\n",
    "    decoder_outputs, _= decoder_gru(decoder_inputs, initial_state=state_h)\n",
    "    \n",
    "    decoder_dense = Dense(309, activation='softmax', name=\"train_output\")\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    model = Model([root_word_input, decoder_inputs, word_feature], decoder_outputs)\n",
    "    encoder_model = Model([root_word_input, word_feature], state_h)\n",
    "    \n",
    "    decoder_state_input_h = Input(shape=(n_dec_units,))\n",
    "    decoder_outputs, state_h= decoder_gru(decoder_inputs, initial_state=decoder_state_input_h)\n",
    "\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model([decoder_inputs, decoder_state_input_h], [decoder_outputs, state_h])\n",
    "\n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(model, int2char, state):\n",
    "    target_seq = np.zeros([1, 1, 309])\n",
    "    target_seq[0, 0, char2int['&']] = 1\n",
    "    decoded_chars = []\n",
    "    for i in range(13):\n",
    "        target_seq, state = model.predict([target_seq, state])\n",
    "        index = np.argmax(target_seq.flatten())\n",
    "        char = int2char[index]\n",
    "        decoded_chars += [char]\n",
    "        \n",
    "#         target_seq = np.zeros([1, 1, 309])\n",
    "#         target_seq[0, 0, index] = 1\n",
    "    return decoded_chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_sep(con, con_max, vow, vow_max):\n",
    "    con_vec = np.zeros((con_max, ))\n",
    "    con_vec[con] = 1\n",
    "    vow_vec = np.zeros((vow_max, ))\n",
    "    vow_vec[vow] = 1\n",
    "    return con_vec, vow_vec\n",
    "    \n",
    "def decode_multi_sequence(model, char2tup, tup2char, state, n_consonant, n_vowels):\n",
    "    con, vow = char2tup['&']\n",
    "    con_vec, vow_vec = one_hot_sep(con, n_consonant, vow, n_vowels) \n",
    "    con_vec = con_vec.reshape((1, 1, -1))\n",
    "    vow_vec = vow_vec.reshape((1, 1, -1))\n",
    "#     target_seq = np.concatenate([con_vec, vow_vec])\n",
    "    decoded_chars = []\n",
    "    for i in range(13):\n",
    "        con_vec, vow_vec, state = model.predict([con_vec, vow_vec, state])\n",
    "#         target_seq = np.concatenate([con_vec, vow_vec])\n",
    "        new_con_vec = np.zeros_like(con_vec)\n",
    "        new_con_vec[0, 0, np.argmax(con_vec[0, 0, :])] = 1\n",
    "        new_vow_vec = np.zeros_like(vow_vec)\n",
    "        new_vow_vec[0, 0, np.argmax(vow_vec[0, 0, :])] = 1\n",
    "        con_vec, vow_vec = new_con_vec, new_vow_vec\n",
    "        name = \"{0}-{1}\".format(np.argmax(con_vec[0, 0, :]), np.argmax(vow_vec[0, 0, :]))\n",
    "        try:\n",
    "            char = tup2char[name]\n",
    "        except:\n",
    "            char = ' '\n",
    "        decoded_chars += [char]\n",
    "    \n",
    "    return decoded_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_model(input_size, output_size, embed_size):\n",
    "    context_word = Input(shape=(input_size,), name=\"context_word\")\n",
    "    x = Dense(256, activation='relu')(context_word)\n",
    "    embeding = Dense(embed_size, activation='tanh')(x)\n",
    "    target_word = Dense(output_size, activation='relu')(embeding)\n",
    "    model = Model(context_word, target_word)\n",
    "    em_model = Model(context_word, embeding)\n",
    "    return model, em_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_loss(yTrue, yPred):\n",
    "    loss = K.sum(K.square(yTrue - yPred))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_model2(input_size, output_size, embed_size):\n",
    "    context_word = Input(shape=(input_size,), name=\"context_word\")\n",
    "    target_word = Input(shape=(input_size,), name=\"target_word\")\n",
    "    \n",
    "    layer1 = Dense(200, activation='tanh')\n",
    "    layer2 = Dense(200, activation='tanh')\n",
    "    \n",
    "    x = layer1(context_word)\n",
    "    y = layer1(target_word)\n",
    "#     y = layer2(y)\n",
    "    cosine_sim = Dot(normalize=True, axes=1)([x, y])\n",
    "#     z = Concatenate(axis=1)([x, y])\n",
    "#     z = Dense(20, activation='tanh')(z)\n",
    "    \n",
    "#     output = Dense(1, activation='tanh')(z)\n",
    "    model = Model([context_word, target_word], cosine_sim)\n",
    "    \n",
    "    con_model = Model(context_word, x)\n",
    "    tar_model = Model(target_word, y)\n",
    "    \n",
    "    return model, con_model, tar_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeder3(input_size, embed_size):\n",
    "    context_word = Input(shape=(input_size,), name=\"context_word\")\n",
    "    \n",
    "    x = Dense(embed_size, activation='tanh')(context_word)\n",
    "    y = Dense(embed_size, activation='tanh')(x)\n",
    "    \n",
    "    model = Model(context_word, y)\n",
    "    em_model = Model(context_word, x)\n",
    "    return model, em_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_model3(input_size, output_size, embed_size):\n",
    "    left_word = Input(shape=(input_size,), name=\"left_word\")\n",
    "    right_word = Input(shape=(input_size,), name=\"right_word\")\n",
    "    \n",
    "    layer1 = Dense(128, activation='relu')\n",
    "    layer2 = Dense(128, activation='linear')\n",
    "    \n",
    "    left = layer1(left_word)\n",
    "    right = layer1(right_word)\n",
    "    \n",
    "    left = layer2(left)\n",
    "    right = layer2(right)\n",
    "    \n",
    "    x = Concatenate(axis=1)([left, right])\n",
    "    x = Dense(embed_size, activation='tanh')(x)\n",
    "    \n",
    "    model = Model([left_word, right_word], x)\n",
    "    con_model = Model(left_word, left)\n",
    "    tar_model = Model(right_word, right)\n",
    "    \n",
    "    return model, con_model, tar_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(final_embedding, word2int, embed_size):\n",
    "    gensim = GensimWrapper('data/news.txt',embed_size, 0, log=False)\n",
    "    gensim.set_embeddings(word2int, final_embedding)\n",
    "    result = gensim.evaluate()\n",
    "    for key in result:\n",
    "        print(\"{0}: {1:.2f}%\".format(key, result[key]), end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(embeddings):\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    return embeddings / norms\n",
    "\n",
    "def normalize2(embeddings):\n",
    "    maxes = np.max(np.abs(embeddings), axis=1, keepdims=True)\n",
    "    return embeddings / maxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = read_file()\n",
    "vocab, word2int, int2word = build_vocab(words)\n",
    "int_words = words_to_ints(word2int, words)\n",
    "word2freq = get_frequency(words, word2int, int2word)\n",
    "char2int, int2char, char2tup, tup2char, n_consonant, n_vowel = build_charset()\n",
    "ns_unigrams = ns_sample(word2freq, word2int, int2word, .75)\n",
    "n_chars = 11 + 2 \n",
    "n_features = len(char2int)\n",
    "batch_size = 120\n",
    "embed_size = 50\n",
    "skip_window = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = generate_word_images_flat(vocab, char2tup, batch_size, n_chars, n_consonant, n_vowel)\n",
    "# x, y = next(gen)\n",
    "model, em_model = autoencoder(embed_size, n_chars*(n_consonant + n_vowel))\n",
    "adam = keras.optimizers.Nadam(0.0001)\n",
    "model.compile(optimizer=adam, loss='mse')\n",
    "n_batches = len(vocab) // batch_size\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(gen, steps_per_epoch=n_batches, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_in = np.ndarray((len(vocab), 13 * (n_consonant + n_vowel)))\n",
    "for i in range(len(vocab)):\n",
    "    con_vec, vow_vec = word2vec_seperated(\n",
    "                char2tup, words[i], n_chars, n_consonant, n_vowel)\n",
    "    word_in[i] = np.concatenate([con_vec, vow_vec], axis=1).flatten()\n",
    "indexes = [word2int[word] for word in vocab]\n",
    "embeddings = em_model.predict(word_in[indexes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(normalize(embeddings), word2int, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297836\n"
     ]
    }
   ],
   "source": [
    "multi_gen = generate_word_images_multi(words, char2tup, batch_size, n_consonant, n_vowel)\n",
    "# multi_gen = generate_word_images_multi_v2(words, char2int, char2tup, batch_size, n_consonant, n_vowel)\n",
    "\n",
    "# [x1, x2, x3], y = next(multi_gen)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "root_word_input (InputLayer)    (None, 13, 50, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 13, 50, 8)    208         root_word_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 4, 16, 8)     0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 512)          0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "target_consonant (InputLayer)   (None, None, 40)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 50)           25650       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "vowel_input (InputLayer)        (None, None, 10)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "consonant_decoder_gru (GRU)     [(None, None, 50), ( 13650       target_consonant[0][0]           \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "vowl_decoder_gru (GRU)          [(None, None, 50), ( 9150        vowel_input[0][0]                \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "consonant_output (Dense)        (None, None, 40)     2040        consonant_decoder_gru[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "vowel_output (Dense)            (None, None, 10)     510         vowl_decoder_gru[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 51,208\n",
      "Trainable params: 51,208\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "multi_train, multi_enc, multi_dec = conv_model_multi(n_chars, n_consonant, n_vowel, embed_size)\n",
    "adam = keras.optimizers.Nadam(0.001)\n",
    "multi_train.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['acc'])\n",
    "multi_train.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_train2, multi_enc2, multi_dec2 = conv_model_multi_v2(n_chars, len(char2int), n_consonant, n_vowel, embed_size)\n",
    "# adam = keras.optimizers.Nadam(0.001)\n",
    "# multi_train2.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['acc'])\n",
    "# multi_train2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "2481/2481 [==============================] - 131s 53ms/step - loss: 0.4485 - consonant_output_loss: 0.2844 - vowel_output_loss: 0.1642 - consonant_output_acc: 0.9300 - vowel_output_acc: 0.9516\n",
      "Epoch 2/6\n",
      "2481/2481 [==============================] - 128s 51ms/step - loss: 0.0774 - consonant_output_loss: 0.0491 - vowel_output_loss: 0.0282 - consonant_output_acc: 0.9929 - vowel_output_acc: 0.9953\n",
      "Epoch 3/6\n",
      "2481/2481 [==============================] - 128s 52ms/step - loss: 0.0373 - consonant_output_loss: 0.0209 - vowel_output_loss: 0.0164 - consonant_output_acc: 0.9972 - vowel_output_acc: 0.9977\n",
      "Epoch 4/6\n",
      "2481/2481 [==============================] - 128s 52ms/step - loss: 0.0209 - consonant_output_loss: 0.0092 - vowel_output_loss: 0.0117 - consonant_output_acc: 0.9989 - vowel_output_acc: 0.9985\n",
      "Epoch 5/6\n",
      "2481/2481 [==============================] - 129s 52ms/step - loss: 0.0137 - consonant_output_loss: 0.0085 - vowel_output_loss: 0.0051 - consonant_output_acc: 0.9989 - vowel_output_acc: 0.9993\n",
      "Epoch 6/6\n",
      "2481/2481 [==============================] - 134s 54ms/step - loss: 0.0112 - consonant_output_loss: 0.0084 - vowel_output_loss: 0.0028 - consonant_output_acc: 0.9990 - vowel_output_acc: 0.99960s - loss: 0.0112 - consonant_output_loss: 0.0084 - vowel_output_loss: 0.0028 - consonant_output_acc: 0.9990 - vowel_output_acc: 0.999\n"
     ]
    }
   ],
   "source": [
    "n_batches = len(vocab) // batch_size\n",
    "history = multi_train.fit_generator(multi_gen, steps_per_epoch=n_batches, epochs = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab = vocab[:int(len(vocab) * .7)]\n",
    "test_vocab = vocab[int(len(vocab) * .7):]\n",
    "# gen = generate_word_images_feat(vocab, word2int, char2int, sem_embed, batch_size)\n",
    "# gen2 = generate_batch_image_v3(words, word2int, char2int, batch_size, skip_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_emb = np.load('results/word2vec_embedding.npy')\n",
    "seq_emb = np.load('results/char_embedding.npy')\n",
    "# def gen_mapping(data, seq, sem, batch_size):\n",
    "#     ci = 0\n",
    "#     while True:\n",
    "#         batch_indexes, ci = get_context_words(data, ci, batch_size)\n",
    "#         ci += batch_size\n",
    "#         batch_inputs = seq[batch_indexes]\n",
    "#         batch_labels = sem[batch_indexes]\n",
    "#         yield batch_inputs, batch_labels\n",
    "\n",
    "# data = [word2int[word] for word in train_vocab]\n",
    "# full = [word2int[word] for word in vocab]\n",
    "# gen = gen_mapping(full, sem_emb, seq_emb, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = seq2sem(embed_size)\n",
    "adam = keras.optimizers.Nadam(0.0001)\n",
    "model.compile(optimizer=adam, loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = len(vocab) // batch_size\n",
    "adam = keras.optimizers.Nadam(0.0001)\n",
    "history = model.fit_generator(gen, steps_per_epoch=n_batches, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indexs = [word2int[word] for word in test_vocab]\n",
    "train_indexs = [word2int[word] for word in train_vocab]\n",
    "# pred = np.random.randn(sem_emb.shape[0], sem_emb.shape[1])\n",
    "# pred[train_indexs] = sem_emb[train_indexs]\n",
    "# pred[test_indexs] = model.predict(seq_emb[test_indexs])\n",
    "pred = model.predict(sem_emb)\n",
    "# dot_prods = np.einsum('ij,ij->i', normalize(sem_emb), normalize(pred))\n",
    "# print(dot_prods[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parseVec(file, delimiter):\n",
    "    lines = open(file, encoding='utf8').readlines()\n",
    "    vocab_size, embed_size = [int(s) for s in lines[0].split()]\n",
    "    embeddings = np.ndarray((vocab_size, embed_size), dtype=np.float32)\n",
    "    for i in range(vocab_size):\n",
    "        line = lines[i+1].split(delimiter)[:-1]\n",
    "        word = line[0]\n",
    "        if word in word2int:\n",
    "            wordvec = np.array([float(j) for j in line[1:]])\n",
    "            embeddings[word2int[word]] = wordvec\n",
    "    return embeddings\n",
    "em = parseVec('results/model.vec', ' ')\n",
    "em2 = parseVec('results/vec.txt', '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ኢትዮጵያ', 0.9999999999999999),\n",
       " ('ኢትዮጽያ', 0.9987390873230826),\n",
       " ('ኢትየጵያ', 0.9979664712148368),\n",
       " ('ኢትዮጵያው', 0.997694764698056),\n",
       " ('ኢትዮጵያዊ', 0.9974197641431508),\n",
       " ('ኢትዮጵያዋ', 0.9973087032394075),\n",
       " ('ኢትዮጵዊ', 0.9972418509132015),\n",
       " ('ኢተዮጵያ', 0.9970514674721621),\n",
       " ('ኢትዮጵያዬ', 0.9969908263368988),\n",
       " ('ኢትዮጵያን', 0.9968322095697312)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings = normalize(np.load('results/wi.npy'))\n",
    "utils = Utils(embedding=normalize(embeddings), word2int=word2int, int2word=int2char)\n",
    "utils.sorted_sim(\"ኢትዮጵያ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomaly: 61.47% semantic: 0.37% syntactic: 11.21% \n"
     ]
    }
   ],
   "source": [
    "# seq_norm = normalize(seq_emb)\n",
    "# sem_norm = normalize(sem_emb)\n",
    "# e = np.concatenate([sem_emb, embeddings], axis=1)\n",
    "evaluate(embed_size=embed_size,final_embedding=normalize(embeddings), word2int=word2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('results/seq_encoding', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, infenc, infdec = conv_model2(13, 13, embed_size, embed_size)\n",
    "adam = keras.optimizers.Nadam(0.001)\n",
    "# train.compile(optimizer=adam, loss='mse')\n",
    "train.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['acc'])\n",
    "# train.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_batches = len(vocab) // batch_size\n",
    "history = train.fit_generator(gen, steps_per_epoch=n_batches, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infdec.save('models/decoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pred_embeddings(vocab, infenc):\n",
    "    embeddings = np.ndarray((len(vocab), embed_size))\n",
    "    i = 0\n",
    "    buffer = []\n",
    "    buffer_size = 10000\n",
    "    embed = []\n",
    "    for wi, word in enumerate(vocab):\n",
    "        word = int2word[word2int[word]]\n",
    "        buffer.append(word2vec(char2int, word, 13))\n",
    "        embed.append(sem_embed[word2int[word]])\n",
    "        if len(buffer) == buffer_size or len(vocab) - wi < buffer_size:\n",
    "            buffer_np = np.stack(buffer).reshape((-1, 13, 309, 1))\n",
    "            embed = np.stack(embed)\n",
    "            result = infenc.predict([buffer_np, embed])\n",
    "            embeddings[i:i+len(buffer)] = result\n",
    "            i += len(buffer)\n",
    "            buffer = []\n",
    "            embed = []\n",
    "            if i % (4 *buffer_size) == 0:\n",
    "                print(\"Predicting: {0:.2f}%\".format((i * 100.0 / len(vocab))))\n",
    "    print(\"finished\")\n",
    "    return embeddings\n",
    "\n",
    "embeddings = pred_embeddings(vocab, infenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 13.43%\n",
      "Predicting: 26.86%\n",
      "Predicting: 40.29%\n",
      "Predicting: 53.72%\n",
      "Predicting: 67.15%\n",
      "Predicting: 80.58%\n",
      "Predicting: 94.01%\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "def pred_embeddings_multi(vocab, encoder, char2tup):\n",
    "    embeddings = np.ndarray((len(vocab), embed_size))\n",
    "    i = 0\n",
    "    buffer = []\n",
    "    buffer_size = 10000\n",
    "    for wi, word in enumerate(vocab):\n",
    "        word = int2word[word2int[word]]\n",
    "        convec, vowvec = word2vec_seperated(char2tup, word, n_chars, n_consonant, n_vowel)\n",
    "        mat = np.concatenate([convec, vowvec], axis=1)\n",
    "        buffer.append(mat)\n",
    "        if len(buffer) == buffer_size or len(vocab) - wi < buffer_size:\n",
    "            buffer_np = np.stack(buffer).reshape((-1, n_chars, (n_consonant + n_vowel), 1))\n",
    "            result = encoder.predict(buffer_np)\n",
    "            embeddings[i:i+len(buffer)] = result\n",
    "            i += len(buffer)\n",
    "            buffer = []\n",
    "            if i % (4 *buffer_size) == 0:\n",
    "                print(\"Predicting: {0:.2f}%\".format((i * 100.0 / len(vocab))))\n",
    "    print(\"finished\")\n",
    "    return embeddings\n",
    "\n",
    "embeddings = pred_embeddings_multi(vocab, multi_enc, char2tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(len(test_vocab)):\n",
    "#     word = test_vocab[i]\n",
    "#     result = decode_sequence(infdec, int2char, embeddings[word2int[word]].reshape((1, -1)))\n",
    "#     result = ''.join(result).strip()#[1:-1]\n",
    "#     print(word, result)\n",
    "#     if i ==  10:\n",
    "#         break\n",
    "rand_vocab = np.random.choice(vocab, 10)\n",
    "for i in range(10):\n",
    "    word = rand_vocab[i]\n",
    "    result = decode_multi_sequence(multi_dec, char2tup, tup2char, \n",
    "                                   embeddings[word2int[word]].reshape((1, -1)), n_consonant, n_vowel)\n",
    "    result = ''.join(result).strip()#[1:-1]\n",
    "    print(word, result)\n",
    "    if i ==  10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_normal = normalize(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(embedding_normal, embed_size=embed_size, word2int=word2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"results/char_embedding\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenses = open('data/news.txt', encoding='utf-8').read().split('*')\n",
    "sentenses = [s.strip().split() for s in sentenses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentenses, \n",
    "                            size=128, \n",
    "                            iter=20, \n",
    "                            min_count=1,\n",
    "                            negative=10,\n",
    "                            sg=1,\n",
    "                            seed=seed_val\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = model.accuracy('data/syntax.txt')\n",
    "result2 = model.accuracy('data/semantic.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_embed = np.ndarray((len(vocab), 128))\n",
    "for voc in vocab:\n",
    "    if voc is not '*':\n",
    "        sem_embed[word2int[voc]] = model.wv[voc]\n",
    "np.save(\"results/word2vec_embedding\", sem_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_embed_normal = normalize(sem_embed)\n",
    "# emb_norm = normalize(embeddings)\n",
    "# ee = sem_embed + embeddings*.1366\n",
    "# full_embed = np.concatenate([5*sem_embed_normal, 2*emb_norm], axis=1)\n",
    "full_embed_normal = sem_embed_normal + embedding_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_embed_normal = normalize(embedding_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(full_embed_normal, word2int, embed_size=full_embed_normal.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emu_model, emu_pred = embeder3(128, 128)\n",
    "adam = keras.optimizers.Nadam(0.0001)\n",
    "sgd = keras.optimizers.SGD(.01)\n",
    "emu_model.compile(optimizer=adam, loss='mse')\n",
    "batch_size = 500\n",
    "skip_window = 5\n",
    "gen4 = generate4(int_words, embeddings, word2int, batch_size, skip_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = len(words) // batch_size\n",
    "history = emu_model.fit_generator(gen4, steps_per_epoch=n_batches, epochs = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myembedding_norm=emu_model.predict(embedding_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myembeddings = normalize(myembedding_norm)\n",
    "# sem_embed_normal = normalize(sem_embed)\n",
    "ee = myembeddings #+ sem_embed_normal\n",
    "evaluate(ee, word2int, embed_size=myembeddings.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min = 1\n",
    "# b = embedding_normal[0]\n",
    "# for i in range(len(embedding_normal)):\n",
    "#     a = embedding_normal[i]\n",
    "#     d = a.dot(b)\n",
    "#     if d < min:\n",
    "#         print(d)\n",
    "#         min = d\n",
    "# embedding_normal[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_words(words, start, length):\n",
    "    if start + length > len(words):\n",
    "        end = start + length - len(words)\n",
    "        return words[start:] + words[0:end], end\n",
    "    else:\n",
    "        end = start + length\n",
    "        return words[start:end], end\n",
    "\n",
    "def get_context_words(words, start, length):\n",
    "    if start + length > len(words):\n",
    "        start = 0\n",
    "    end = start + length\n",
    "    return words[start:end], start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(data, embeds, word2int, int2word, unigrams, batch_size, skip_window):\n",
    "    embed_szie = embeds.shape[1]\n",
    "    assert batch_size % skip_window == 0\n",
    "    ci = skip_window  # current_index\n",
    "    batch_y = np.ones(shape=(batch_size, 1), dtype=np.float32)\n",
    "    while True:\n",
    "        batch_inputs = np.ndarray(shape=(batch_size, embed_size), dtype=np.float32)\n",
    "        batch_labels = np.ndarray(shape=(batch_size, embed_size), dtype=np.float32)\n",
    "        batch_index = 0\n",
    "        shuffle_index = np.random.shuffle(np.arange(batch_size))\n",
    "        for batch_index in range(0, batch_size, skip_window * 2):  # fill the batch inputs\n",
    "            context = data[ci - skip_window:ci + skip_window + 1]\n",
    "            # remove the target from context words\n",
    "            target = context.pop(skip_window)\n",
    "            # context = random.sample(context, skip_window * 2)\n",
    "            word_index = 0\n",
    "            for b in range(batch_index, batch_index + skip_window * 2):\n",
    "                con_vec = embeds[word2int[context[word_index]]]\n",
    "                target_vec = embeds[word2int[target]]\n",
    "                batch_inputs[b] = con_vec\n",
    "                batch_labels[b] = target_vec\n",
    "                word_index += 1\n",
    "\n",
    "            ci += 1\n",
    "        if len(data) - ci - skip_window < batch_size:\n",
    "            ci = skip_window\n",
    "        for ri  in range(0, batch_size, 2):\n",
    "            batch_labels[ri] = embeds[np.random.randint(len(embeds))]\n",
    "            batch_y[ri][0] = batch_labels[ri].dot(batch_inputs[ri])\n",
    "#         print(batch_labels.shape)\n",
    "#         batch_labels = batch_labels[shuffle_index].reshape((-1, 128))\n",
    "# #         print(batch_labels.shape)\n",
    "#         batch_inputs = batch_inputs[shuffle_index].reshape((-1, 128))\n",
    "#         batch_y = batch_y[shuffle_index].reshape((-1, 1))\n",
    "        yield [batch_inputs, batch_labels], batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate2(data, embeds, word2int, batch_size, skip_window):\n",
    "    embed_size = embeds.shape[1]\n",
    "    assert batch_size % skip_window == 0\n",
    "    ci = skip_window  # current_index\n",
    "    input_width = embed_size * 2 * skip_window\n",
    "    while True:\n",
    "        batch_inputs = np.ndarray(shape=(batch_size, input_width), dtype=np.float32)\n",
    "        batch_labels = np.ndarray(shape=(batch_size, embed_size), dtype=np.float32)\n",
    "        batch_index = 0\n",
    "        for batch_index in range(batch_size):  # fill the batch inputs\n",
    "            context = data[ci - skip_window:ci + skip_window + 1]\n",
    "            target = context.pop(skip_window)\n",
    "#             print(context, target)\n",
    "            context_vec = []\n",
    "            target_vec = embeds[word2int[target]]\n",
    "            for word in context:\n",
    "                con_vec = embeds[word2int[word]]\n",
    "                context_vec.append(con_vec)\n",
    "            context_vec = np.hstack(context_vec)\n",
    "#             batch_inputs[batch_index] = context_vec\n",
    "#             batch_labels[batch_index] = target_vec\n",
    "            \n",
    "            ci += 1\n",
    "        if len(data) - ci - skip_window < batch_size:\n",
    "            ci = skip_window\n",
    "        yield batch_inputs, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate3(data, embeds, word2int, batch_size, skip_window):\n",
    "    embed_size = embeds.shape[1]\n",
    "    ci = 0  # current_word_index\n",
    "    input_width = embed_size * 2 * skip_window\n",
    "    while True:\n",
    "        batch_inputs_left = np.ndarray(shape=(batch_size, embed_size), dtype=np.float32)\n",
    "        batch_inputs_right = np.ndarray(shape=(batch_size, embed_size), dtype=np.float32)\n",
    "        batch_labels = np.ndarray(shape=(batch_size, embed_size), dtype=np.float32)\n",
    "        batch_index = 0\n",
    "        for batch_index in range(batch_size):  # fill the batch inputs\n",
    "            context, ci = get_context_words(data, ci, 3)\n",
    "            ci = ci + 1\n",
    "            left_word_vec = embeds[word2int[context[0]]]\n",
    "            target_vec = embeds[word2int[context[1]]]\n",
    "            right_word_vec = embeds[word2int[context[2]]]\n",
    "            batch_inputs_left[batch_index] = left_word_vec\n",
    "            batch_inputs_right[batch_index] = right_word_vec\n",
    "            batch_labels[batch_index] = target_vec\n",
    "\n",
    "        yield [batch_inputs_left, batch_inputs_right], batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate4(data, embeds, word2int, batch_size, skip_window):\n",
    "    embed_size = embeds.shape[1]\n",
    "    ci = 0  # current_word_index\n",
    "    input_width = embed_size\n",
    "    while True:\n",
    "        batch_inputs = np.ndarray(shape=(batch_size, embed_size), dtype=np.float32)\n",
    "        batch_labels = np.ndarray(shape=(batch_size, embed_size), dtype=np.float32)\n",
    "        for batch_index in range(0, batch_size, skip_window):  # fill the batch inputs\n",
    "            context, ci = get_context_words(data, ci, skip_window * 2 + 1)\n",
    "            ci = ci + 1\n",
    "            target = context.pop(skip_window)\n",
    "            context = np.random.choice(context, skip_window)\n",
    "            context_vec = embeds[context[0]]\n",
    "            target_vec = embeds[context[1]]\n",
    "            \n",
    "            batch_inputs[batch_index:batch_index +\n",
    "                             skip_window] = embeddings[context]\n",
    "            batch_labels[batch_index:batch_index +\n",
    "                             skip_window] = embeddings[target]\n",
    "\n",
    "        yield batch_inputs, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen3 = generate3(words, embedding_normal, word2int, batch_size, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 3\n",
    "semantic_batch_size = 120\n",
    "input_size = 128\n",
    "# gg =  generate2(words, embeds_norm, word2int, batch_size=semantic_batch_size, skip_window=window) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model, con_model, tar_model = embedding_model3(input_size, 128, embed_size)\n",
    "adam = keras.optimizers.Nadam(0.0001)\n",
    "train_model.compile(optimizer=adam, loss=\"mse\")\n",
    "train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = len(words) // semantic_batch_size\n",
    "history = train_model.fit_generator(gen3, steps_per_epoch=n_batches, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = generate(words, embedding_normal, word2int, int2word, ns_unigrams, batch_size=semantic_batch_size, skip_window=3)\n",
    "[a, b], y = next(g)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 128#window * 2* embed_size\n",
    "# em_train, em_out = embedding_model(input_size, 128, embed_size)\n",
    "# adam = keras.optimizers.Nadam(lr=0.002)\n",
    "# em_train.compile(optimizer=adam, loss='mean_squared_error')\n",
    "# em_train.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model, con_model, tar_model = embedding_model2(input_size, 128, embed_size)\n",
    "adam = keras.optimizers.SGD(0.001)\n",
    "train_model.compile(optimizer=adam, loss=\"mse\", metrics=['mse', 'acc'])\n",
    "train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = len(words) // semantic_batch_size\n",
    "history = train_model.fit_generator(g, steps_per_epoch=n_batches, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vecs = []\n",
    "for i_word in range(len(vocab)):\n",
    "    word = int2word[i_word]\n",
    "    context_vecs.append(embedding_normal[word2int[word]])\n",
    "context_vecs = np.stack(context_vecs)\n",
    "context_embed = con_model.predict(context_vecs)\n",
    "# target_embed = tar_model.predict(context_vecs)\n",
    "# em = context_embed + target_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_normal = normalize(full_embed)\n",
    "evaluate(em_normal, word2int, embed_size=em_normal.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils = Utils(word2int,int2word, embedding_normal)\n",
    "# v = -em_normal[word2int['ገንዘብ']] + em_normal[word2int['ብር']]\n",
    "# dots = em_normal.dot(v).flatten()\n",
    "# int2word[np.argmax(dots)]\n",
    "utils.sorted_sim(\"ዶላር\")\n",
    "# utils.sorted_sim(\"ብር\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_input = [\n",
    "    embedding_normal[word2int['ነበር']].reshape((1, 128)),\n",
    "    embedding_normal[word2int['ነው']].reshape((1, 128)),\n",
    "]\n",
    "vec = train_model.predict(con_input).flatten()\n",
    "print(embedding_normal[word2int['ነበር']].dot(embedding_normal[word2int['ነው']]))\n",
    "int2word[np.argmax(embedding_normal.dot(vec))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min = 1\n",
    "b = em_normal[0]\n",
    "for i in range(len(em_normal)):\n",
    "    a = em_normal[i]\n",
    "    d = a.dot(b)\n",
    "    if d < min:\n",
    "        print(d)\n",
    "        min = d\n",
    "# embedding_normal[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# semantic = em_out.predict(embeds_norm)\n",
    "# gensim = GensimWrapper(embed_size, 0, log=True)\n",
    "# embeds = embeds.reshape((-1, 128))\n",
    "# norms = np.linalg.norm(semantic, axis=1, keepdims=True)\n",
    "# semantic_norm = semantic / norms\n",
    "vecs = []\n",
    "discovered = {}\n",
    "for i in range(window, len(words) - window):\n",
    "    context = words[i - window: i + window + 1]\n",
    "    target = context.pop(window)\n",
    "    if target not in discovered:\n",
    "        discovered[target] = len(discovered)\n",
    "        c_vec = []\n",
    "        for cword in context:\n",
    "            vec = embeds_norm[word2int[cword]]\n",
    "            c_vec.append(vec)\n",
    "        context_vec = np.hstack(c_vec)\n",
    "        vecs.append(context_vec)\n",
    "    if len(discovered) == len(vocab):\n",
    "        print(\"discovered\")\n",
    "        break\n",
    "    \n",
    "semantic = np.stack(vecs).reshape(-1, input_size)\n",
    "print(len(vecs), embeds_norm.shape)\n",
    "assert semantic.shape[0] == embeds_norm.shape[0]\n",
    "# semantic = em_out.predict(embeds_norm)\n",
    "# gensim = GensimWrapper(embed_size, 0, log=True)\n",
    "# embeds = embeds.reshape((-1, 128))\n",
    "# norms = np.linalg.norm(semantic, axis=1, keepdims=True)\n",
    "# semantic_norm = semantic / norms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
