{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, TimeDistributed\n",
    "from keras.layers import Concatenate, Flatten\n",
    "from keras.layers import GRU, Conv2D, MaxPooling2D\n",
    "from keras.layers import Input, Reshape, Dot\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from data_handle import *\n",
    "from gensim_wrapper import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model(n_input, n_output, n_enc_units, n_dec_units):\n",
    "    root_word_input = Input(shape=(13, 309, 1), name=\"root_word_input\")\n",
    "    \n",
    "    x = Conv2D(16, (3, 3), padding='same', activation='relu')(root_word_input)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    x = Conv2D(8, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "\n",
    "    state_out = Dense(n_dec_units, activation='tanh')(x)\n",
    "    state_h = Dense(n_dec_units, activation='relu')(state_out)\n",
    "    \n",
    "    decoder_inputs = Input(shape=(None, 309), name=\"target_word_input\")\n",
    "    decoder_gru = GRU(n_dec_units, return_sequences=True, return_state=True, name=\"decoder_gru\")\n",
    "    decoder_outputs, _= decoder_gru(decoder_inputs, initial_state=state_h)\n",
    "    \n",
    "    decoder_dense = Dense(309, activation='softmax', name=\"train_output\")\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    model = Model([root_word_input, decoder_inputs], decoder_outputs)\n",
    "    encoder_model = Model(root_word_input, state_out)\n",
    "    \n",
    "    decoder_state_input_h = Input(shape=(n_dec_units,))\n",
    "    decoder_outputs, state_h= decoder_gru(decoder_inputs, initial_state=decoder_state_input_h)\n",
    "\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model([decoder_inputs, decoder_state_input_h], [decoder_outputs, state_h])\n",
    "\n",
    "    return model, encoder_model, decoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_model(input_size, output_size, embed_size):\n",
    "    context_word = Input(shape=(input_size,), name=\"context_word\")\n",
    "    x = Dense(256, activation='relu')(context_word)\n",
    "    embeding = Dense(embed_size, activation='tanh')(x)\n",
    "    target_word = Dense(output_size, activation='relu')(embeding)\n",
    "    model = Model(context_word, target_word)\n",
    "    em_model = Model(context_word, embeding)\n",
    "    return model, em_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_loss(yTrue, yPred):\n",
    "    loss = K.sum(K.square(yTrue - yPred))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_model2(input_size, output_size, embed_size):\n",
    "    context_word = Input(shape=(input_size,), name=\"context_word\")\n",
    "    target_word = Input(shape=(input_size,), name=\"target_word\")\n",
    "    \n",
    "    layer1 = Dense(200, activation='tanh')\n",
    "    layer2 = Dense(200, activation='tanh')\n",
    "    \n",
    "    x = layer1(context_word)\n",
    "    y = layer1(target_word)\n",
    "#     y = layer2(y)\n",
    "    cosine_sim = Dot(normalize=True, axes=1)([x, y])\n",
    "#     z = Concatenate(axis=1)([x, y])\n",
    "#     z = Dense(20, activation='tanh')(z)\n",
    "    \n",
    "#     output = Dense(1, activation='tanh')(z)\n",
    "    model = Model([context_word, target_word], cosine_sim)\n",
    "    \n",
    "    con_model = Model(context_word, x)\n",
    "    tar_model = Model(target_word, y)\n",
    "    \n",
    "    return model, con_model, tar_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(final_embedding, word2int, embed_size):\n",
    "    gensim = GensimWrapper(embed_size, 0, log=False)\n",
    "    gensim.set_embeddings(word2int, final_embedding)\n",
    "    result = gensim.evaluate()\n",
    "    for key in result:\n",
    "        print(\"{0}: {1:.2f}%\".format(key, result[key]), end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(embeddings):\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    return embeddings / norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = read_file()\n",
    "vocab, word2int, int2word = build_vocab(words)\n",
    "word2freq = get_frequency(words, word2int, int2word)\n",
    "char2int, int2char, char2tup, tup2char, n_consonant, n_vowel = build_charset()\n",
    "ns_unigrams = ns_sample(word2freq, word2int, int2word, .75)\n",
    "n_chars = 11 + 2 \n",
    "n_features = len(char2int)\n",
    "batch_size = 128\n",
    "embed_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = generate_word_images(vocab, char2int, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, infenc, infdec = conv_model(13, 13, embed_size, embed_size)\n",
    "train.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2326/2326 [==============================] - 288s 124ms/step - loss: 0.2501 - acc: 0.9573\n",
      "Epoch 2/2\n",
      "  29/2326 [..............................] - ETA: 4:43 - loss: 0.0103 - acc: 0.9988"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-d46f82838e1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mn_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2224\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2226\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1883\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_batches = len(vocab) // batch_size\n",
    "history = train.fit_generator(gen, steps_per_epoch=n_batches, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 13.43%\n",
      "Predicting: 26.86%\n",
      "Predicting: 40.29%\n",
      "Predicting: 53.72%\n",
      "Predicting: 67.15%\n",
      "Predicting: 80.58%\n",
      "Predicting: 94.01%\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.ndarray((len(vocab), embed_size))\n",
    "i = 0\n",
    "buffer = []\n",
    "buffer_size = 10000\n",
    "for i_word in range(len(vocab)):\n",
    "    word = int2word[i_word]\n",
    "    buffer.append(word2vec(char2int, word, 13))\n",
    "    if len(buffer) == buffer_size or len(vocab) - i_word < buffer_size:\n",
    "        buffer_np = np.stack(buffer).reshape((-1, 13, 309, 1))\n",
    "        result = infenc.predict(buffer_np)\n",
    "        embeddings[i:i+len(buffer)] = result\n",
    "        i += len(buffer)\n",
    "        buffer = []\n",
    "        if i % (4 *buffer_size) == 0:\n",
    "            print(\"Predicting: {0:.2f}%\".format((i * 100.0 / len(vocab))))\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_normal = normalize(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomaly: 74.31% semantic: 0.00% syntactic: 6.06% \n"
     ]
    }
   ],
   "source": [
    "evaluate(embedding_normal, embed_size=embed_size, word2int=word2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.746993816644687\n",
      "0.7455731075560058\n",
      "0.7405070451351925\n",
      "0.6837609011569525\n",
      "0.6553698024067582\n",
      "0.6534981861889038\n",
      "0.6170378916517292\n",
      "0.578099309382692\n",
      "0.5738307158201177\n",
      "0.5706667930047029\n",
      "0.5479654795724307\n",
      "0.5402338485465943\n",
      "0.5386981173617826\n",
      "0.5354247981723298\n",
      "0.5320975320227048\n",
      "0.5182689613368392\n",
      "0.5119981997459266\n",
      "0.5082673162149771\n",
      "0.5036355894396558\n",
      "0.502581256811274\n",
      "0.49970151474682856\n",
      "0.4940049595318914\n",
      "0.490995922373351\n",
      "0.48110292678378785\n"
     ]
    }
   ],
   "source": [
    "min = 1\n",
    "b = embedding_normal[0]\n",
    "for i in range(len(embedding_normal)):\n",
    "    a = embedding_normal[i]\n",
    "    d = a.dot(b)\n",
    "    if d < min:\n",
    "        print(d)\n",
    "        min = d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_words(words, start, length):\n",
    "    if start + length > len(words):\n",
    "        end = start + length - len(words)\n",
    "        return words[start:] + words[0:end], end\n",
    "    else:\n",
    "        end = start + length\n",
    "        return words[start:end], end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(data, embeds, word2int, int2word, unigrams, batch_size, skip_window):\n",
    "    embed_szie = embeds.shape[1]\n",
    "    assert batch_size % skip_window == 0\n",
    "    ci = skip_window  # current_index\n",
    "    batch_y = np.ones(shape=(batch_size, 1), dtype=np.float32)\n",
    "    while True:\n",
    "        batch_inputs = np.ndarray(shape=(batch_size, embed_size), dtype=np.float32)\n",
    "        batch_labels = np.ndarray(shape=(batch_size, embed_size), dtype=np.float32)\n",
    "        batch_index = 0\n",
    "        shuffle_index = np.random.shuffle(np.arange(batch_size))\n",
    "        for batch_index in range(0, batch_size, skip_window * 2):  # fill the batch inputs\n",
    "            context = data[ci - skip_window:ci + skip_window + 1]\n",
    "            # remove the target from context words\n",
    "            target = context.pop(skip_window)\n",
    "            # context = random.sample(context, skip_window * 2)\n",
    "            word_index = 0\n",
    "            for b in range(batch_index, batch_index + skip_window * 2):\n",
    "                con_vec = embeds[word2int[context[word_index]]]\n",
    "                target_vec = embeds[word2int[target]]\n",
    "                batch_inputs[b] = con_vec\n",
    "                batch_labels[b] = target_vec\n",
    "                word_index += 1\n",
    "\n",
    "            ci += 1\n",
    "        if len(data) - ci - skip_window < batch_size:\n",
    "            ci = skip_window\n",
    "        for ri  in range(0, batch_size, 2):\n",
    "            batch_labels[ri] = embeds[np.random.randint(len(embeds))]\n",
    "            batch_y[ri][0] = batch_labels[ri].dot(batch_inputs[ri])\n",
    "#         print(batch_labels.shape)\n",
    "#         batch_labels = batch_labels[shuffle_index].reshape((-1, 128))\n",
    "# #         print(batch_labels.shape)\n",
    "#         batch_inputs = batch_inputs[shuffle_index].reshape((-1, 128))\n",
    "#         batch_y = batch_y[shuffle_index].reshape((-1, 1))\n",
    "        yield [batch_inputs, batch_labels], batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate2(data, embeds, word2int, batch_size, skip_window):\n",
    "    embed_size = embeds.shape[1]\n",
    "    assert batch_size % skip_window == 0\n",
    "    ci = skip_window  # current_index\n",
    "    input_width = embed_size * 2 * skip_window\n",
    "    while True:\n",
    "        batch_inputs = np.ndarray(shape=(batch_size, input_width), dtype=np.float32)\n",
    "        batch_labels = np.ndarray(shape=(batch_size, embed_size), dtype=np.float32)\n",
    "        batch_index = 0\n",
    "        for batch_index in range(batch_size):  # fill the batch inputs\n",
    "            context = data[ci - skip_window:ci + skip_window + 1]\n",
    "            target = context.pop(skip_window)\n",
    "#             print(context, target)\n",
    "            context_vec = []\n",
    "            target_vec = embeds[word2int[target]]\n",
    "            for word in context:\n",
    "                con_vec = embeds[word2int[word]]\n",
    "                context_vec.append(con_vec)\n",
    "            context_vec = np.hstack(context_vec)\n",
    "#             batch_inputs[batch_index] = context_vec\n",
    "#             batch_labels[batch_index] = target_vec\n",
    "            \n",
    "            ci += 1\n",
    "        if len(data) - ci - skip_window < batch_size:\n",
    "            ci = skip_window\n",
    "        yield batch_inputs, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 3\n",
    "semantic_batch_size = 120\n",
    "# gg =  generate2(words, embeds_norm, word2int, batch_size=semantic_batch_size, skip_window=window) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.66121984]\n",
      " [1.        ]\n",
      " [0.77953994]\n",
      " [1.        ]\n",
      " [0.7287003 ]\n",
      " [1.        ]\n",
      " [0.75767225]\n",
      " [1.        ]\n",
      " [0.726268  ]\n",
      " [1.        ]\n",
      " [0.7316121 ]\n",
      " [1.        ]\n",
      " [0.79719603]\n",
      " [1.        ]\n",
      " [0.6823764 ]\n",
      " [1.        ]\n",
      " [0.74326587]\n",
      " [1.        ]\n",
      " [0.73710036]\n",
      " [1.        ]\n",
      " [0.7228323 ]\n",
      " [1.        ]\n",
      " [0.72566265]\n",
      " [1.        ]\n",
      " [0.8295275 ]\n",
      " [1.        ]\n",
      " [0.8262774 ]\n",
      " [1.        ]\n",
      " [0.71502846]\n",
      " [1.        ]\n",
      " [0.7049107 ]\n",
      " [1.        ]\n",
      " [0.8028245 ]\n",
      " [1.        ]\n",
      " [0.7224753 ]\n",
      " [1.        ]\n",
      " [0.68767524]\n",
      " [1.        ]\n",
      " [0.73093307]\n",
      " [1.        ]\n",
      " [0.7040472 ]\n",
      " [1.        ]\n",
      " [0.80999637]\n",
      " [1.        ]\n",
      " [0.762498  ]\n",
      " [1.        ]\n",
      " [0.6563698 ]\n",
      " [1.        ]\n",
      " [0.794647  ]\n",
      " [1.        ]\n",
      " [0.6811607 ]\n",
      " [1.        ]\n",
      " [0.76549244]\n",
      " [1.        ]\n",
      " [0.71231496]\n",
      " [1.        ]\n",
      " [0.7278384 ]\n",
      " [1.        ]\n",
      " [0.8051803 ]\n",
      " [1.        ]\n",
      " [0.69364685]\n",
      " [1.        ]\n",
      " [0.6980398 ]\n",
      " [1.        ]\n",
      " [0.7374277 ]\n",
      " [1.        ]\n",
      " [0.7708698 ]\n",
      " [1.        ]\n",
      " [0.7635394 ]\n",
      " [1.        ]\n",
      " [0.84554994]\n",
      " [1.        ]\n",
      " [0.78186667]\n",
      " [1.        ]\n",
      " [0.7640996 ]\n",
      " [1.        ]\n",
      " [0.73791283]\n",
      " [1.        ]\n",
      " [0.73104024]\n",
      " [1.        ]\n",
      " [0.74419427]\n",
      " [1.        ]\n",
      " [0.6929618 ]\n",
      " [1.        ]\n",
      " [0.7708759 ]\n",
      " [1.        ]\n",
      " [0.7470805 ]\n",
      " [1.        ]\n",
      " [0.86047983]\n",
      " [1.        ]\n",
      " [0.78546786]\n",
      " [1.        ]\n",
      " [0.7471479 ]\n",
      " [1.        ]\n",
      " [0.75358546]\n",
      " [1.        ]\n",
      " [0.6673235 ]\n",
      " [1.        ]\n",
      " [0.85202014]\n",
      " [1.        ]\n",
      " [0.79529434]\n",
      " [1.        ]\n",
      " [0.7603429 ]\n",
      " [1.        ]\n",
      " [0.7817709 ]\n",
      " [1.        ]\n",
      " [0.78136903]\n",
      " [1.        ]\n",
      " [0.7304417 ]\n",
      " [1.        ]\n",
      " [0.7610117 ]\n",
      " [1.        ]\n",
      " [0.72312856]\n",
      " [1.        ]\n",
      " [0.8022293 ]\n",
      " [1.        ]\n",
      " [0.7034673 ]\n",
      " [1.        ]\n",
      " [0.78911936]\n",
      " [1.        ]]\n"
     ]
    }
   ],
   "source": [
    "g = generate(words, embedding_normal, word2int, int2word, ns_unigrams, batch_size=semantic_batch_size, skip_window=3)\n",
    "[a, b], y = next(g)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 128#window * 2* embed_size\n",
    "# em_train, em_out = embedding_model(input_size, 128, embed_size)\n",
    "# adam = keras.optimizers.Nadam(lr=0.002)\n",
    "# em_train.compile(optimizer=adam, loss='mean_squared_error')\n",
    "# em_train.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "context_word (InputLayer)       (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "target_word (InputLayer)        (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 200)          25800       context_word[0][0]               \n",
      "                                                                 target_word[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1)            0           dense_16[0][0]                   \n",
      "                                                                 dense_16[1][0]                   \n",
      "==================================================================================================\n",
      "Total params: 25,800\n",
      "Trainable params: 25,800\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "train_model, con_model, tar_model = embedding_model2(input_size, 128, embed_size)\n",
    "adam = keras.optimizers.SGD(0.001)\n",
    "train_model.compile(optimizer=adam, loss=\"mse\", metrics=['mse', 'acc'])\n",
    "train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "34078/34078 [==============================] - 212s 6ms/step - loss: 0.0175 - mean_squared_error: 0.0175 - acc: 0.5000\n",
      "Epoch 2/2\n",
      "34078/34078 [==============================] - 215s 6ms/step - loss: 0.0168 - mean_squared_error: 0.0168 - acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "n_batches = len(words) // semantic_batch_size\n",
    "history = train_model.fit_generator(g, steps_per_epoch=n_batches, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vecs = []\n",
    "for i_word in range(len(vocab)):\n",
    "    word = int2word[i_word]\n",
    "    context_vecs.append(embedding_normal[word2int[word]])\n",
    "context_vecs = np.stack(context_vecs)\n",
    "context_embed = con_model.predict(context_vecs)\n",
    "target_embed = tar_model.predict(context_vecs)\n",
    "em = context_embed + target_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomaly: 74.31% semantic: 0.00% syntactic: 6.06% \n"
     ]
    }
   ],
   "source": [
    "em_normal = normalize(context_embed)\n",
    "evaluate(em_normal, word2int, embed_size=em_normal.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ሸር'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils = Utils(word2int, em_normal)\n",
    "v = -em_normal[word2int['ገንዘብ']] + em_normal[word2int['ብር']]\n",
    "dots = em_normal.dot(v).flatten()\n",
    "int2word[np.argmax(dots)]\n",
    "# utils.sorted_sim(\"ዶላር\")\n",
    "# utils.sorted_sim(\"ብር\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9628557]], dtype=float32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_input = [\n",
    "    embedding_normal[word2int['ዶላር']].reshape((1, 128)),\n",
    "    embedding_normal[word2int['ብር']].reshape((1, 128)),\n",
    "]\n",
    "train_model.predict(con_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeds_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-0f979ca861d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mc_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mvec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membeds_norm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword2int\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0mc_vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mcontext_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embeds_norm' is not defined"
     ]
    }
   ],
   "source": [
    "# semantic = em_out.predict(embeds_norm)\n",
    "# gensim = GensimWrapper(embed_size, 0, log=True)\n",
    "# embeds = embeds.reshape((-1, 128))\n",
    "# norms = np.linalg.norm(semantic, axis=1, keepdims=True)\n",
    "# semantic_norm = semantic / norms\n",
    "vecs = []\n",
    "discovered = {}\n",
    "for i in range(window, len(words) - window):\n",
    "    context = words[i - window: i + window + 1]\n",
    "    target = context.pop(window)\n",
    "    if target not in discovered:\n",
    "        discovered[target] = len(discovered)\n",
    "        c_vec = []\n",
    "        for cword in context:\n",
    "            vec = embeds_norm[word2int[cword]]\n",
    "            c_vec.append(vec)\n",
    "        context_vec = np.hstack(c_vec)\n",
    "        vecs.append(context_vec)\n",
    "    if len(discovered) == len(vocab):\n",
    "        print(\"discovered\")\n",
    "        break\n",
    "    \n",
    "semantic = np.stack(vecs).reshape(-1, input_size)\n",
    "print(len(vecs), embeds_norm.shape)\n",
    "assert semantic.shape[0] == embeds_norm.shape[0]\n",
    "# semantic = em_out.predict(embeds_norm)\n",
    "# gensim = GensimWrapper(embed_size, 0, log=True)\n",
    "# embeds = embeds.reshape((-1, 128))\n",
    "# norms = np.linalg.norm(semantic, axis=1, keepdims=True)\n",
    "# semantic_norm = semantic / norms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
