{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, TimeDistributed\n",
    "from keras.layers import Concatenate, Flatten\n",
    "from keras.layers import GRU, Conv2D, MaxPooling2D\n",
    "from keras.layers import Input, Reshape\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "import keras\n",
    "from data_handle import *\n",
    "from gensim_wrapper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model(n_input, n_output, n_enc_units, n_dec_units):\n",
    "    root_word_input = Input(shape=(13, 309, 1), name=\"root_word_input\")\n",
    "    \n",
    "    x = Conv2D(16, (3, 3), padding='same', activation='relu')(root_word_input)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    x = Conv2D(8, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "\n",
    "    state_h = Dense(n_dec_units, activation='relu')(x)\n",
    "    \n",
    "    decoder_inputs = Input(shape=(None, 309), name=\"target_word_input\")\n",
    "    decoder_gru = GRU(n_dec_units, return_sequences=True, return_state=True, name=\"decoder_gru\")\n",
    "    decoder_outputs, _= decoder_gru(decoder_inputs, initial_state=state_h)\n",
    "    \n",
    "    decoder_dense = Dense(309, activation='softmax', name=\"train_output\")\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    model = Model([root_word_input, decoder_inputs], decoder_outputs)\n",
    "    encoder_model = Model(root_word_input, state_h)\n",
    "    \n",
    "    decoder_state_input_h = Input(shape=(n_dec_units,))\n",
    "    decoder_outputs, state_h= decoder_gru(decoder_inputs, initial_state=decoder_state_input_h)\n",
    "\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model([decoder_inputs, decoder_state_input_h], [decoder_outputs, state_h])\n",
    "\n",
    "    return model, encoder_model, decoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_model(embed_size):\n",
    "    context_word = Input(shape=(embed_size,), name=\"context_word\")\n",
    "    target_word = Input(shape=(embed_size,), name=\"target_word\")\n",
    "    embeder = Dense(200, activation='tanh')\n",
    "    con_emb = embeder(context_word)\n",
    "    tar_emb = embeder(tar_emb)\n",
    "    target_word = Dense(embed_size, activation='tanh')(embeding)\n",
    "    model = Model(context_word, target_word)\n",
    "    em_model = Model(context_word, embeding)\n",
    "    return model, em_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = read_file()\n",
    "vocab, word2int, int2word = build_vocab(words)\n",
    "word2freq = get_frequency(words, word2int, int2word)\n",
    "char2int, int2char, char2tup, tup2char, n_consonant, n_vowel = build_charset()\n",
    "\n",
    "n_chars = 11 + 2\n",
    "n_features = len(char2int)\n",
    "batch_size = 128\n",
    "embed_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = generate_word_images(vocab, char2int, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, infenc, infdec = conv_model(13, 13, embed_size, embed_size)\n",
    "train.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "root_word_input (InputLayer)    (None, 13, 309, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 13, 309, 16)  160         root_word_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 6, 154, 16)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 6, 154, 8)    1160        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 3, 77, 8)     0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1848)         0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "target_word_input (InputLayer)  (None, None, 309)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          236672      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru (GRU)               [(None, None, 128),  168192      target_word_input[0][0]          \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "train_output (Dense)            (None, None, 309)    39861       decoder_gru[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 446,045\n",
      "Trainable params: 446,045\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "train.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2326/2326 [==============================] - 294s 127ms/step - loss: 0.3463 - acc: 0.9510\n",
      "Epoch 2/2\n",
      "2326/2326 [==============================] - 283s 122ms/step - loss: 0.0626 - acc: 0.9961\n"
     ]
    }
   ],
   "source": [
    "n_batches = len(vocab) // batch_size\n",
    "history = train.fit_generator(gen, steps_per_epoch=n_batches, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297836\n",
      "0.0\n",
      "3.3575524785452395\n",
      "6.715104957090479\n",
      "10.072657435635719\n",
      "13.430209914180958\n",
      "16.787762392726197\n",
      "20.145314871271438\n",
      "23.50286734981668\n",
      "26.860419828361916\n",
      "30.217972306907157\n",
      "33.575524785452394\n",
      "36.93307726399764\n",
      "40.290629742542876\n",
      "43.64818222108811\n",
      "47.00573469963336\n",
      "50.363287178178595\n",
      "53.72083965672383\n",
      "57.07839213526908\n",
      "60.435944613814314\n",
      "63.79349709235955\n",
      "67.15104957090479\n",
      "70.50860204945003\n",
      "73.86615452799528\n",
      "77.22370700654051\n",
      "80.58125948508575\n",
      "83.938811963631\n",
      "87.29636444217623\n",
      "90.65391692072147\n",
      "94.01146939926672\n",
      "97.36902187781195\n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "word2int = {}\n",
    "i = 0\n",
    "print(len(vocab))\n",
    "for word in vocab:\n",
    "    word2int[word] = len(word2int)\n",
    "    vec = word2vec(char2int, word, 13).reshape((1, 13, 309, 1))\n",
    "    emb = infenc.predict(vec)\n",
    "    embeddings.append(emb)\n",
    "    if i % 10000 == 0:\n",
    "        print(i * 100.0 / len(vocab))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-24 16:18:20,852 : INFO : collecting all words and their counts\n",
      "2018-10-24 16:18:20,853 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-10-24 16:18:20,912 : INFO : PROGRESS: at sentence #10000, processed 150787 words, keeping 38040 word types\n",
      "2018-10-24 16:18:21,025 : INFO : PROGRESS: at sentence #20000, processed 309265 words, keeping 61588 word types\n",
      "2018-10-24 16:18:21,080 : INFO : PROGRESS: at sentence #30000, processed 477795 words, keeping 81867 word types\n",
      "2018-10-24 16:18:21,164 : INFO : PROGRESS: at sentence #40000, processed 647737 words, keeping 100652 word types\n",
      "2018-10-24 16:18:21,227 : INFO : PROGRESS: at sentence #50000, processed 805941 words, keeping 116075 word types\n",
      "2018-10-24 16:18:21,291 : INFO : PROGRESS: at sentence #60000, processed 967897 words, keeping 129675 word types\n",
      "2018-10-24 16:18:21,365 : INFO : PROGRESS: at sentence #70000, processed 1144759 words, keeping 143321 word types\n",
      "2018-10-24 16:18:21,417 : INFO : PROGRESS: at sentence #80000, processed 1302619 words, keeping 154734 word types\n",
      "2018-10-24 16:18:21,487 : INFO : PROGRESS: at sentence #90000, processed 1465448 words, keeping 165946 word types\n",
      "2018-10-24 16:18:21,552 : INFO : PROGRESS: at sentence #100000, processed 1622592 words, keeping 176420 word types\n",
      "2018-10-24 16:18:21,641 : INFO : PROGRESS: at sentence #110000, processed 1781965 words, keeping 188202 word types\n",
      "2018-10-24 16:18:21,690 : INFO : PROGRESS: at sentence #120000, processed 1935355 words, keeping 198206 word types\n",
      "2018-10-24 16:18:21,754 : INFO : PROGRESS: at sentence #130000, processed 2094818 words, keeping 207824 word types\n",
      "2018-10-24 16:18:21,804 : INFO : PROGRESS: at sentence #140000, processed 2254406 words, keeping 216851 word types\n",
      "2018-10-24 16:18:21,869 : INFO : PROGRESS: at sentence #150000, processed 2415368 words, keeping 225189 word types\n",
      "2018-10-24 16:18:21,938 : INFO : PROGRESS: at sentence #160000, processed 2563945 words, keeping 233083 word types\n",
      "2018-10-24 16:18:21,991 : INFO : PROGRESS: at sentence #170000, processed 2722052 words, keeping 241913 word types\n",
      "2018-10-24 16:18:22,055 : INFO : PROGRESS: at sentence #180000, processed 2877801 words, keeping 250381 word types\n",
      "2018-10-24 16:18:22,117 : INFO : PROGRESS: at sentence #190000, processed 3043652 words, keeping 259214 word types\n",
      "2018-10-24 16:18:22,200 : INFO : PROGRESS: at sentence #200000, processed 3207579 words, keeping 267821 word types\n",
      "2018-10-24 16:18:22,261 : INFO : PROGRESS: at sentence #210000, processed 3360065 words, keeping 275715 word types\n",
      "2018-10-24 16:18:22,318 : INFO : PROGRESS: at sentence #220000, processed 3527065 words, keeping 283402 word types\n",
      "2018-10-24 16:18:22,380 : INFO : PROGRESS: at sentence #230000, processed 3682274 words, keeping 289997 word types\n",
      "2018-10-24 16:18:22,430 : INFO : collected 297835 word types from a corpus of 3849457 raw words and 239983 sentences\n",
      "2018-10-24 16:18:22,431 : INFO : Loading a fresh vocabulary\n",
      "2018-10-24 16:18:23,302 : INFO : effective_min_count=1 retains 297835 unique words (100% of original 297835, drops 0)\n",
      "2018-10-24 16:18:23,303 : INFO : effective_min_count=1 leaves 3849457 word corpus (100% of original 3849457, drops 0)\n",
      "2018-10-24 16:18:24,283 : INFO : deleting the raw counts dictionary of 297835 items\n",
      "2018-10-24 16:18:24,289 : INFO : sample=0.001 downsamples 13 most-common words\n",
      "2018-10-24 16:18:24,290 : INFO : downsampling leaves estimated 3737727 word corpus (97.1% of prior 3849457)\n",
      "2018-10-24 16:18:25,252 : INFO : estimated required memory for 297835 words and 128 dimensions: 453900540 bytes\n",
      "2018-10-24 16:18:25,253 : INFO : resetting layer weights\n",
      "2018-10-24 16:18:30,152 : INFO : training model with 3 workers on 297835 vocabulary and 128 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-10-24 16:18:30,153 : INFO : training on a 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-10-24 16:18:30,154 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "gensim = GensimWrapper(embed_size, 0, log=True)\n",
    "embeds = np.stack(embeddings).reshape((-1, 128))\n",
    "norms = np.linalg.norm(embeds, axis=1, keepdims=True)\n",
    "embeds_normal = embeds / norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "2018-10-24 16:18:49,495 : INFO : semantic word embedding: 0.0% (0/185)\n",
      "2018-10-24 16:18:49,495 : INFO : total: 0.0% (0/185)\n",
      "2018-10-24 16:18:50,030 : INFO : syntactic evaluation: 9.8% (13/132)\n",
      "2018-10-24 16:18:50,031 : INFO : total: 9.8% (13/132)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.39449541284404\n"
     ]
    }
   ],
   "source": [
    "gensim.set_embeddings(word2int, embeds_normal)\n",
    "gensim.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "utils = Utils(word2int, embeds_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ኢትዮጵያን', 1.0),\n",
       " ('አትዮጵያን', 0.9997844),\n",
       " ('ኢትዮጵያና', 0.9996847),\n",
       " ('ኢትዮጵያያን', 0.99963176),\n",
       " ('ኢትዮጵያንና', 0.9995229),\n",
       " ('ኢትዮጵያንን', 0.9994656),\n",
       " ('ኤትዮጵያና', 0.99933463),\n",
       " ('ኢትዮጵያንም', 0.9992549),\n",
       " ('የኢትዮጵያን', 0.99925405),\n",
       " ('ኢትዮዮጵያና', 0.9991602)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.sorted_sim(\"ኢትዮጵያን\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(data, embeds, word2int, batch_size, skip_window):\n",
    "    embed_szie = embeds.shape[1]\n",
    "    assert batch_size % skip_window == 0\n",
    "    ci = skip_window  # current_index\n",
    "    while True:\n",
    "        batch_inputs = np.ndarray(shape=(batch_size, embed_size), dtype=np.float32)\n",
    "        batch_labels = np.ndarray(shape=(batch_size, embed_size), dtype=np.float32)\n",
    "        batch_index = 0\n",
    "        for batch_index in range(0, batch_size, skip_window * 2):  # fill the batch inputs\n",
    "            context = data[ci - skip_window:ci + skip_window + 1]\n",
    "            # remove the target from context words\n",
    "            target = context.pop(skip_window)\n",
    "            # context = random.sample(context, skip_window * 2)\n",
    "            word_index = 0\n",
    "            for b in range(batch_index, batch_index + skip_window * 2):\n",
    "                con_vec = embeds[word2int[context[word_index]]]\n",
    "                target_vec = embeds[word2int[target]]\n",
    "                batch_inputs[b] = con_vec\n",
    "                batch_labels[b] = target_vec\n",
    "                word_index += 1\n",
    "\n",
    "            ci += 1\n",
    "        if len(data) - ci - skip_window < batch_size:\n",
    "            ci = skip_window\n",
    "        yield batch_labels, batch_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = generate(words, embeds_normal, word2int, batch_size=120, skip_window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "context_word (InputLayer)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 200)               25800     \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 128)               25728     \n",
      "=================================================================\n",
      "Total params: 51,528\n",
      "Trainable params: 51,528\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "em_train, em_out = embedding_model(embed_size)\n",
    "adam = keras.optimizers.adam(lr=0.001)\n",
    "em_train.compile(optimizer=adam, loss='hinge')\n",
    "em_train.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "34078/34078 [==============================] - 162s 5ms/step - loss: 0.9478\n",
      "Epoch 2/2\n",
      "34078/34078 [==============================] - 165s 5ms/step - loss: 0.9478\n"
     ]
    }
   ],
   "source": [
    "n_batches = len(words) // 120\n",
    "history = em_train.fit_generator(g, steps_per_epoch=n_batches, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-24 18:16:12,318 : INFO : collecting all words and their counts\n",
      "2018-10-24 18:16:12,319 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-10-24 18:16:12,371 : INFO : PROGRESS: at sentence #10000, processed 150787 words, keeping 38040 word types\n",
      "2018-10-24 18:16:12,420 : INFO : PROGRESS: at sentence #20000, processed 309265 words, keeping 61588 word types\n",
      "2018-10-24 18:16:12,484 : INFO : PROGRESS: at sentence #30000, processed 477795 words, keeping 81867 word types\n",
      "2018-10-24 18:16:12,548 : INFO : PROGRESS: at sentence #40000, processed 647737 words, keeping 100652 word types\n",
      "2018-10-24 18:16:12,611 : INFO : PROGRESS: at sentence #50000, processed 805941 words, keeping 116075 word types\n",
      "2018-10-24 18:16:12,667 : INFO : PROGRESS: at sentence #60000, processed 967897 words, keeping 129675 word types\n",
      "2018-10-24 18:16:12,726 : INFO : PROGRESS: at sentence #70000, processed 1144759 words, keeping 143321 word types\n",
      "2018-10-24 18:16:12,778 : INFO : PROGRESS: at sentence #80000, processed 1302619 words, keeping 154734 word types\n",
      "2018-10-24 18:16:12,838 : INFO : PROGRESS: at sentence #90000, processed 1465448 words, keeping 165946 word types\n",
      "2018-10-24 18:16:12,900 : INFO : PROGRESS: at sentence #100000, processed 1622592 words, keeping 176420 word types\n",
      "2018-10-24 18:16:12,963 : INFO : PROGRESS: at sentence #110000, processed 1781965 words, keeping 188202 word types\n",
      "2018-10-24 18:16:13,016 : INFO : PROGRESS: at sentence #120000, processed 1935355 words, keeping 198206 word types\n",
      "2018-10-24 18:16:13,070 : INFO : PROGRESS: at sentence #130000, processed 2094818 words, keeping 207824 word types\n",
      "2018-10-24 18:16:13,130 : INFO : PROGRESS: at sentence #140000, processed 2254406 words, keeping 216851 word types\n",
      "2018-10-24 18:16:13,178 : INFO : PROGRESS: at sentence #150000, processed 2415368 words, keeping 225189 word types\n",
      "2018-10-24 18:16:13,247 : INFO : PROGRESS: at sentence #160000, processed 2563945 words, keeping 233083 word types\n",
      "2018-10-24 18:16:13,302 : INFO : PROGRESS: at sentence #170000, processed 2722052 words, keeping 241913 word types\n",
      "2018-10-24 18:16:13,351 : INFO : PROGRESS: at sentence #180000, processed 2877801 words, keeping 250381 word types\n",
      "2018-10-24 18:16:13,411 : INFO : PROGRESS: at sentence #190000, processed 3043652 words, keeping 259214 word types\n",
      "2018-10-24 18:16:13,467 : INFO : PROGRESS: at sentence #200000, processed 3207579 words, keeping 267821 word types\n",
      "2018-10-24 18:16:13,530 : INFO : PROGRESS: at sentence #210000, processed 3360065 words, keeping 275715 word types\n",
      "2018-10-24 18:16:13,583 : INFO : PROGRESS: at sentence #220000, processed 3527065 words, keeping 283402 word types\n",
      "2018-10-24 18:16:13,637 : INFO : PROGRESS: at sentence #230000, processed 3682274 words, keeping 289997 word types\n",
      "2018-10-24 18:16:13,693 : INFO : collected 297835 word types from a corpus of 3849457 raw words and 239983 sentences\n",
      "2018-10-24 18:16:13,694 : INFO : Loading a fresh vocabulary\n",
      "2018-10-24 18:16:14,490 : INFO : effective_min_count=1 retains 297835 unique words (100% of original 297835, drops 0)\n",
      "2018-10-24 18:16:14,490 : INFO : effective_min_count=1 leaves 3849457 word corpus (100% of original 3849457, drops 0)\n",
      "2018-10-24 18:16:15,390 : INFO : deleting the raw counts dictionary of 297835 items\n",
      "2018-10-24 18:16:15,395 : INFO : sample=0.001 downsamples 13 most-common words\n",
      "2018-10-24 18:16:15,398 : INFO : downsampling leaves estimated 3737727 word corpus (97.1% of prior 3849457)\n",
      "2018-10-24 18:16:16,194 : INFO : estimated required memory for 297835 words and 200 dimensions: 625453500 bytes\n",
      "2018-10-24 18:16:16,194 : INFO : resetting layer weights\n",
      "2018-10-24 18:16:20,500 : INFO : training model with 3 workers on 297835 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-10-24 18:16:20,500 : INFO : training on a 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-10-24 18:16:20,501 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "semantic = em_out.predict(embeds_normal)\n",
    "gensim = GensimWrapper(200, 0, log=True)\n",
    "norms = np.linalg.norm(semantic, axis=1, keepdims=True)\n",
    "semantic_normal = semantic / norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "2018-10-24 18:16:46,287 : INFO : semantic word embedding: 0.0% (0/185)\n",
      "2018-10-24 18:16:46,288 : INFO : total: 0.0% (0/185)\n",
      "2018-10-24 18:16:46,842 : INFO : syntactic evaluation: 3.0% (4/132)\n",
      "2018-10-24 18:16:46,842 : INFO : total: 3.0% (4/132)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.4770642201835\n"
     ]
    }
   ],
   "source": [
    "gensim.set_embeddings(word2int, semantic_normal)\n",
    "gensim.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
