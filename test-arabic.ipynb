{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import Dropout\n",
    "# from keras.layers import LSTM, TimeDistributed\n",
    "# from keras.layers import Concatenate, Flatten\n",
    "# from keras.layers import GRU, Conv2D, MaxPooling2D\n",
    "# from keras.layers import Input, Reshape, Dot, Add\n",
    "# from keras.models import Model\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.optimizers import RMSprop\n",
    "# # from keras.utils.vis_utils import plot_model\n",
    "# import keras\n",
    "# import keras.backend as K\n",
    "from data_handle import *\n",
    "from gensim_wrapper import *\n",
    "from utils import *\n",
    "import gensim\n",
    "import random\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "seed_val = 1000\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "# tf.set_random_seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseVec(file, delimiter):\n",
    "    lines = open(file, encoding='utf8').readlines()\n",
    "    vocab_size, embed_size = [int(s) for s in lines[0].split()]\n",
    "    vocab_size  = min(len(word2int), vocab_size)\n",
    "    embeddings = np.zeros((vocab_size + 1, embed_size), dtype=np.float64)\n",
    "    for i in range(1, vocab_size):\n",
    "        try:\n",
    "            line = lines[i][:-1].split(delimiter)\n",
    "            word = line[0]\n",
    "            if 'unk' in word:\n",
    "                continue\n",
    "            if word in word2int:\n",
    "                wordvec = np.array([np.float64(j) for j in line[1:] if j != ''])\n",
    "                embeddings[word2int[word]] = wordvec\n",
    "        except Exception as e:\n",
    "            print(lines[i])\n",
    "            print(e)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Library\\Projects\\amharic_word_embedding\\data_handle.py:338: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return array / norms\n",
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.931441649941231 7.449205851582461 2.2861349408470932 3.5049293241551083\n",
      "7.701715948315732 6.978946020994152 2.122857560482329 4.278155295399345\n"
     ]
    }
   ],
   "source": [
    "for dataId in [20]:\n",
    "    words = read_file(filename='data/corpus/{0}'.format(dataId))\n",
    "    xvocab, xword2int, xint2word = build_vocab(words)\n",
    "    words, word2freq = min_count_threshold(words)\n",
    "    vocab, word2int, int2word = build_vocab(words)\n",
    "\n",
    "    for trainId in range(2):\n",
    "        emb = parseVec('results/arabic/{0}_100-{1}.vec'.format(dataId, trainId), ' ')\n",
    "        nr = evaluate(word2int, normalize(emb), embed_size=emb.shape[1], corpus='data/corpus/{0}'.format(dataId), analogy='data/ar_questions.txt')\n",
    "        ur = evaluate(word2int, emb, embed_size=emb.shape[1], corpus='data/corpus/{0}'.format(dataId), analogy='data/ar_questions.txt')\n",
    "        nsemantic = 0\n",
    "        nsynatctic = 0\n",
    "        for nrr in nr:\n",
    "            if 'total' in nrr:\n",
    "                continue\n",
    "            if \"gram\" in nrr:\n",
    "                nsynatctic += nr[nrr]/6\n",
    "            else:\n",
    "                nsemantic += nr[nrr]/9\n",
    "\n",
    "        usemantic = 0\n",
    "        usynatctic = 0\n",
    "        for nrr in ur:\n",
    "            if 'total' in nrr:\n",
    "                continue\n",
    "            if \"gram\" in nrr:\n",
    "                usemantic += ur[nrr]/6\n",
    "            else:\n",
    "                usynatctic += ur[nrr]/9\n",
    "        print(nsynatctic, nsemantic, usynatctic, usemantic)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_emb = parseVec('results/10_100-0.vec', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Library\\Projects\\amharic_word_embedding\\data_handle.py:338: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return array / norms\n",
      "c:\\users\\leo\\appdata\\local\\conda\\conda\\envs\\gpu-tf\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.86916681017879 19.84990870412632 14.498638824971996 17.42845703447439\n"
     ]
    }
   ],
   "source": [
    "emb = seq_emb\n",
    "nr = evaluate(word2int, normalize(emb), embed_size=emb.shape[1], corpus='data/10', analogy='data/ar_questions.txt')\n",
    "ur = evaluate(word2int, emb, embed_size=emb.shape[1], corpus='data/10', analogy='data/ar_questions.txt')\n",
    "nsemantic = 0\n",
    "nsynatctic = 0\n",
    "for nrr in nr:\n",
    "    if 'total' in nrr:\n",
    "        continue\n",
    "    if \"gram\" in nrr:\n",
    "        nsynatctic += nr[nrr]/6\n",
    "    else:\n",
    "        nsemantic += nr[nrr]/9\n",
    "\n",
    "usemantic = 0\n",
    "usynatctic = 0\n",
    "for nrr in ur:\n",
    "    if 'total' in nrr:\n",
    "        continue\n",
    "    if \"gram\" in nrr:\n",
    "        usemantic += ur[nrr]/6\n",
    "    else:\n",
    "        usynatctic += ur[nrr]/9\n",
    "print(nsynatctic, nsemantic, usynatctic, usemantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'capital-common-countries': 5.9523809523809526, 'capital-world': 0.9979353062629044, 'currency': 0.0, 'city-in-state': 0.0, 'family': 8.823529411764707, 'gram1-adjective-to-adverb': 1.8421052631578947, 'gram2-opposite': 0.0, 'gram3-comparative': 4.273504273504273, 'gram4-superlative': 5.517241379310345, 'gram5-present-participle': 1.8279569892473118, 'gram6-nationality-adjective': 15.459723352318958, 'gram7-past-tense': 2.1839080459770117, 'gram8-plural': 1.075268817204301, 'gram9-plural-verbs': 8.68945868945869, 'total': 4.076063033717757}\n"
     ]
    }
   ],
   "source": [
    "print(nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = 0,  0\n",
    "for nrr in ur:\n",
    "    if 'total' in nrr:\n",
    "        continue\n",
    "    if \"gram\" in nrr:\n",
    "        a += ur[nrr]\n",
    "    else:\n",
    "        b += ur[nrr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.904742839079065 1.3810883040648816\n"
     ]
    }
   ],
   "source": [
    "print(a/6, b/9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_vecs = np.zeros(shape=(58385, 100))\n",
    "mmax = 11\n",
    "result = np.zeros(shape=(mmax, 6))\n",
    "for i in range(mmax):\n",
    "    emb = parseVec('results/sq/w2v{0}.txt'.format(i), ' ')\n",
    "    all_vecs += emb\n",
    "    nr = evaluate(word2int, normalize(emb), embed_size=emb.shape[1])\n",
    "    ur = evaluate(word2int, emb, embed_size=emb.shape[1])\n",
    "    result[i][0] = nr['syntactic']\n",
    "    result[i][1] = nr['semantic']\n",
    "    result[i][2] = nr['pick-one-out']\n",
    "    \n",
    "    result[i][3] = ur['syntactic']\n",
    "    result[i][4] = ur['semantic']\n",
    "    result[i][5] = ur['pick-one-out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = all_vecs/mmax\n",
    "print(evaluate(word2int, normalize(emb), embed_size=emb.shape[1]))\n",
    "print(evaluate(word2int, emb, embed_size=emb.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb2D = np.zeros((seq_emb.shape[0], seq_emb.shape[1] * sem_emb.shape[1]))\n",
    "for i in range(seq_emb.shape[0]):\n",
    "    seq = seq_emb[i]\n",
    "    sem = sem_emb[i]\n",
    "    vec = np.outer(seq, sem).flatten()\n",
    "    emb2D[i] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('data/newan2.txt', encoding='utf-8').readlines()\n",
    "emb = emb2D\n",
    "correct = 0\n",
    "for line in lines:\n",
    "    if ':' in line:\n",
    "        continue\n",
    "    try:\n",
    "        line = [x.strip() for x in line[:-1].split(' ')]\n",
    "        vs = [emb[word2int[w]] for w in line]\n",
    "        v = vs[1] - vs[0] + vs[2]\n",
    "        sims = emb.dot(v)\n",
    "        sim_with_word = [(word, sims[word2int[word]])\n",
    "                             for word in word2int.keys()]\n",
    "        sorted_sim = sorted(sim_with_word, key=lambda t: t[1], reverse=True)[:4]\n",
    "#         print(type(sorted_sim[0][1]), sorted_sim[0][1])\n",
    "        result = [pred[0] for pred in sorted_sim]\n",
    "        if not np.isnan(sorted_sim[0][1]):\n",
    "#             print(result, line[-1])\n",
    "            if line[-1] in result:\n",
    "#                 print(result, line[-1])\n",
    "                correct += 1\n",
    "    except:\n",
    "        pass\n",
    "print(correct * 100.0 / (len(lines) - 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util = Utils(word2int, int2word, emb2D)\n",
    "# lines = open('data/newan2.txt', encoding='utf-8').readlines()\n",
    "# acc = np.array([0, 0])\n",
    "# total = np.array([0, 0])\n",
    "# ind = 0\n",
    "# for line in lines[1:]:\n",
    "#     if ':' in line[:-1]:\n",
    "#         ind = 1\n",
    "#         continue\n",
    "#     ws = line[:-1].split()\n",
    "#     result = util.solve(ws)\n",
    "#     if result is None:\n",
    "#         continue\n",
    "#     if ws[-1] in result:\n",
    "#         acc[ind] += 1\n",
    "#     total[ind] += 1\n",
    "# print(acc * 100 / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = open('data/analogies.txt', encoding='utf-8').readlines()\n",
    "# e = normalize(cew)\n",
    "# # e = cew\n",
    "# for line in lines:\n",
    "# #     ws = \"ልጅቱ ልጁ ወይዘሮ አቶ\".split(' ')\n",
    "#     if \":\" in line:\n",
    "#         continue\n",
    "#     try:\n",
    "#         ws = line[:-1].split(' ')\n",
    "#         v = normalize(vec(ws[1], e) - vec(ws[0], e) + vec(ws[2], e))\n",
    "#         V = np.dot(e, v)\n",
    "#         sort = list(sorted(enumerate(V[1:]), key=lambda x: x[1], reverse=True))\n",
    "#         so = [int2word[index[0]] for index in sort[:4]]\n",
    "#         if ws[-1] in so:\n",
    "#             print(so, ws[-1])\n",
    "# #         break\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = open('data/analogies.txt', encoding='utf-8').readlines()\n",
    "# words = []\n",
    "# for line in lines:\n",
    "#     ws = line[:-1].split(' ')\n",
    "#     in_vocab = True\n",
    "#     for w in ws:\n",
    "#         if w not in word2int:\n",
    "#             in_vocab = False\n",
    "#         if in_vocab:\n",
    "#             words.append(ws)\n",
    "# ws = []\n",
    "# for wl in words:\n",
    "#     s = ' '.join(wl)\n",
    "#     ws.append(s)\n",
    "# ws = '\\n'.join(ws)\n",
    "# open('data/newan.txt', mode='w', encoding='utf-8').write(ws)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = open('data/analogies.txt', encoding='utf-8').readlines()\n",
    "# words = []\n",
    "# lns = {}\n",
    "# for line in lines:\n",
    "#     ws = line[:-1].split(' ')\n",
    "#     in_vocab = True\n",
    "#     for w in ws:\n",
    "#         if w not in word2int:\n",
    "#             in_vocab = False\n",
    "#         if in_vocab:\n",
    "#             ln = ' '.join(ws)\n",
    "#             if ln not in lns:\n",
    "#                 lns[ln] = len(lns)\n",
    "#             ls = ' '.join(ws[2:4] + ws[0:2])\n",
    "#             if ln not in lns:\n",
    "#                 lns[ln] = len(lns)\n",
    "# ws = '\\n'.join(lns.keys())\n",
    "# open('data/newan2.txt', mode='w', encoding='utf-8').write(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
