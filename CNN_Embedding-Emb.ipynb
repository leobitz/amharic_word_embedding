{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amany\\anaconda2\\envs\\tf2\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "seed_val = 1000\n",
    "random.seed(seed_val)\n",
    "import numpy as np\n",
    "np.random.seed(seed_val)\n",
    "import tensorflow as tf\n",
    "# tf.set_random_seed(seed_val)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM, TimeDistributed, SimpleRNN, ReLU, BatchNormalization\n",
    "from tensorflow.keras.layers import Concatenate, Flatten, Embedding\n",
    "from tensorflow.keras.layers import GRU, Conv2D, MaxPooling2D, AveragePooling2D, AvgPool2D, MaxPool1D, TimeDistributed\n",
    "from tensorflow.keras.layers import Input, Reshape, Dot, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras import regularizers\n",
    "# from tensorflow.keras.utils.vis_utils import plot_model\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "from data_handle import *\n",
    "from gensim_wrapper import *\n",
    "from utils import *\n",
    "import gensim\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model_multi(n_chars, n_features, n_units, char_emb_size=32):\n",
    "    root_word_input = Input(shape=(n_chars,), name=\"word_input\")\n",
    "    embedding = Embedding(input_dim=n_features, input_length=n_chars, output_dim=char_emb_size)(root_word_input)\n",
    "    x = Reshape([n_chars, char_emb_size, 1])(embedding)\n",
    "    \n",
    "    x = Conv2D(32, (3, 3), padding='same', activation='linear')(x)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "#     x = Conv2D(16, (3, 3), padding='same', activation='linear')(x)\n",
    "#     x = MaxPooling2D(2, 2)(x)\n",
    "#     x = ReLU()(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    print(x)\n",
    "    state_h = Dense(n_units, activation='linear', name=\"state_h\")(x)\n",
    "\n",
    "    \n",
    "    consonant_decoder_inputs = Input(shape=(None, n_features), name=\"target_consonant\")\n",
    "    consonant_decoder_gru = GRU(n_units, return_sequences=True, return_state=True,  name=\"consonant_decoder_gru\")\n",
    "    consonant_decoder_outputs, _= consonant_decoder_gru(consonant_decoder_inputs, initial_state=state_h)\n",
    "\n",
    "#     vowel_decoder_inputs = Input(shape=(None, n_vowels), name=\"vowel_input\")\n",
    "#     vowel_decoder_gru = GRU(n_units, return_sequences=True, return_state=True, name=\"vowl_decoder_gru\")\n",
    "#     vowel_decoder_outputs, _= vowel_decoder_gru(vowel_decoder_inputs, initial_state=state_h)\n",
    "\n",
    "    consonant_decoder_dense = Dense(n_features, activation='softmax', name=\"consonant_output\")\n",
    "    consonant_decoder_outputs = consonant_decoder_dense(consonant_decoder_outputs)\n",
    "    \n",
    "#     vowel_decoder_dense = Dense(n_vowels, activation='softmax', name=\"vowel_output\")\n",
    "#     vowel_decoder_outputs = vowel_decoder_dense(vowel_decoder_outputs)\n",
    "    \n",
    "    main_model = Model([root_word_input, consonant_decoder_inputs], consonant_decoder_outputs)\n",
    "    \n",
    "    encoder_model = Model(root_word_input, state_h)\n",
    "    \n",
    "    decoder_state_input_h = Input(shape=(n_units,))\n",
    "    \n",
    "    consonant_decoder_outputs, state_h= consonant_decoder_gru(consonant_decoder_inputs, initial_state=decoder_state_input_h)\n",
    "    consonant_decoder_outputs = consonant_decoder_dense(consonant_decoder_outputs)\n",
    "    \n",
    "#     vowel_decoder_outputs, state_h= vowel_decoder_gru(vowel_decoder_inputs, initial_state=decoder_state_input_h)\n",
    "#     vowel_decoder_outputs = vowel_decoder_dense(vowel_decoder_outputs)\n",
    "    \n",
    "    decoder_model = Model([consonant_decoder_inputs, decoder_state_input_h], [consonant_decoder_outputs, state_h])\n",
    "\n",
    "    return main_model, encoder_model, decoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(word2int, batch_size):\n",
    "    words = list(word2int.keys())\n",
    "    targets, target_inputs = {}, {}\n",
    "    for word in words:\n",
    "        target = word + '|'\n",
    "        target_input = '&' + target \n",
    "        targets[word] = target\n",
    "        target_inputs[word] = target_input\n",
    "        \n",
    "    current = 0\n",
    "    indexes = np.arange(len(targets))\n",
    "    n_batchs = len(words) // batch_size\n",
    "    n_chars = 13\n",
    "    \n",
    "    while True:\n",
    "        batch_input = []\n",
    "        batch_decoder_cons_input = []\n",
    "        batch_output_cons = []\n",
    "        \n",
    "        for bi in range(0, batch_size):\n",
    "            input_word = words[indexes[current]]\n",
    "            input_map = word2vec_sparse(char2int, input_word, n_chars)\n",
    "#             print(input_map)\n",
    "            target_map = word2vec(char2tup, targets[input_word], n_chars)\n",
    "            decoder_map = word2vec(char2tup, target_inputs[input_word], n_chars)\n",
    "\n",
    "            batch_input.append(input_map)\n",
    "            batch_decoder_cons_input.append(decoder_map)\n",
    "            batch_output_cons.append(target_map)\n",
    "            \n",
    "            current += 1\n",
    "        if current + batch_size > len(vocab):\n",
    "            current = 0\n",
    "            random.shuffle(indexes)\n",
    "            \n",
    "        batch_input = np.array(batch_input)\n",
    "        batch_decoder_cons_input = np.array(batch_decoder_cons_input)\n",
    "        batch_output_cons = np.array(batch_output_cons)\n",
    "\n",
    "        yield [batch_input, batch_decoder_cons_input], batch_output_cons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = read_file()\n",
    "vocab, word2int, int2word = build_vocab(words)\n",
    "word2freq = get_frequency(words, word2int, int2word)\n",
    "char2int, int2char, char2tup, tup2char, n_consonant, n_vowel = build_charset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chars = 11 + 2 \n",
    "n_features = len(char2int)\n",
    "batch_size = 300\n",
    "embed_size = 100\n",
    "n_batches = len(vocab)  // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"flatten/Reshape:0\", shape=(None, 3072), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del multi_train\n",
    "    del multi_enc\n",
    "    del multi_dec\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "except:\n",
    "    pass\n",
    "multi_train, multi_enc, multi_dec = conv_model_multi(n_chars, len(char2int), n_units=embed_size)\n",
    "adam = keras.optimizers.Adam(.001)\n",
    "multi_train.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['acc'])\n",
    "multi_gen = generate(word2int,  batch_size)\n",
    "# words, char2int, char2tup, batch_size, n_consonant, n_vowels\n",
    "# plot_model(multi_train, show_shapes=True, show_layer_names=True)\n",
    "# multi_train.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensg = generate( word2int,  batch_size)\n",
    "# [x1, x2], y = next(gensg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# SVG(model_to_dot(multi_train, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "992/992 [==============================] - 115s 115ms/step - loss: 2.1236 - acc: 0.4998\n",
      "Epoch 2/5\n",
      "992/992 [==============================] - 108s 109ms/step - loss: 1.5572 - acc: 0.5176\n",
      "Epoch 3/5\n",
      "992/992 [==============================] - 112s 113ms/step - loss: 1.4919 - acc: 0.5149\n",
      "Epoch 4/5\n",
      "992/992 [==============================] - 110s 111ms/step - loss: 1.4631 - acc: 0.5141\n",
      "Epoch 5/5\n",
      "992/992 [==============================] - 109s 110ms/step - loss: 1.4473 - acc: 0.5139s - loss: 1.4473 - ac\n"
     ]
    }
   ],
   "source": [
    "history = multi_train.fit_generator(multi_gen, steps_per_epoch=n_batches, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_embeddings(vocab, encoder, char2int):\n",
    "    embeddings = np.ndarray((len(vocab), embed_size))\n",
    "    i = 0\n",
    "    buffer = []\n",
    "    buffer_size = 10000\n",
    "    for wi, word in enumerate(vocab):\n",
    "        word = int2word[word2int[word]]\n",
    "        mat = word2vec_sparse(char2int, word, n_chars)\n",
    "#         mat = convec.reshape((-1, n_chars, len(char2int), 1))\n",
    "#         mat = np.concatenate([convec, vowvec], axis=2)\n",
    "        buffer.append(mat)\n",
    "        if len(buffer) == buffer_size or len(vocab) - wi < buffer_size:\n",
    "            buffer_np = np.stack(buffer)#.reshape((-1, 13, len(char2int)))\n",
    "            result = encoder.predict(buffer_np)\n",
    "            embeddings[i:i+len(buffer)] = result\n",
    "            i += len(buffer)\n",
    "            buffer = []\n",
    "            if i % (4 * buffer_size) == 0:\n",
    "                print(\"Predicting: {0:.2f}%\".format((i * 100.0 / len(vocab))))\n",
    "                \n",
    "    print(\"finished\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 13.43%\n",
      "Predicting: 26.86%\n",
      "Predicting: 40.29%\n",
      "Predicting: 53.72%\n",
      "Predicting: 67.15%\n",
      "Predicting: 80.58%\n",
      "Predicting: 94.01%\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# del int2word[0]\n",
    "# i = vocab.index('<unk>')\n",
    "# del vocab[i]\n",
    "\n",
    "embeddings = pred_embeddings(vocab, multi_enc, char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0630 23:55:42.284978 11376 base_any2vec.py:686] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "W0630 23:55:42.564231 11376 smart_open_lib.py:379] this function is deprecated, use smart_open.open instead\n",
      "W0630 23:55:55.093498 11376 base_any2vec.py:686] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "W0630 23:55:55.388223 11376 smart_open_lib.py:379] this function is deprecated, use smart_open.open instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'syntactic': 2.5700934579439254, 'semantic': 0.0, 'total': 1.5736766809728182}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(word2int, embeddings) \n",
    "evaluate(word2int, normalize(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open(\"results/text.txt\", encoding='utf8', mode='w')\n",
    "# file.write(\"{0} {1}\\n\".format(len(vocab), embed_size))\n",
    "# for word, index in word2int.items():\n",
    "#     e = embeddings[index]\n",
    "#     e = ' '.join(map(lambda x: str(x), e))\n",
    "#     file.write(\"{0} {1}\\n\".format(word, e))\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
